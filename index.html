<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Overflowytics</title>
    <link rel="stylesheet" href="./Website/style.css" />

    <!-- Google Font -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600&display=swap"
      rel="stylesheet"
    />
  </head>
  <body>
    <!-- Navbar -->
    <nav class="navbar">
      <div class="title">
        <img
          src="Website/assets/stack_logo.png"
          alt="Stack Overflow Icon"
          class="navbar-icon"
        />
        Overflowytics
      </div>
      <ul class="menu">
        <li><a href="#" data-target="home">Home</a></li>
        <li><a href="#" data-target="introduction">Introduction</a></li>
        <li><a href="#" data-target="data-prep">Data Prep</a></li>
        <li><a href="#" data-target="models">Models</a></li>
        <li><a href="#" data-target="conclusion">Conclusion</a></li>
      </ul>
    </nav>

    <!-- Page Content -->
    <div class="content">
      <!-- Home Section -->
      <section id="home" class="page-section active"></section>
      <!-- Introduction Section -->
      <section id="introduction" class="page-section">
        <h1>Stack Overflow: A Pillar of the Developer Community</h1>
        <p>
          Stack Overflow is the world's largest and most prominent platform
          designed for developers and programmers to learn, share knowledge, and
          collaborate. Established in 2008 by Jeff Atwood and Joel Spolsky, the
          platform was created to address critical challenges faced by
          developers seeking accurate and timely programming solutions.
          Operating as a question-and-answer forum, Stack Overflow facilitates
          peer-reviewed responses to technical queries across a broad spectrum
          of programming languages, frameworks, and tools. By fostering
          high-quality knowledge exchange, the platform builds a credibility
          system where users earn reputation points and achievement-based badges
          for their contributions, enhancing community trust and engagement.
        </p>
        <img
          src="Website/assets/stack_iver_flow_logo1.jpeg"
          alt="Stack Overflow Logo"
          class="profile-image"
        />
        <p>
          With over 22 million questions and hundreds of millions of monthly
          users, Stack Overflow has become a cornerstone of the global software
          development ecosystem. The platform empowers developers to solve
          complex problems efficiently while supporting continuous learning and
          technical growth. Many professionals rely on Stack Overflow as their
          first point of reference when debugging issues, exploring new
          technologies, or improving their skills. Its mission to "help
          developers write the script of the future" underscores its
          significance in driving technological innovation and progress in the
          industry.
        </p>
        <h1>The Impact of AI Tools on Stack Overflow</h1>
        <p>
          The rapid rise of AI tools, particularly generative models such as
          ChatGPT, has introduced significant challenges to Stack Overflow’s
          operational model. These AI tools provide instant, conversational
          responses to programming-related questions, disrupting traditional
          forums by reducing the need for users to post questions and await
          peer-reviewed answers. Consequently, the platform has experienced a
          noticeable decline in user activity and engagement. Research indicates
          a 16% reduction in weekly posts soon after the release of ChatGPT,
          suggesting that developers are increasingly turning to AI for quick
          solutions rather than contributing to Stack Overflow’s knowledge base.
        </p>
        <p>
          This decline has directly affected the core pillars of the platform:
          user-generated content, community discussions, and expert
          contributions. Many experienced users who previously provided
          high-quality answers have reduced their activity on the platform,
          resulting in a diminished pool of expertise. This impacts both the
          diversity and depth of available solutions, ultimately threatening the
          collaborative knowledge-sharing ecosystem upon which Stack Overflow
          thrives.
        </p>
        <p>
          Additionally, the increased presence of AI-generated responses on
          Stack Overflow itself has presented further challenges. Moderation
          teams have reported a surge in low-quality answers generated by AI
          tools. While these responses may appear accurate on the surface, they
          often lack context or detailed reasoning, placing a greater burden on
          moderators to filter and maintain the platform's content quality.
        </p>
        <p>
          According to Stack Overflow's 2024 Developer Survey, 76% of developers
          are actively using or considering AI tools to aid their programming
          tasks. Despite the growing adoption of AI, 43% of respondents
          expressed concerns regarding the reliability of AI-generated answers,
          particularly in complex or nuanced scenarios. Nonetheless, developers
          continue to favor AI for its speed and convenience, further reducing
          reliance on community-driven solutions.
        </p>
        <img
          src="Website/assets/sto_vs_chatpgt.webp"
          alt="Impact of AI on Stack Overflow"
          class="profile-image"
        />
        <h1>Project Motivation</h1>
        <p>
          The motivation behind this project is to analyze Stack Overflow's user
          data and activity patterns using machine learning techniques. By
          identifying key trends, including shifts in user engagement and the
          influence of AI tools, the project aims to uncover factors that impact
          the platform's long-term sustainability. Understanding these patterns
          can provide actionable insights to help Stack Overflow strategize for
          future growth, improve user retention, and maintain its position as a
          vital resource for the global developer community.
        </p>
        <h1>Objectives</h1>
        <p>
          The objective of this project is to derive insights into user
          engagement and activity trends on Stack Overflow by leveraging data
          analysis and machine learning techniques. The study focuses on
          exploring and addressing key research questions, including patterns of
          user participation, reputation dynamics, content interaction, and the
          influence of external factors like AI tools. These insights aim to
          support the development of strategies to enhance user retention,
          improve content quality, and sustain long-term platform growth.
        </p>
        <ol>
          <li>
            1. Which user types (e.g., experienced, new, or moderators) are most
            active on Stack Overflow?
          </li>
          <li>2. What factors influence user reputation growth over time?</li>
          <li>
            3. Which tags or topics lead to the highest engagement (views,
            upvotes, answers)?
          </li>
          <li>
            4. How does user activity (e.g., number of posts, last access date)
            change over time?
          </li>
          <li>
            5. What types of users are more likely to leave the platform (low
            engagement, declining activity, etc.)?
          </li>
          <li>
            6. Which types of questions (based on topic, length, or complexity)
            are most likely to receive answers?
          </li>
          <li>
            7. How does the time of posting (day of the week, time of day)
            impact the likelihood of receiving quick responses?
          </li>
          <li>
            8. What are the common characteristics of highly upvoted or highly
            answered questions?
          </li>
          <li>
            9. How do AI tools influence the frequency and type of questions
            being asked?
          </li>
          <li>
            10. Which tags or topics have seen a decline in user engagement due
            to the rise of AI tools?
          </li>
          <li>
            11. What user behaviors are predictive of long-term retention and
            active participation on the platform?
          </li>
          <li>
            12. Which regions or locations (if location data is available) have
            the highest or lowest participation rates on Stack Overflow?
          </li>
          <li>
            13. What factors (e.g., reputation changes, badges earned) predict
            whether a user will become a top contributor?
          </li>
          <li>
            14. How can Stack Overflow tailor its platform to encourage users to
            stay engaged despite competition from AI tools?
          </li>
          <li>
            15. How might user activity and engagement trends evolve over the
            next few years given the current usage patterns and the increasing
            adoption of AI-based coding tools?
          </li>
        </ol>
      </section>

      <!-- Data Prep Section -->
      <section id="data-prep" class="page-section">
        <h1>Data Collection</h1>
        <p>
          Effective data collection is crucial in any data-driven project,
          particularly when using machine learning to analyze trends and make
          predictions. Reliable and high-quality data enables accurate model
          training, which, in turn, provides better insights and predictions. In
          this project, user data from Stack Overflow was collected to analyze
          user engagement, activity trends, and platform growth patterns. Since
          the platform is large and continuously evolving, accessing up-to-date
          data through manual extraction would be inefficient and error-prone.
          Instead, APIs (Application Programming Interfaces) provide a
          systematic way to collect structured data in real-time.
        </p>
        <p>
          APIs allow automated access to vast datasets while ensuring that data
          retrieval follows the platform's usage policies and structure. By
          using Stack Exchange's API, this project leveraged a reliable method
          to retrieve relevant user information, including reputation, location,
          activity dates, and earned badges. This API offers endpoints that
          provide data in a machine-readable JSON format, making it ideal for
          large-scale data collection and processing.
        </p>
        <p>
          This automated approach ensures that the data collection process is
          efficient, scalable, and capable of handling large volumes of user
          data from Stack Overflow. By using APIs, the project can be easily
          updated with fresh data in the future, enabling continuous analysis
          and improvement of machine learning models.
        </p>
        <p>
          The script used for data collection follows a structured approach:
        </p>
        <ul>
          <li>
            <strong>API Request and Pagination:</strong>
            The script builds API requests to fetch user data in batches of 100
            profiles per request. It supports pagination to iterate through
            multiple pages of data, starting from page 1 and continuing until
            all available data is retrieved.
          </li>
          <li>
            <strong>Data Saving:</strong>
            The response data from each API call is saved both as a JSON file
            and a CSV file for analysis. The JSON file stores the complete API
            response, preserving all details, while the CSV file organizes key
            fields in tabular format.
          </li>
          <li>
            <strong>Dynamic Extraction:</strong>
            The script dynamically extracts important fields, including user ID,
            display name, reputation, location, profile image, and badge counts.
            It also compiles information about collectives (groups or
            communities) the users may belong to.
          </li>
          <li>
            <strong>Folder and File Organization:</strong>
            To keep the collected data organized, the script creates separate
            folders for JSON and CSV files. Each file is named using a timestamp
            and page number, ensuring easy identification and retrieval.
          </li>
          <li>
            <strong>Rate Limiting:</strong>
            To prevent exceeding the API's request limits, the script includes a
            short delay between requests. This helps maintain compliance with
            API usage guidelines while avoiding service interruptions.
          </li>
          <li>
            <strong>Error Handling:</strong>
            The script verifies the success of each API call by checking the
            HTTP status code. If a request fails, an error message is displayed,
            and the data collection loop terminates gracefully. The loop also
            ends when there are no more pages of data to fetch.
          </li>
        </ul>
        <img
          src="Website/assets/api_data_extrcation_log.png"
          alt="extraction log"
          class="profile-image"
        />
        <p>
          Stack Exchange API Documentation -
          <a href="https://api.stackexchange.com/docs" target="_blank"
            >Link to API details</a
          >
        </p>
        <p>
          Python Script for Data Extraction -
          <a
            href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/stackexchange_data_extraction.py"
            target="_blank"
            >Data Extraction Script</a
          >
        </p>
        <h1>Data Preparation</h1>
        <p>
          After extracting the data from the Stack Exchange API, the next
          crucial step was data preparation. Since the API returned multiple CSV
          files, each containing a batch of user records, a Python script was
          developed to merge these individual files into a single dataset. The
          script scans a directory containing the CSV files, reads each file
          into a Pandas DataFrame, and then concatenates all DataFrames into one
          comprehensive dataset. This merged file, containing approximately
          100,000 records, provides a centralized and organized dataset that
          will be used for further analysis and machine learning model training.
        </p>
        <p>
          This step ensures that all relevant data is consolidated, allowing for
          efficient analysis without the need to repeatedly access or process
          multiple smaller files. Proper data preparation is essential to
          eliminate redundancy, handle errors, and structure data in a way that
          optimizes subsequent data exploration and model development.
        </p>
        <p>
          Python Script for Data Preparation -
          <a
            href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/data_merging_script.py"
            target="_blank"
            >Data Preparation Script</a
          >
        </p>
        <h1>Data Cleaning</h1>
        <h3>Importance of Data Cleaning</h3>
        <p>
          Data cleaning is an essential step in preparing data for analysis and
          model training. Real-world data often contains missing values,
          inconsistencies, and errors, which can lead to unreliable and
          inaccurate insights. Unclean data negatively impacts the performance
          of machine learning models, leading to bias and misleading results. By
          thoroughly cleaning the data, we ensure its quality and reliability,
          allowing models to make accurate predictions. Effective data cleaning
          helps to minimize errors, improves model performance, and provides
          meaningful insights during data analysis.
        </p>
        <p>
          The data cleaning process began with analyzing the dataset structure,
          including columns, data types, and sample data points. Both
          categorical and numerical fields were reviewed to identify data points
          requiring further cleaning. Missing values were assessed by
          calculating the percentage of missing entries per column. Columns with
          excessively high missing values, such as collective_names, were
          dropped due to limited analytical value. Columns with moderate missing
          data, such as website_url and accept_rate, were handled appropriately.
          For numerical columns like accept_rate, missing values were filled
          with the median. Missing values in categorical columns like
          display_name were replaced with a unique string pattern,
          username_{user_id}. Duplicate records were identified and removed to
          ensure data integrity. Derived features were added to enhance
          analytical capabilities, including account_age_years, calculated from
          the creation_date column to analyze user engagement trends over time.
          Data type conversions were performed to standardize formats. Date
          fields such as creation_date, last_modified_date, and last_access_date
          stored in epoch format were converted to datetime format for
          time-based analysis. These steps resulted in a clean, structured
          dataset, free of missing values, duplicates, and data type
          inconsistencies. The refined dataset is prepared for exploratory
          analysis and machine learning model training.
        </p>
        <h3>Data Attributes from Raw Data</h3>
        <img
          class="profile-image"
          src="Website/assets/data columns.png"
          alt="data columns"
        />
        <h3>Merged Raw Data</h3>
        <img
          class="profile-image"
          src="Website/assets/merged_raw_data.png"
          alt="data columns"
        />
        <h3>Cleaned Data</h3>
        <img
          class="profile-image"
          src="Website/assets/cleaned_data.png"
          alt="data columns"
        />
        <p>
          Python Code for Data Cleaning -
          <a
            href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Datacleaning/Data_cleaning.ipynb"
            target="_blank"
            >Data Cleaning Code</a
          >
        </p>
        <p>
          Raw Data -
          <a
            href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/merged_users.csv"
            target="_blank"
            >Link to Raw Data</a
          >
        </p>
        <p>
          Cleaned Data -
          <a
            href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/cleaned_user_data.csv"
            target="_blank"
            >Lint to Cleaned Data</a
          >
        </p>
        <h1>Data Visualization</h1>
        <h4>Visualization 1: Total Badge Count by Type</h4>
        <img
          class="profile-image"
          src="Website/assets/Total_Badge_Count_by_Type.png"
          alt="Visualisation-1"
        />
        <ol>
          <li>
            - <b>Distribution Shape:</b> The badge distribution shows that
            bronze badges are the most common, followed by silver, while gold
            badges are comparatively rare.
          </li>
          <li>
            - <b>Central Tendency:</b> The count difference between badge types
            suggests that users are more frequently awarded lower-tier badges.
          </li>
          <li>
            - <b>Spread:</b> The spread is significant between the badge types,
            with a substantial gap between bronze and gold.
          </li>
          <li>
            - <b>Outliers:</b> There are no evident outliers since the count of
            each badge type follows the expected hierarchical structure of
            difficulty and rarity.
          </li>
          <li>
            - <b>Overall Interpretation:</b> The visualization indicates that
            while users are highly rewarded with bronze and silver badges, gold
            badges remain exclusive to top contributors or achievements.
          </li>
        </ol>

        <h4>Visualization 2: Reputation Distribution</h4>
        <img
          class="profile-image"
          src="Website/assets/Reputation_Distribution.png"
          alt="Visualisation-2"
        />
        <ol>
          <li>
            - <b>Distribution Shape:</b> The reputation distribution is heavily
            right-skewed, with most users having low reputation scores.
          </li>
          <li>
            - <b>Central Tendency:</b> The majority of users have a reputation
            close to zero, indicating a high concentration near the lower end.
          </li>
          <li>
            - <b>Spread:</b> The range of reputation values extends to over 1.4
            million, showing a wide disparity between users with low and high
            reputation.
          </li>
          <li>
            - <b>Outliers:</b> A few users have extremely high reputations,
            representing outliers with significant contributions or
            long-standing presence.
          </li>
          <li>
            - <b>Overall Interpretation:</b> The reputation distribution
            highlights that while a small subset of users is highly reputed, the
            majority are new or infrequent contributors to the platform.
          </li>
        </ol>

        <h4>Visualization 3: Account Age vs Reputation</h4>
        <img
          class="profile-image"
          src="Website/assets/Account_Age_vs_Reputation.png"
          alt="Visualisation-3"
        />
        <ol>
          <li>
            - <b>Visualization:</b> A scatterplot representing the relationship
            between account age (in years) and user reputation, with points
            plotted in a light red color.
          </li>
          <li>
            - <b>Insight:</b> The plot shows a positive correlation, where older
            accounts tend to have higher reputations. However, many users with
            older accounts still maintain low reputation scores, indicating that
            active contribution over time is necessary to build reputation,
            rather than account longevity alone.
          </li>
        </ol>
        <h4>Visualization 4: Reputation Change Over the Year</h4>
        <img
          class="profile-image"
          src="Website/assets/Reputation_Change_Over_the_Year.png"
          alt="Visualisation-4"
        />
        <ol>
          <li>
            - <b>Distribution Shape:</b> The annual reputation change
            distribution is highly skewed, with most users showing minimal to
            zero change in reputation.
          </li>
          <li>
            - <b>Central Tendency:</b> The majority of reputation changes are
            centered near zero, indicating little or no yearly variation for
            most users.
          </li>
          <li>
            - <b>Spread:</b> The reputation change values span a broad range,
            with a few users experiencing significant positive or negative
            changes.
          </li>
          <li>
            - <b>Outliers:</b> A small number of users have large positive
            reputation increases, representing exceptional activity or
            contributions within a year.
          </li>
          <li>
            - <b>Overall Interpretation:</b> This distribution suggests that
            while most users do not experience major changes in reputation
            annually, a select few have substantial fluctuations driven by their
            activity levels on the platform.
          </li>
        </ol>
        <h4>Visualization 5: Badge Counts by User</h4>
        <img
          class="profile-image"
          src="Website/assets/Badge_Counts_by_User.png"
          alt="Visualisation-5"
        />
        <ol>
          <li>
            - <b>Distribution Shape:</b> The distribution of total badges per
            user is highly skewed, with the majority of users having very few
            badges.
          </li>
          <li>
            - <b>Central Tendency:</b> Most users have a total badge count close
            to zero, indicating minimal recognition or contributions.
          </li>
          <li>
            - <b>Spread:</b> The badge count ranges from 0 to over 10,000,
            demonstrating wide variability in user achievements on the platform.
          </li>
          <li>
            - <b>Outliers:</b> A few users have extremely high badge counts,
            representing highly active and accomplished contributors.
          </li>
          <li>
            - <b>Overall Interpretation:</b> The badge distribution indicates
            that while a select group of users earn significant recognition
            through badges, the vast majority engage less frequently or with
            fewer impactful contributions.
          </li>
        </ol>
        <h4>Visualization 6: User Count by Account Age Range</h4>
        <img
          class="profile-image"
          src="Website/assets/User_Count_by_Account _Age_Range.png"
          alt="Visualisation-6"
        />
        <ol>
          <li>
            - <b>Distribution Shape:</b> The distribution shows a peak in user
            count for accounts aged between 12 to 15 years, with a steady
            decrease in older and newer accounts.
          </li>
          <li>
            - <b>Central Tendency:</b> The majority of users fall within the 10
            to 18-year age groups, suggesting a high concentration of users with
            long-standing accounts.
          </li>
          <li>
            - <b>Spread:</b> The user base is unevenly spread, with fewer users
            in the less than 1-year and greater than 20-year account age ranges.
          </li>
          <li>
            - <b>Outliers:</b> The newest accounts (less than 1 year old) and
            the oldest accounts (over 20 years old) have minimal user
            representation.
          </li>
          <li>
            - <b>Overall Interpretation:</b> The platform maintains a strong
            core of users with significant account longevity, reflecting
            long-term engagement and retention trends.
          </li>
        </ol>
        <h4>Visualization 7: Distribution of Accept Rate</h4>
        <img
          class="profile-image"
          src="Website/assets/Distribution_of_Accept_Rate.png"
          alt="Visualisation-7"
        />
        <ol>
          <li>
            - <b>Distribution Shape:</b> The accept rate distribution shows a
            right-skewed pattern, with a peak around the 80% mark.
          </li>
          <li>
            - <b>Central Tendency:</b> A significant portion of users have an
            accept rate between 70% and 90%, indicating a strong tendency toward
            high acceptance rates.
          </li>
          <li>
            - <b>Spread:</b> Accept rates vary widely, ranging from 0% to 100%,
            though lower acceptance rates are much less frequent.
          </li>
          <li>
            - <b>Outliers:</b> Very low accept rates near 0% have minimal
            representation in the dataset.
          </li>
          <li>
            - <b>Overall Interpretation:</b> The graph highlights that the
            majority of users maintain a high accept rate, suggesting consistent
            user engagement with answers received on the platform.
          </li>
        </ol>
        <h4>Visualization 8: Reputation vs Accept Rate</h4>
        <img
          class="profile-image"
          src="Website/assets/Reputation_vs_Accept_Rate.png"
          alt="Visualisation-8"
        />
        <ol>
          <li>
            - <b>Visualization:</b> A scatterplot showing the relationship
            between 'Reputation' and 'Accept Rate', with data points distributed
            across varying values of both variables.
          </li>
          <li>
            - <b>Insight:</b> The plot reveals a positive trend where higher
            accept rates tend to be associated with higher reputations. However,
            there is a significant concentration of users with low reputation
            across all accept rate levels, indicating that reputation growth
            might require consistent high acceptance over time or other factors.
          </li>
        </ol>
        <h4>Visualization 9: Box Plot of Reputation by User Type</h4>
        <img
          class="profile-image"
          src="Website/assets/Box Plot_of_Reputation_by_User_Type.png"
          alt="Visualisation-9"
        />
        <ol>
          <li>
            - <b>Visualization:</b> A box plot showing the distribution of user
            reputation categorized by user types: moderator, registered, and
            unregistered.
          </li>
          <li>
            - <b>Insight:</b> Registered users show a wide spread in reputation
            with many outliers, including extremely high reputations. Moderators
            have a relatively consistent reputation range with fewer outliers,
            while unregistered users have minimal reputation variations.
          </li>
        </ol>
        <h4>Visualization 10: Most Recent User Access Dates</h4>
        <img
          class="profile-image"
          src="Website/assets/Most_Recent_User_ Access_Dates.png"
          alt="Visualisation-10"
        />
        <ol>
          <li>
            - <b>Visualization:</b> A line plot illustrating the number of users
            accessing the platform over time in January 2025.
          </li>
          <li>
            - <b>Insight:</b> There is a steady, minimal user access trend until
            late January, followed by a sharp spike around January 29th,
            potentially due to a major event or system update, before the user
            count declines quickly after.
          </li>
        </ol>
        <p>
          Python Code for Data Visualization -
          <a
            href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Datacleaning/Data_cleaning.ipynb"
            target="_blank"
            >Data Visualization Code</a
          >
        </p>
        <p>
          Github Repository -
          <a
            href="https://github.com/AkhilaAnnireddy/Machine_learning_project"
            target="_blank"
            >Link to gihub code</a
          >
        </p>
      </section>

      <!-- Models Section -->
      <section id="models" class="page-section">
        <!-- Internal Tabs for PCA and Clustering -->
        <div class="internal-tabs">
          <button class="tab-btn active" data-target="pca">PCA</button>
          <button class="tab-btn" data-target="clustering">Clustering</button>
          <button class="tab-btn" data-target="arm">ARM</button>
        </div>

        <!-- PCA Content -->
        <div id="pca" class="model-content active">
          <h1>Overview</h1>

          <h2>Principal Component Analysis (PCA)</h2>
          <p>
            Principal Component Analysis (PCA) is a widely used technique for
            dimensionality reduction in data analysis and machine learning. It
            simplifies high-dimensional datasets by transforming them into a
            smaller set of meaningful features while preserving most of the
            original information. PCA works by identifying the directions,
            called <strong>principal components</strong>, along which the data
            varies the most. By projecting the dataset onto these new axes, PCA
            reduces redundancy, improves computational efficiency, and makes
            pattern analysis easier.
          </p>
          <p>
            PCA is particularly beneficial when datasets contain
            <strong>correlated features</strong> that add complexity without
            providing additional insights. By converting the original variables
            into
            <strong
              >a smaller number of uncorrelated principal components</strong
            >, PCA ensures that only the most important patterns are retained.
            This technique is widely applied in
            <strong
              >image processing, finance, genetics, and recommendation
              systems</strong
            >, where high-dimensional data can be challenging to interpret
            effectively.
          </p>

          <h2>Principal Components</h2>
          <p>
            Principal components are the <strong>new features</strong> created
            by PCA that capture the <strong>maximum variance</strong> in the
            dataset.
          </p>
          <ul>
            <li>
              <strong>First Principal Component (PC1):</strong> Represents the
              direction with the most variation, retaining the highest amount of
              information from the original data.
            </li>
            <li>
              <strong>Second Principal Component (PC2):</strong> Captures the
              second-highest variance while remaining uncorrelated with the
              first.
            </li>
            <li>
              <strong>Subsequent Components:</strong> Each additional component
              captures progressively less variance, ensuring that only the most
              essential structure of the dataset is retained.
            </li>
          </ul>
          <p>
            By selecting only the <strong>top principal components</strong>, PCA
            reduces the number of dimensions while preserving essential
            patterns, making analysis more efficient and interpretable.
          </p>

          <h2>Variance Explained by Principal Components</h2>
          <p>
            To determine how many components are needed,
            <strong>variance explained plots</strong> are used:
          </p>

          <h3>Scree Plot (Variance Explained per Component)</h3>
          <img
            class="profile-image"
            src="Website/assets/cumulative_variance_2d_pca.png"
            alt="Scree Plot"
          />
          <p>
            The scree plot shows how much variance each principal component
            captures. The
            <strong>first few components retain most of the variance</strong>,
            while additional components contribute less.
          </p>

          <h3>Cumulative Variance Plot</h3>
          <img
            class="profile-image"
            src="Website/assets/cummulative_variance_vs_components.png"
            alt="Cumulative Variance Plot"
          />
          <p>
            This graph indicates the
            <strong>cumulative variance retained</strong> as more components are
            added. In this project, the
            <strong
              >first X principal components capture at least 95% of the
              variance</strong
            >, making them sufficient for further analysis.
          </p>

          <h2>Why PCA?</h2>
          <p>
            In this project, PCA is applied to analyze
            <strong>user engagement patterns on Stack Overflow</strong> by
            <strong>reducing the dataset’s dimensionality</strong> while
            preserving crucial information. The dataset contains
            <strong
              >user reputation, badge counts, accept rates, and activity
              levels</strong
            >, which are often <strong>highly correlated</strong>. Direct
            analysis of such high-dimensional data is both computationally
            expensive and difficult to visualize.
          </p>
          <p>
            By applying PCA, the dataset is transformed into a smaller set of
            principal components that
            <strong>retain the highest variance</strong>, helping to
            <strong>remove redundancy and improve efficiency</strong>. This
            allows for
            <strong
              >better interpretation of key factors driving user
              engagement</strong
            >
            while filtering out less significant features.
          </p>

          <h2>How PCA Improves This Project</h2>
          <p>The main goal of PCA in this project is to:</p>
          <ul>
            <li>
              <strong>Enhance Data Visualization:</strong> Reducing dimensions
              makes user engagement patterns easier to visualize.
            </li>
            <li>
              <strong>Improve Model Performance:</strong> Selecting the most
              <strong>informative</strong> components helps refine predictions.
            </li>
            <li>
              <strong>Optimize Feature Selection:</strong> Identifies the most
              relevant attributes without unnecessary complexity.
            </li>
            <li>
              <strong>Increase Computational Efficiency:</strong> Reducing data
              dimensions speeds up analysis and reduces noise.
            </li>
          </ul>

          <h1>Data Preprocessing</h1>
          <p>
            Preprocessing is a crucial step before applying Principal Component
            Analysis (PCA) to ensure that the dataset is properly formatted for
            dimensionality reduction. PCA works by identifying the principal
            components that capture the most variance in the data, but if the
            dataset is not cleaned and standardized, the results can be
            misleading. Features with larger numerical values may dominate the
            analysis, while missing or inconsistent data can introduce errors.
            To ensure that PCA provides meaningful and reliable insights,
            several preprocessing steps are performed, including removing
            non-numeric data, handling missing values, and standardizing the
            dataset.
          </p>
          <p>
            Since PCA relies on variance and covariance calculations, it can
            only process numerical data. Therefore, any categorical or
            text-based columns are removed to ensure that PCA operates
            correctly. Additionally, PCA is sensitive to differences in scale,
            meaning that features with varying magnitudes—such as "reputation"
            in thousands and "badge counts" in single digits—could
            disproportionately influence the principal components. To prevent
            this, standardization is applied using StandardScaler from
            sklearn.preprocessing, which transforms all features to have a mean
            of 0 and standard deviation of 1, ensuring equal contribution to the
            PCA transformation.
          </p>
          <p>
            Another critical preprocessing step is handling missing values, as
            incomplete data can distort variance calculations and affect the
            reliability of principal components. The dataset is checked for
            missing values using isnull().sum(), and missing data is either
            removed or imputed using statistical techniques like the mean or
            median to maintain consistency. Once the dataset is fully cleaned
            and standardized, it is converted back into a structured pandas
            DataFrame, making it easier to interpret and apply PCA effectively.
            These preprocessing steps ensure that PCA successfully reduces
            dimensionality while retaining the most essential information,
            leading to more efficient and insightful data analysis.
          </p>
          <h3>Raw Data</h3>
          <img
            class="profile-image"
            src="Website/assets/cleaned_data.png"
            alt="data columns"
          />
          <h3>Cleaned Data for PCA</h3>
          <img
            class="profile-image"
            src="Website/assets/cleaned_data_for_pca.png"
            alt="data columns"
          />
          <p>
            Code for preproccesing -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/pca_model_impl.ipynb"
              target="_blank"
              >PCA Data Preproccesing Code</a
            >
          </p>
          <p>
            Link to raw dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/cleaned_user_data.csv"
              target="_blank"
              >Raw Dataset</a
            >
          </p>
          <p>
            Link to proccessed dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/user_data_cleaned_for_pca.csv"
              target="_blank"
              >Processed Dataset</a
            >
          </p>

          <h1>Implementation</h1>
          <p>
            Principal Component Analysis (PCA) was applied to the dataset to
            reduce dimensionality while retaining the most important
            information. The dataset initially contained multiple numerical
            features, making it complex to visualize and analyze directly. PCA
            was performed twice—once with two principal components (2D PCA) and
            once with three principal components (3D PCA)—to compare how much
            variance is preserved in each transformation.
          </p>
          <h3>PCA with 2 Components</h3>
          <ul>
            <li>
              - The data was projected onto two principal components to
              visualize it in <b>2D space</b>.
            </li>
            <li>
              - The <b>variance retained</b> in this transformation was
              <b>69.17%</b>, meaning that the two components capture most of the
              important structure in the dataset but still leave out about
              <b>30.83%</b> of the information.
            </li>
            <li>
              - The <b>2D scatter plot</b> shows a
              <b>dense clustering of points</b>, with some outliers indicating
              potential variations or distinct patterns in the data.
            </li>
          </ul>

          <h3>PCA with 3 Components</h3>
          <ul>
            <li>
              - The data was then projected onto
              <b>three principal components</b> for a <b>3D representation</b>.
            </li>
            <li>
              - This transformation retained <b>78.60% of the variance</b>,
              suggesting that adding one more component helps capture an
              additional <b>9.43%</b> of the dataset’s variability.
            </li>
            <li>
              - The <b>3D scatter plot</b> provides a clearer separation of data
              points, indicating that higher dimensions may help in retaining
              more meaningful patterns.
            </li>
          </ul>

          <h3>Visualization for PCA with 2 Components</h3>
          <img
            class="profile-image"
            src="Website/assets/pca_with_2c.png"
            alt="data columns"
          />
          <h4>Observations</h4>
          <ul>
            <li>
              <b>Variance Retention:</b> Captures <b>69.17%</b> of the variance,
              losing <b>30.83%</b> of the dataset’s information.
            </li>
            <li>
              <b>Data Clustering:</b> Points are <b>densely packed</b>,
              indicating that PC1 and PC2 capture major patterns.
            </li>
            <li>
              <b>Outliers:</b> A few <b>distant points</b> suggest potential
              anomalies or distinct patterns.
            </li>
            <li>
              <b>Data Spread:</b> PC1 explains the <b>most variance</b>, while
              PC2 adds minimal new information.
            </li>
            <li>
              <b>Limitation:</b> Loss of variance may <b>oversimplify</b> data
              relationships.
            </li>
          </ul>
          <h3>Visualization for PCA with 3 Components</h3>
          <img
            class="profile-image"
            src="Website/assets/pca_with_3c.png"
            alt="data columns"
          />
          <h4>Observations</h4>
          <ul>
            <li>
              <b>Variance Retention:</b> Preserves <b>78.60%</b> of variance,
              capturing <b>9.43% more information</b> than 2D PCA.
            </li>
            <li>
              <b>Better Separation:</b> Points are <b>more evenly spread</b>,
              meaning PC3 contributes valuable structural details.
            </li>
            <li>
              <b>Outliers:</b> Still present, but <b>better distributed</b>,
              requiring further analysis.
            </li>
            <li>
              <b>Data Depth:</b> PC3 <b>reduces variance loss</b>, making
              patterns more distinguishable.
            </li>
            <li>
              <b>Advantage:</b> <b>Improved representation</b> over 2D, but
              higher dimensions may still be needed for
              <b>95% variance retention</b>.
            </li>
          </ul>

          <h3>Visualization for Cumulative variance retained in 2D PCA</h3>
          <img
            class="profile-image"
            src="Website/assets/cumulative_variance_2d_pca.png"
            alt="data columns"
          />
          <h4>Observations</h4>
          <ul>
            <li>
              <strong>First Component Dominance:</strong> The first principal
              component captures around 45-50% of the variance.
            </li>
            <li>
              <strong>Cumulative Variance:</strong> Adding the second component
              increases total variance retention to 69.17%, reducing information
              loss.
            </li>
            <li>
              <strong>Diminishing Returns:</strong> The second component
              contributes less variance than the first, indicating reduced
              impact of additional components.
            </li>
            <li>
              <strong>Information Loss:</strong> 30.83% variance is lost,
              suggesting that higher dimensions may be needed for better data
              representation.
            </li>
          </ul>

          <h3>Visualization for Cumulative variance retained in 3D PCA</h3>
          <img
            class="profile-image"
            src="Website/assets/cumulative_variance_3d_pca.png"
            alt="data columns"
          />
          <h4>Observations</h4>
          <ul>
            <li>
              <strong>First Component Impact:</strong> The first principal
              component captures around 40-45% of the variance.
            </li>
            <li>
              <strong>Increased Retention:</strong> Adding the second component
              raises cumulative variance to 69.17%, similar to 2D PCA.
            </li>
            <li>
              <strong>Third Component Benefit:</strong> The third principal
              component improves variance retention to 78.60%, adding 9.43% more
              information over 2D PCA.
            </li>
            <li>
              <strong>Reduced Information Loss:</strong> Only 21.40% variance is
              lost, making 3D PCA a better representation compared to 2D PCA.
            </li>
          </ul>

          <h3>Optimal Number of Components for 95% Variance Retention</h3>
          <img
            class="profile-image"
            src="Website/assets/cummulative_variance_vs_components.png"
            alt="data columns"
          />
          <h4>Observations</h4>
          <p>
            To balance dimensionality reduction and information retention, PCA
            was used to determine how many components are needed to preserve at
            least <strong>95%</strong> of the dataset's variance. The analysis
            found that <strong>6 principal components</strong> achieve this
            goal.
          </p>
          <p>The cumulative variance plot visually represents this:</p>
          <ul>
            <li>
              <strong>The red dashed line</strong> marks the
              <strong>95% variance threshold</strong>.
            </li>
            <li>
              <strong>The blue vertical line</strong> shows that
              <strong>6 components</strong> are enough to retain most of the
              data’s structure.
            </li>
          </ul>
          <p>
            Using <strong>6 components</strong> simplifies the dataset while
            ensuring <strong>minimal information loss</strong>, making it
            efficient for further analysis.
          </p>

          <h3>Top three Eigen values</h3>
          <img
            class="profile-image"
            src="Website/assets/eigen_values_pca.png"
            alt="data columns"
          />
          <p>
            Eigenvalues represent the
            <strong>amount of variance</strong> captured by each principal
            component. The higher the eigenvalue, the more important the
            component is in explaining the dataset's variability.
          </p>
          <p>
            For this dataset, the <strong>top three eigenvalues</strong> are:
          </p>
          <ul>
            <li><strong>PC1:</strong> 5.75 (captures the highest variance)</li>
            <li><strong>PC2:</strong> 3.94 (adds significant variance)</li>
            <li>
              <strong>PC3:</strong> 1.32 (contributes less but still meaningful)
            </li>
          </ul>
          <p>
            The <strong>bar chart visualization</strong> highlights the
            difference in variance captured by each component.
            <strong>PC1 dominates</strong>, followed by <strong>PC2</strong>,
            while <strong>PC3 captures much less variance</strong>. This
            analysis helps in understanding how much information each component
            retains and guides the selection of an
            <strong>optimal number of dimensions</strong> for data
            representation.
          </p>
          <p>
            Link to Model Implementation -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/pca_model_impl.ipynb"
              target="_blank"
              >Model Implementation</a
            >
          </p>

          <h1>Results and Conclusion</h1>
          <h2>Results</h2>
          <p>
            The Principal Component Analysis (PCA) was applied to simplify the
            dataset while preserving its core information. The analysis provided
            key insights into the balance between dimensionality reduction and
            information retention.
          </p>

          <h3>Variance Retention in 2D PCA</h3>
          <ul>
            <li>
              <b>Two principal components</b> retained
              <b>69.17% of the total variance</b>.
            </li>
            <li>
              This allowed for <b>simplified visualization</b>, but resulted in
              a <b>30.83% information loss</b>, meaning some patterns were not
              fully captured.
            </li>
            <li>
              The <b>2D PCA scatter plot</b> highlights distinct clusters,
              though some overlap and outliers indicate missing variability.
            </li>
          </ul>

          <h3>Variance Retention in 3D PCA</h3>
          <ul>
            <li>
              Using <b>three principal components</b> increased variance
              retention to <b>78.60%</b>, improving information capture.
            </li>
            <li>
              This represents a <b>9.43% gain</b> compared to 2D PCA, making the
              data structure more distinct.
            </li>
            <li>
              The <b>3D PCA scatter plot</b> provides better separation of data
              points, revealing clearer patterns.
            </li>
            <li>
              However, <b>21.40% of variance remains unaccounted</b>, suggesting
              the need for more dimensions to fully represent the dataset.
            </li>
          </ul>

          <h3>Optimal Number of Components</h3>
          <ul>
            <li>
              <b>Cumulative variance analysis</b> shows that at least
              <b>six principal components</b> are required to retain
              <b>95% of the total variance</b>.
            </li>
            <li>
              This ensures the dataset’s key structure and relationships are
              <b>preserved while reducing complexity</b>.
            </li>
            <li>
              Using <b>six components</b> provides an optimal balance between
              <b>computational efficiency</b> and <b>data integrity</b>.
            </li>
          </ul>

          <h3>Eigenvalue Analysis</h3>
          <ul>
            <li>
              Eigenvalues represent the
              <b>importance of each principal component</b> in capturing
              variance:
            </li>
            <li>
              <b>PC1 (5.75)</b> captures the highest variance, making it the
              most influential.
            </li>
            <li><b>PC2 (3.94)</b> significantly improves representation.</li>
            <li>
              <b>PC3 (1.32)</b> holds meaningful information but contributes
              less.
            </li>
            <li>
              The <b>eigenvalue bar chart</b> visually confirms that
              <b>PC1 dominates</b>, followed by <b>PC2 and PC3</b>, reinforcing
              the importance of the first few components.
            </li>
          </ul>

          <h2>Conclusion</h2>
          <p>
            PCA effectively reduced the dataset's dimensionality while
            preserving its most significant features. The analysis demonstrated
            that:
          </p>
          <ul>
            <li>
              <b>2-component PCA</b> is useful for visualization but causes
              <b>substantial information loss</b>.
            </li>
            <li>
              <b>3-component PCA</b> provides better structure while still
              losing <b>21.40% of variance</b>.
            </li>
            <li>
              <b>6 principal components</b> retain at least
              <b>95% of the dataset's information</b>, making them ideal for
              analysis.
            </li>
            <li>
              <b>Eigenvalue analysis</b> confirms that the first few components
              capture most of the dataset's meaningful structure.
            </li>
          </ul>
          <p>
            By applying PCA, this project successfully
            <b>removed redundancy while maintaining interpretability</b>. The
            findings provide
            <b
              >valuable insights into user engagement patterns on Stack
              Overflow</b
            >, helping to <b>identify key features</b> that drive activity on
            the platform.
          </p>

          <p>
            Link to complete code -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/pca_model_impl.ipynb"
              target="_blank"
              >Complete Implementation</a
            >
          </p>
          <p>
            Github Repository -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project"
              target="_blank"
              >Link to gihub code</a
            >
          </p>
        </div>

        <!-- Clustering Content -->
        <div id="clustering" class="model-content">
          <h1>Overview</h1>
          <h2>Clustering</h2>
          <p>
            Clustering is an unsupervised machine learning technique used to
            group similar data points based on patterns and relationships.
            Unlike classification, clustering does not rely on predefined
            labels; instead, it identifies natural structures within the
            dataset. It is widely used in applications such as customer
            segmentation, anomaly detection, and pattern recognition. The three
            main types of clustering algorithms—K-Means (partition-based),
            Hierarchical Clustering (tree-based), and DBSCAN
            (density-based)—each have unique strengths and weaknesses depending
            on the dataset's characteristics. Clustering helps uncover hidden
            insights, simplifies data for further analysis, and enhances
            decision-making in various domains, from marketing to
            bioinformatics.
          </p>
          <h2>Role of Distance Metrics in Clustering</h2>
          <p>
            Distance metrics play a crucial role in clustering algorithms as
            they determine how similarity between data points is measured. The
            choice of distance metric impacts how clusters are formed and how
            well-separated they are. Commonly used distance metrics include:
          </p>
          <ul>
            <li>
              <b>Euclidean Distance:</b> Measures the straight-line distance
              between two points in a multi-dimensional space. It is widely used
              in K-Means clustering but struggles with high-dimensional data.
            </li>
            <li>
              <b>Manhattan Distance:</b> Measures the distance between two
              points by summing the absolute differences of their coordinates.
              It is useful for grid-like data structures.
            </li>
            <li>
              <b>Cosine Similarity:</b> Measures the angle between two vectors
              rather than the actual distance. It is commonly used in text
              mining and hierarchical clustering.
            </li>
            <li>
              <b>Minkowski Distance:</b> A generalization of Euclidean and
              Manhattan distances, allowing flexibility in measuring distances
              in different ways.
            </li>
          </ul>
          <p>
            Each clustering algorithm may perform differently based on the
            distance metric chosen. K-Means primarily relies on Euclidean
            distance, while hierarchical clustering can use various metrics such
            as cosine similarity or correlation distance. DBSCAN, on the other
            hand, uses epsilon-based (density) distance to group points
            effectively.
          </p>
          <h2>Clustering Techniques</h2>
          <h2>K-Means Clustering</h2>
          <p>
            K-Means is a partition-based clustering algorithm that divides data
            into k groups by minimizing the variance within each cluster. It
            begins by randomly selecting k centroids, then assigns each data
            point to the closest centroid based on Euclidean distance. The
            centroids are iteratively updated until cluster assignments
            stabilize. K-Means is computationally efficient and works well for
            large datasets with well-defined, spherical clusters. However, it
            requires k to be predefined, making it less flexible when the number
            of natural clusters is unknown. It is also sensitive to outliers, as
            they can skew the centroid locations and affect clustering accuracy.
          </p>
          <h2>Hierarchical clustering</h2>
          <p>
            Hierarchical clustering builds a hierarchy of clusters by either
            merging smaller clusters (agglomerative) or splitting larger
            clusters (divisive). It does not require specifying the number of
            clusters beforehand and produces a dendrogram, which visually
            represents the merging or splitting process. This allows users to
            determine the optimal number of clusters by analyzing the hierarchy.
            Unlike K-Means, hierarchical clustering can capture nested
            relationships between clusters, making it useful for structured
            data. However, it is computationally expensive for large datasets,
            as it requires storing a distance matrix for all data points,
            leading to scalability issues.
          </p>
          <h2>DBSCAN (Density-Based Clustering)</h2>
          <p>
            DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
            identifies clusters based on high-density regions separated by
            sparse areas. Unlike K-Means and hierarchical clustering, DBSCAN
            does not require the number of clusters to be predefined. It works
            by defining core points that have a minimum number of neighbors
            within a specified distance (eps). Clusters expand by grouping
            together nearby core points, while points in sparse regions are
            treated as noise. This makes DBSCAN highly effective for identifying
            arbitrarily shaped clusters and handling outliers. However, its
            performance is sensitive to the choice of eps and min_samples,
            making parameter tuning crucial for accurate clustering results.
          </p>
          <h2>Comparison of Clustering Algorithms</h2>
          <table border="1" cellspacing="0" cellpadding="8">
            <thead>
              <tr>
                <th>Criteria</th>
                <th>K-Means</th>
                <th>Hierarchical Clustering</th>
                <th>DBSCAN</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><b>Clustering Approach</b></td>
                <td>Partition-based</td>
                <td>Hierarchical (Agglomerative/Divisive)</td>
                <td>Density-based</td>
              </tr>
              <tr>
                <td><b>Number of Clusters</b></td>
                <td>Requires predefining k</td>
                <td>No need to specify k; determined by dendrogram</td>
                <td>Automatically detects clusters based on density</td>
              </tr>
              <tr>
                <td><b>Shape of Clusters</b></td>
                <td>Assumes spherical clusters</td>
                <td>Can capture complex hierarchical relationships</td>
                <td>Can detect arbitrarily shaped clusters</td>
              </tr>
              <tr>
                <td><b>Handling Noise & Outliers</b></td>
                <td>Sensitive to outliers; can distort centroids</td>
                <td>Less sensitive but still influenced by outliers</td>
                <td>Effectively handles noise by marking it as outliers</td>
              </tr>
              <tr>
                <td><b>Computational Complexity</b></td>
                <td>O(n*k*d) (Fast for large datasets)</td>
                <td>O(n²) (Slow for large datasets)</td>
                <td>O(n log n) (Efficient for large datasets)</td>
              </tr>
              <tr>
                <td><b>Memory Requirement</b></td>
                <td>Low (only centroids stored)</td>
                <td>High (stores distance matrix)</td>
                <td>Moderate (depends on density parameters)</td>
              </tr>
              <tr>
                <td><b>Best Use Cases</b></td>
                <td>Large, well-separated clusters in structured data</td>
                <td>Hierarchical structures, small datasets</td>
                <td>Detecting noise, arbitrary cluster shapes</td>
              </tr>
              <tr>
                <td><b>Weaknesses</b></td>
                <td>Needs k; struggles with non-spherical clusters</td>
                <td>Not scalable; sensitive to noise</td>
                <td>Hard to tune parameters; struggles with varying density</td>
              </tr>
            </tbody>
          </table>
          <p>
            Applying clustering techniques to this project provides valuable
            insights into user behavior and engagement patterns on Stack
            Overflow. By grouping users based on similarities in reputation,
            activity levels, and badge counts, clustering helps identify
            distinct user segments, such as highly active contributors,
            occasional participants, and inactive users. This segmentation
            allows for a deeper understanding of how different user groups
            interact with the platform, enabling targeted strategies to improve
            user retention and engagement. Additionally, clustering helps detect
            anomalies, such as users with unusual activity spikes or potential
            spam accounts, allowing for better moderation and community
            management. By leveraging clustering, the project enhances
            data-driven decision-making, optimizes content recommendations, and
            improves overall platform effectiveness.
          </p>
          <h1>Data Preprocessing</h1>
          <h2>Importance of Data Preprocessing</h2>
          <p>
            Clustering is an unsupervised learning technique that groups similar
            data points based on shared characteristics. However, the quality of
            clustering results is highly dependent on proper data preprocessing.
            Raw data often contains inconsistencies, varying feature scales, and
            non-numeric attributes that can distort the clustering process. To
            ensure meaningful and reliable clusters, essential preprocessing
            steps include
            <b
              >removing labels, selecting only numerical features, standardizing
              the data, and applying dimensionality reduction techniques like
              PCA</b
            >. These steps enhance computational efficiency and improve the
            interpretability of clustering results.
          </p>

          <h2>Removing Labels for Unbiased Clustering</h2>
          <p>
            A key preprocessing step involves
            <b>removing the age_group column</b> before clustering. Clustering
            aims to identify natural groupings in the data without predefined
            labels. By excluding this categorical variable, the model is allowed
            to find inherent structures without bias. The removed labels are
            stored separately for later comparison, enabling an evaluation of
            how well the clustering results align with actual user engagement
            patterns.
          </p>

          <h2>Selection of Numerical Features</h2>
          <p>
            To ensure compatibility with clustering algorithms,
            <b>only numerical features</b> are retained, while categorical and
            text-based columns such as display names and profile links are
            removed. Clustering relies on mathematical distance calculations,
            making non-numeric features unsuitable for analysis.
          </p>

          <h2>Standardization for Consistent Scaling</h2>
          <p>
            To ensure fair feature representation, <b>standardization</b> is
            performed using <b>StandardScaler</b>, which transforms all
            numerical features to have a mean of <b>0</b> and a standard
            deviation of <b>1</b>. This process prevents features with larger
            numerical values from disproportionately influencing the clustering
            results.
          </p>

          <h2>Dimensionality Reduction Using PCA</h2>
          <p>
            For further optimization,
            <b>Principal Component Analysis (PCA)</b> is applied to reduce the
            dataset's dimensionality while preserving most of its variance. Two
            versions of the dataset are prepared:
          </p>
          <ul>
            <li>
              <b>Original Dataset (Without PCA):</b> This dataset retains all
              numerical features after preprocessing, maintaining the full
              feature set.
            </li>
            <li>
              <b>PCA-Transformed Dataset:</b> This dataset is reduced to
              <b>six principal components</b>, preserving
              <b>95% of the variance</b> while eliminating redundancy.
            </li>
          </ul>

          <h2>Impact on Clustering</h2>
          <p>
            Using both versions, clustering will be performed to assess whether
            dimensionality reduction enhances the clustering process. The
            PCA-transformed dataset may provide better-defined clusters by
            reducing noise, while the original dataset retains more detailed
            information. Comparing results from both approaches will determine
            the effectiveness of PCA in improving clustering accuracy.
          </p>

          <h2>Final Preprocessed Dataset</h2>
          <p>
            By implementing these preprocessing steps, the dataset is structured
            to ensure efficient and meaningful clustering, leading to more
            interpretable insights into user engagement and behavior patterns.
          </p>
          <p>
            Code for preproccesing -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Datacleaning/Data_cleaning.ipynb"
              target="_blank"
              >PCA Data Preproccesing Code</a
            >
          </p>
          <p>
            Link to raw dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Datacleaning/Data_cleaning.ipynb"
              target="_blank"
              >Raw Dataset</a
            >
          </p>
          <p>
            Link to proccessed dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Datacleaning/Data_cleaning.ipynb"
              target="_blank"
              >Processed Dataset</a
            >
          </p>
        </div>
        <div id="arm" class="model-content">
          <h1>Overview</h1>
          <h2>Association Rule Mining</h2>
          <p>
            Association Rule Mining is a data mining technique used to identify
            hidden relationships between variables in large datasets. It is
            commonly used in market basket analysis, where businesses analyze
            customer purchases to uncover patterns such as customers who buy
            bread often buy butter. ARM helps in making data-driven decisions
            for product recommendations, inventory management, and marketing
            strategies by discovering frequent itemsets and generating rules
            that highlight meaningful associations.
          </p>
          <h2>Key Measures in ARM: Support, Confidence, and Lift</h2>
          <p>
            Support measures how frequently an itemset appears in the dataset. A
            higher support value indicates that the pattern is more commonly
            occurring.
          </p>
          <p>
            Confidence represents how often the rule "If X, then Y" holds true.
            It calculates the probability that Y is purchased given X is already
            present in the transaction.
          </p>
          <p>
            Lift measures the strength of an association by comparing its
            occurrence with what would be expected if X and Y were independent.
            A lift value greater than one indicates a strong positive
            correlation between X and Y.
          </p>
          <p>
            These measures help filter out weak rules and ensure that only the
            strongest relationships are identified.
          </p>
          <img
            class="profile-image"
            src="Website/assets/arm_pic_1.png"
            alt="data columns"
          />

          <h2>Association Rules</h2>
          <p>
            Association rules are statements that define relationships between
            two or more items in a dataset. They take the form:
          </p>

          <p class="rule">X ⇒ Y</p>

          <p>
            Where <strong>X (Antecedent)</strong> is the condition and
            <strong>Y (Consequent)</strong> is the outcome. For example:
          </p>

          <div class="example">
            <p>
              <strong>{Laptop} → {Mouse}</strong> (If a customer buys a laptop,
              they are likely to buy a mouse.)
            </p>
            <p>
              <strong>{Milk, Bread} → {Butter}</strong> (If a customer buys milk
              and bread, they are likely to buy butter.)
            </p>
          </div>

          <p>
            Strong association rules help in making
            <strong>business recommendations</strong>, improving
            <strong>user experience</strong>, and optimizing
            <strong>inventory management</strong>.
          </p>
          <h2>The Apriori Algorithm and How It Works</h2>

          <p>
            The Apriori Algorithm is a widely used method for discovering
            association rules by identifying frequently occurring itemsets and
            generating meaningful rules.
          </p>

          <p>
            The first step involves scanning the dataset to find individual
            items that appear frequently based on a minimum support threshold.
            Pairs, triplets, and higher-order itemsets are generated, keeping
            only those that meet the support criteria.
          </p>

          <p>
            Next, the algorithm applies the Apriori Property, which states that
            if an itemset is infrequent, its supersets will also be infrequent.
            This helps eliminate unnecessary calculations, making the algorithm
            efficient and scalable.
          </p>

          <p>
            Frequent itemsets are then used to create rules, and their
            confidence and lift values are calculated. Only rules meeting
            predefined confidence and lift thresholds are retained.
          </p>

          <p>
            Finally, the rules are ranked based on Support, Confidence, and
            Lift, ensuring only the strongest and most meaningful associations
            are considered.
          </p>
          <img
            class="profile-image"
            src="Website/assets/arm_pic_2.png"
            alt="data columns"
          />
          <h2>How ARM is Used in This Project</h2>
          <p>
            In this project, ARM was applied to analyze user engagement based on
            factors like reputation, badges, and account longevity. The dataset
            was preprocessed to ensure compatibility with the Apriori algorithm.
          </p>
          <p>
            Numerical values were converted into binary categories, such as
            assigning one to high reputation if the reputation score is greater
            than one thousand.
          </p>
          <p>
            Categorical attributes were one-hot encoded to ensure efficient rule
            extraction. Frequent itemsets were extracted using Apriori with a
            minimum support of five percent. Association rules were generated
            based on Lift greater than one, ensuring only meaningful connections
            were retained.
          </p>
          <p>
            The results showed strong correlations between high reputation and
            badge accumulation, as well as a connection between high acceptance
            rates and silver badges, reinforcing how user engagement impacts
            recognition.
          </p>
          <h1>Data Preparation</h1>
          <p>
            Before applying <strong>Association Rule Mining (ARM)</strong>, the
            dataset needs to be transformed into a format suitable for
            algorithms like <strong>Apriori</strong>. The following
            preprocessing steps were applied to prepare the data:
          </p>

          <h3>1. Converting Numerical Attributes into Binary Categories</h3>
          <p>
            ARM works with binary data, where each attribute is either present
            (1) or absent (0). To achieve this, numerical attributes were
            categorized into meaningful binary indicators. Examples include:
          </p>
          <ul>
            <li>
              <code>high_reputation = 1</code> if
              <code>reputation > 1000</code>, else <code>0</code>.
            </li>
            <li>
              <code>many_bronze_badges = 1</code> if
              <code>badge_bronze > 10</code>, else <code>0</code>.
            </li>
            <li>
              <code>old_account = 1</code> if
              <code>account_age_days > 1000</code>, else <code>0</code>.
            </li>
          </ul>

          <h3>2. Encoding Categorical Variables</h3>
          <p>
            To ensure categorical attributes can be used in ARM, one-hot
            encoding was applied. This converts categories into separate binary
            columns. Example:
          </p>
          <ul>
            <li>
              <code>user_type</code> and <code>age_group</code> were transformed
              using one-hot encoding.
            </li>
            <li>
              The first category in each variable was dropped
              (<code>drop_first=True</code>) to avoid redundancy.
            </li>
          </ul>

          <h3>3. Selecting Relevant Features for ARM</h3>
          <p>
            Only binary-transformed attributes were retained for ARM. The final
            dataset included:
          </p>
          <ul>
            <li>
              <code>high_reputation</code>, <code>many_bronze_badges</code>,
              <code>many_silver_badges</code>
            </li>
            <li>
              <code>many_gold_badges</code>, <code>old_account</code>,
              <code>high_accept_rate</code>
            </li>
          </ul>

          <h3>4. Final Data Structure</h3>
          <p>
            The transformed dataset now contains binary values (1 or 0),
            ensuring compatibility with ARM algorithms. Example:
          </p>
          <pre>
         high_reputation  many_bronze_badges  many_silver_badges  many_gold_badges  old_account  high_accept_rate
      0              1                   1                   1                 1            1                 1
      1              1                   1                   1                 1            1                 1
      2              1                   1                   1                 1            1                 1
              </pre
          >

          <p>
            With this preprocessing, the dataset is now ready for ARM, allowing
            for efficient discovery of meaningful association rules.
          </p>
          <h3>Raw Data</h3>
          <img
            class="profile-image"
            src="Website/assets/cleaned_data.png"
            alt="data columns"
          />
          <h3>Cleaned Data for ARM</h3>
          <img
            class="profile-image"
            src="Website/assets/arm_cleaned_dataset.png"
            alt="data columns"
          />
          <p>
            Code for preproccesing -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/arm_model_impl.ipynb"
              target="_blank"
              >ARM Data Preproccesing Code</a
            >
          </p>
          <p>
            Link to raw dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/cleaned_user_data.csv"
              target="_blank"
              >Raw Dataset</a
            >
          </p>
          <p>
            Link to proccessed dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/user_data_cleaned_for_arm.csv"
              target="_blank"
              >Processed Dataset</a
            >
          </p>
          <h1>Implementation</h1>
          <p>
            Association Rule Mining (ARM) was applied using the Apriori
            algorithm to uncover meaningful relationships within the dataset.
            The process began with identifying frequent itemsets that appeared
            in at least 5% of transactions, ensuring that only significant
            patterns were considered. Once the frequent itemsets were
            established, association rules were generated based on the lift
            metric, with a minimum threshold of 1.0, ensuring that the
            discovered relationships held strong and valuable connections.
          </p>
          <p>
            The extracted rules provided valuable insights into user behavior.
            Users with many bronze badges almost always had high reputation,
            emphasizing that active participation contributes to credibility.
            Similarly, older accounts were strongly linked to high reputation,
            indicating that platform longevity plays a key role in building user
            trust. Another notable pattern revealed that users with high
            acceptance rates had a moderate likelihood of having high
            reputation, highlighting that consistent contribution acceptance
            influences recognition.
          </p>
          <p>
            To further refine these insights, rules were ranked based on
            support, confidence, and lift, with the top 15 rules selected for
            each metric. This ranking helped filter out weaker patterns and
            highlighted the most impactful associations. These insights can be
            leveraged for recommendation systems, user engagement strategies,
            and customer segmentation, allowing for data-driven decision-making.
          </p>
          <h3>Association Rules Network Visualization</h3>
          <img
            class="profile-image"
            src="Website/assets/arm_network_final.png"
            alt="data columns"
          />
          <p>
            The Association Rules Network Graph visually represents the
            discovered relationships between different user attributes. Each
            node in the graph represents an attribute, while edges between nodes
            indicate a strong association between them.
          </p>
          <p>
            The visualization clearly highlights that high reputation is
            strongly connected to many badges and old accounts, suggesting that
            long-term engagement and earning multiple badges contribute to user
            credibility. Additionally, a high acceptance rate is linked to many
            silver badges, indicating that users whose contributions are
            frequently accepted tend to receive greater recognition.
          </p>
          <p>
            The network structure forms distinct clusters, showing how
            engagement, experience, and recognition are interconnected. This
            visualization provides an intuitive understanding of user behavior,
            helping platforms identify key influencers, optimize engagement
            strategies, and enhance recommendation systems.
          </p>

          <h1>Results</h1>
          <p>
            Association Rule Mining was applied using the Apriori algorithm to
            uncover relationships between different user attributes. Frequent
            itemsets were extracted with a minimum support threshold of 5%,
            ensuring that only the most significant patterns were considered.
            The association rules were then generated based on the lift metric,
            with a minimum threshold of 1.0, ensuring that the discovered
            patterns had meaningful relationships.
          </p>
          <p>
            The extracted rules provided key insights into user behavior. A
            strong relationship was observed between users with many bronze
            badges and high reputation, indicating that reputation grows with
            increased participation. Similarly, users with old accounts were
            highly likely to have high reputation, showing that longer platform
            engagement contributes to credibility. Additionally, users with a
            high acceptance rate had a moderate likelihood of possessing a high
            reputation, suggesting that accepted contributions play a role in
            recognition.
          </p>
          <p>
            To refine these insights, the rules were ranked based on support,
            confidence, and lift, selecting the top 15 rules for each metric.
            These rankings helped in identifying the strongest relationships,
            ensuring that only the most impactful patterns were retained.
          </p>
          <p>
            A network visualization was created to represent these relationships
            graphically. The visualization illustrated the key factors
            influencing reputation, engagement, and recognition, making it
            easier to understand the underlying connections in user behavior.
          </p>
          <h2>Conclusion</h2>
          <p>
            Understanding how users engage with a platform is crucial for
            improving user experience, recognition systems, and retention
            strategies. The findings from Association Rule Mining highlight the
            importance of engagement, consistency, and contribution quality in
            building reputation and credibility. Just like in real-world
            communities where trust is earned over time, users on digital
            platforms also gain recognition through continued participation and
            meaningful contributions.
          </p>
          <p>
            These insights can be applied in various ways. Platforms can use
            these patterns to design better reward systems, recommend relevant
            content, and identify active users for engagement programs.
            Businesses can leverage similar techniques for customer
            segmentation, personalized recommendations, and targeted marketing
            strategies.
          </p>
          <p>
            By understanding these patterns, platforms can create a more
            engaging and rewarding experience for users, fostering stronger
            communities and encouraging meaningful contributions. The
            implementation of Association Rule Mining not only enhances
            decision-making but also provides a data-driven approach to
            improving engagement and user satisfaction.
          </p>
          <p>
            Link to complete code -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/arm_model_impl.ipynb"
              target="_blank"
              >Complete Implementation</a
            >
          </p>
          <p>
            Github Repository -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project"
              target="_blank"
              >Link to gihub code</a
            >
          </p>
        </div>
      </section>

      <!-- Conclusion Section -->
      <section id="conclusion" class="page-section">
        <h1>Conclusion</h1>
        <p>The conclusion will be added in future milestones.</p>
      </section>
    </div>

    <!-- JavaScript to handle section switching -->
    <script>
      document.addEventListener("DOMContentLoaded", function () {
        // Main navigation switching logic
        const menuLinks = document.querySelectorAll(".menu a");
        const sections = document.querySelectorAll(".page-section");

        menuLinks.forEach((link) => {
          link.addEventListener("click", (e) => {
            e.preventDefault();
            const target = link.getAttribute("data-target");
            sections.forEach((section) => section.classList.remove("active"));
            document.getElementById(target).classList.add("active");
          });
        });

        // Internal tab switching for PCA & Clustering inside "models" section
        const tabButtons = document.querySelectorAll(".tab-btn");
        const tabContents = document.querySelectorAll(".model-content");

        tabButtons.forEach((button) => {
          button.addEventListener("click", function () {
            tabButtons.forEach((btn) => btn.classList.remove("active"));
            tabContents.forEach((content) =>
              content.classList.remove("active")
            );

            this.classList.add("active");
            const target = this.getAttribute("data-target");
            document.getElementById(target).classList.add("active");
          });
        });
      });
    </script>
  </body>
</html>
