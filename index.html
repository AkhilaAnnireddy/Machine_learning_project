<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Overflowytics</title>
    <link rel="stylesheet" href="./Website/style.css" />

    <!-- Google Font -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600&display=swap"
      rel="stylesheet"
    />
  </head>
  <body>
    <!-- Navbar -->
    <nav class="navbar">
      <div class="title">
        <img
          src="Website/assets/stack_logo.png"
          alt="Stack Overflow Icon"
          class="navbar-icon"
        />
        Overflowytics
      </div>
      <ul class="menu">
        <li><a href="#" data-target="home">Home</a></li>
        <li><a href="#" data-target="introduction">Introduction</a></li>
        <li><a href="#" data-target="data-prep">Data Prep</a></li>
        <li><a href="#" data-target="models">Models</a></li>
        <li><a href="#" data-target="conclusion">Conclusion</a></li>
        <li><a href="#" data-target="aboutme">About Me</a></li>
      </ul>
    </nav>

    <!-- Page Content -->
    <div class="content">
      <!-- Home Section -->
      <section id="home" class="page-section active">
        <h1>Welcome to Stack Overflow User Engagement Analysis</h1>

        <p>
          Stack Overflow has been a fundamental platform for programmers
          worldwide, fostering a knowledge-sharing ecosystem that helps
          developers troubleshoot, learn, and collaborate. However, with the
          rise of AI-driven tools like ChatGPT, the way users engage with Stack
          Overflow is evolving. This project leverages Machine Learning (ML) to
          explore user behavior trends, engagement patterns, and the impact of
          AI on community-driven learning.
        </p>

        <div class="highlight">
          <h2>About This Project</h2>
          <p>
            The goal of this project is to analyze Stack Overflow data using
            data science and machine learning models to uncover trends in user
            participation, reputation growth, and question-answer interactions.
          </p>
          <p>
            Over the course of this study, various ML models will be applied to
            gain insights into what drives user engagement, how AI influences
            participation, and which factors impact long-term user retention.
          </p>
        </div>

        <div class="section">
          <h2>Why This Study Matters</h2>
          <p>
            Stack Overflow has traditionally thrived on human-generated content,
            where developers contribute questions and answers to help others.
            However, AI-generated solutions are changing user behavior, leading
            to fewer questions being posted, altered response patterns, and a
            potential decline in community engagement. This project aims to:
          </p>
          <ul>
            <li>Understand how user engagement has shifted over time.</li>
            <li>
              Analyze how AI tools impact the frequency and quality of
              interactions.
            </li>
            <li>Predict user retention based on engagement trends.</li>
            <li>
              Identify which topics and tags are most affected by AI-generated
              responses.
            </li>
            <li>
              Explore ways to enhance Stack Overflow's long-term sustainability.
            </li>
          </ul>
        </div>

        <div class="section">
          <h2>What to Expect</h2>
          <p>
            This study will involve implementing multiple machine learning
            models to analyze the Stack Overflow dataset. The findings will help
            answer key questions such as:
          </p>
          <ul>
            <li>
              Which user types (experienced vs. new users) contribute the most?
            </li>
            <li>What factors drive high-reputation growth?</li>
            <li>Which tags and topics receive the most engagement?</li>
            <li>How do user activity levels change over time?</li>
            <li>Are there patterns in unanswered or low-quality questions?</li>
            <li>How do AI-generated responses affect user behavior?</li>
          </ul>
          <p>
            As the project progresses, various machine learning techniques such
            as Association Rule Mining (ARM), Principal Component Analysis
            (PCA), and predictive modeling will be applied to extract meaningful
            insights.
          </p>
        </div>

        <div class="highlight">
          <h2>Project Scope</h2>
          <p>
            This project will evolve over time, implementing multiple machine
            learning models to analyze user interactions on Stack Overflow. The
            findings will be shared and updated progressively.
          </p>
          <p>
            The goal is to help data scientists, developers, and platform
            administrators better understand user engagement trends and predict
            the future of knowledge-sharing platforms in an AI-driven world.
          </p>
        </div>
      </section>
      <!-- Introduction Section -->
      <section id="introduction" class="page-section">
        <h1>Stack Overflow: A Pillar of the Developer Community</h1>
        <p>
          Stack Overflow is the world's largest and most prominent platform
          designed for developers and programmers to learn, share knowledge, and
          collaborate. Established in 2008 by Jeff Atwood and Joel Spolsky, the
          platform was created to address critical challenges faced by
          developers seeking accurate and timely programming solutions.
          Operating as a question-and-answer forum, Stack Overflow facilitates
          peer-reviewed responses to technical queries across a broad spectrum
          of programming languages, frameworks, and tools. By fostering
          high-quality knowledge exchange, the platform builds a credibility
          system where users earn reputation points and achievement-based badges
          for their contributions, enhancing community trust and engagement.
        </p>
        <img
          src="Website/assets/stack_iver_flow_logo1.jpeg"
          alt="Stack Overflow Logo"
          class="profile-image"
        />
        <p>
          With over 22 million questions and hundreds of millions of monthly
          users, Stack Overflow has become a cornerstone of the global software
          development ecosystem. The platform empowers developers to solve
          complex problems efficiently while supporting continuous learning and
          technical growth. Many professionals rely on Stack Overflow as their
          first point of reference when debugging issues, exploring new
          technologies, or improving their skills. Its mission to "help
          developers write the script of the future" underscores its
          significance in driving technological innovation and progress in the
          industry.
        </p>
        <h1>The Impact of AI Tools on Stack Overflow</h1>
        <p>
          The rapid rise of AI tools, particularly generative models such as
          ChatGPT, has introduced significant challenges to Stack Overflow's
          operational model. These AI tools provide instant, conversational
          responses to programming-related questions, disrupting traditional
          forums by reducing the need for users to post questions and await
          peer-reviewed answers. Consequently, the platform has experienced a
          noticeable decline in user activity and engagement. Research indicates
          a 16% reduction in weekly posts soon after the release of ChatGPT,
          suggesting that developers are increasingly turning to AI for quick
          solutions rather than contributing to Stack Overflow's knowledge base.
        </p>
        <p>
          This decline has directly affected the core pillars of the platform:
          user-generated content, community discussions, and expert
          contributions. Many experienced users who previously provided
          high-quality answers have reduced their activity on the platform,
          resulting in a diminished pool of expertise. This impacts both the
          diversity and depth of available solutions, ultimately threatening the
          collaborative knowledge-sharing ecosystem upon which Stack Overflow
          thrives.
        </p>
        <p>
          Additionally, the increased presence of AI-generated responses on
          Stack Overflow itself has presented further challenges. Moderation
          teams have reported a surge in low-quality answers generated by AI
          tools. While these responses may appear accurate on the surface, they
          often lack context or detailed reasoning, placing a greater burden on
          moderators to filter and maintain the platform's content quality.
        </p>
        <p>
          According to Stack Overflow's 2024 Developer Survey, 76% of developers
          are actively using or considering AI tools to aid their programming
          tasks. Despite the growing adoption of AI, 43% of respondents
          expressed concerns regarding the reliability of AI-generated answers,
          particularly in complex or nuanced scenarios. Nonetheless, developers
          continue to favor AI for its speed and convenience, further reducing
          reliance on community-driven solutions.
        </p>
        <img
          src="Website/assets/sto_vs_chatpgt.webp"
          alt="Impact of AI on Stack Overflow"
          class="profile-image"
        />
        <h1>Project Motivation</h1>
        <p>
          The motivation behind this project is to analyze Stack Overflow's user
          data and activity patterns using machine learning techniques. By
          identifying key trends, including shifts in user engagement and the
          influence of AI tools, the project aims to uncover factors that impact
          the platform's long-term sustainability. Understanding these patterns
          can provide actionable insights to help Stack Overflow strategize for
          future growth, improve user retention, and maintain its position as a
          vital resource for the global developer community.
        </p>
        <h1>Objectives</h1>
        <p>
          The objective of this project is to derive insights into user
          engagement and activity trends on Stack Overflow by leveraging data
          analysis and machine learning techniques. The study focuses on
          exploring and addressing key research questions, including patterns of
          user participation, reputation dynamics, content interaction, and the
          influence of external factors like AI tools. These insights aim to
          support the development of strategies to enhance user retention,
          improve content quality, and sustain long-term platform growth.
        </p>
        <ol>
          <li>
            1. Which user types (e.g., experienced, new, or moderators) are most
            active on Stack Overflow?
          </li>
          <li>2. What factors influence user reputation growth over time?</li>
          <li>
            3. Which tags or topics lead to the highest engagement (views,
            upvotes, answers)?
          </li>
          <li>
            4. How does user activity (e.g., number of posts, last access date)
            change over time?
          </li>
          <li>
            5. What types of users are more likely to leave the platform (low
            engagement, declining activity, etc.)?
          </li>
          <li>
            6. Which types of questions (based on topic, length, or complexity)
            are most likely to receive answers?
          </li>
          <li>
            7. How does the time of posting (day of the week, time of day)
            impact the likelihood of receiving quick responses?
          </li>
          <li>
            8. What are the common characteristics of highly upvoted or highly
            answered questions?
          </li>
          <li>
            9. How do AI tools influence the frequency and type of questions
            being asked?
          </li>
          <li>
            10. Which tags or topics have seen a decline in user engagement due
            to the rise of AI tools?
          </li>
          <li>
            11. What user behaviors are predictive of long-term retention and
            active participation on the platform?
          </li>
          <li>
            12. Which regions or locations (if location data is available) have
            the highest or lowest participation rates on Stack Overflow?
          </li>
          <li>
            13. What factors (e.g., reputation changes, badges earned) predict
            whether a user will become a top contributor?
          </li>
          <li>
            14. How can Stack Overflow tailor its platform to encourage users to
            stay engaged despite competition from AI tools?
          </li>
          <li>
            15. How might user activity and engagement trends evolve over the
            next few years given the current usage patterns and the increasing
            adoption of AI-based coding tools?
          </li>
        </ol>
      </section>

      <!-- Data Prep Section -->
      <section id="data-prep" class="page-section">
        <h1>Data Collection</h1>
        <p>
          Effective data collection is crucial in any data-driven project,
          particularly when using machine learning to analyze trends and make
          predictions. Reliable and high-quality data enables accurate model
          training, which, in turn, provides better insights and predictions. In
          this project, user data from Stack Overflow was collected to analyze
          user engagement, activity trends, and platform growth patterns. Since
          the platform is large and continuously evolving, accessing up-to-date
          data through manual extraction would be inefficient and error-prone.
          Instead, APIs (Application Programming Interfaces) provide a
          systematic way to collect structured data in real-time.
        </p>
        <p>
          APIs allow automated access to vast datasets while ensuring that data
          retrieval follows the platform's usage policies and structure. By
          using Stack Exchange's API, this project leveraged a reliable method
          to retrieve relevant user information, including reputation, location,
          activity dates, and earned badges. This API offers endpoints that
          provide data in a machine-readable JSON format, making it ideal for
          large-scale data collection and processing.
        </p>
        <p>
          This automated approach ensures that the data collection process is
          efficient, scalable, and capable of handling large volumes of user
          data from Stack Overflow. By using APIs, the project can be easily
          updated with fresh data in the future, enabling continuous analysis
          and improvement of machine learning models.
        </p>
        <p>
          The script used for data collection follows a structured approach:
        </p>
        <ul>
          <li>
            <strong>API Request and Pagination:</strong>
            The script builds API requests to fetch user data in batches of 100
            profiles per request. It supports pagination to iterate through
            multiple pages of data, starting from page 1 and continuing until
            all available data is retrieved.
          </li>
          <li>
            <strong>Data Saving:</strong>
            The response data from each API call is saved both as a JSON file
            and a CSV file for analysis. The JSON file stores the complete API
            response, preserving all details, while the CSV file organizes key
            fields in tabular format.
          </li>
          <li>
            <strong>Dynamic Extraction:</strong>
            The script dynamically extracts important fields, including user ID,
            display name, reputation, location, profile image, and badge counts.
            It also compiles information about collectives (groups or
            communities) the users may belong to.
          </li>
          <li>
            <strong>Folder and File Organization:</strong>
            To keep the collected data organized, the script creates separate
            folders for JSON and CSV files. Each file is named using a timestamp
            and page number, ensuring easy identification and retrieval.
          </li>
          <li>
            <strong>Rate Limiting:</strong>
            To prevent exceeding the API's request limits, the script includes a
            short delay between requests. This helps maintain compliance with
            API usage guidelines while avoiding service interruptions.
          </li>
          <li>
            <strong>Error Handling:</strong>
            The script verifies the success of each API call by checking the
            HTTP status code. If a request fails, an error message is displayed,
            and the data collection loop terminates gracefully. The loop also
            ends when there are no more pages of data to fetch.
          </li>
        </ul>
        <img
          src="Website/assets/api_data_extrcation_log.png"
          alt="extraction log"
          class="profile-image"
        />
        <p>
          Stack Exchange API Documentation -
          <a href="https://api.stackexchange.com/docs" target="_blank"
            >Link to API details</a
          >
        </p>
        <p>
          Python Script for Data Extraction -
          <a
            href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/stackexchange_data_extraction.py"
            target="_blank"
            >Data Extraction Script</a
          >
        </p>
        <h1>Data Preparation</h1>
        <p>
          After extracting the data from the Stack Exchange API, the next
          crucial step was data preparation. Since the API returned multiple CSV
          files, each containing a batch of user records, a Python script was
          developed to merge these individual files into a single dataset. The
          script scans a directory containing the CSV files, reads each file
          into a Pandas DataFrame, and then concatenates all DataFrames into one
          comprehensive dataset. This merged file, containing approximately
          100,000 records, provides a centralized and organized dataset that
          will be used for further analysis and machine learning model training.
        </p>
        <p>
          This step ensures that all relevant data is consolidated, allowing for
          efficient analysis without the need to repeatedly access or process
          multiple smaller files. Proper data preparation is essential to
          eliminate redundancy, handle errors, and structure data in a way that
          optimizes subsequent data exploration and model development.
        </p>
        <p>
          Python Script for Data Preparation -
          <a
            href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/data_merging_script.py"
            target="_blank"
            >Data Preparation Script</a
          >
        </p>
        <h1>Data Cleaning</h1>
        <h3>Importance of Data Cleaning</h3>
        <p>
          Data cleaning is an essential step in preparing data for analysis and
          model training. Real-world data often contains missing values,
          inconsistencies, and errors, which can lead to unreliable and
          inaccurate insights. Unclean data negatively impacts the performance
          of machine learning models, leading to bias and misleading results. By
          thoroughly cleaning the data, we ensure its quality and reliability,
          allowing models to make accurate predictions. Effective data cleaning
          helps to minimize errors, improves model performance, and provides
          meaningful insights during data analysis.
        </p>
        <p>
          The data cleaning process began with analyzing the dataset structure,
          including columns, data types, and sample data points. Both
          categorical and numerical fields were reviewed to identify data points
          requiring further cleaning. Missing values were assessed by
          calculating the percentage of missing entries per column. Columns with
          excessively high missing values, such as collective_names, were
          dropped due to limited analytical value. Columns with moderate missing
          data, such as website_url and accept_rate, were handled appropriately.
          For numerical columns like accept_rate, missing values were filled
          with the median. Missing values in categorical columns like
          display_name were replaced with a unique string pattern,
          username_{user_id}. Duplicate records were identified and removed to
          ensure data integrity. Derived features were added to enhance
          analytical capabilities, including account_age_years, calculated from
          the creation_date column to analyze user engagement trends over time.
          Data type conversions were performed to standardize formats. Date
          fields such as creation_date, last_modified_date, and last_access_date
          stored in epoch format were converted to datetime format for
          time-based analysis. These steps resulted in a clean, structured
          dataset, free of missing values, duplicates, and data type
          inconsistencies. The refined dataset is prepared for exploratory
          analysis and machine learning model training.
        </p>
        <h3>Data Attributes from Raw Data</h3>
        <img
          class="profile-image"
          src="Website/assets/data columns.png"
          alt="data columns"
        />
        <h3>Merged Raw Data</h3>
        <img
          class="profile-image"
          src="Website/assets/merged_raw_data.png"
          alt="data columns"
        />
        <h3>Cleaned Data</h3>
        <img
          class="profile-image"
          src="Website/assets/cleaned_data.png"
          alt="data columns"
        />
        <p>
          Python Code for Data Cleaning -
          <a
            href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Datacleaning/Data_cleaning.ipynb"
            target="_blank"
            >Data Cleaning Code</a
          >
        </p>
        <p>
          Raw Data -
          <a
            href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/merged_users.csv"
            target="_blank"
            >Link to Raw Data</a
          >
        </p>
        <p>
          Cleaned Data -
          <a
            href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/cleaned_user_data.csv"
            target="_blank"
            >Lint to Cleaned Data</a
          >
        </p>
        <h1>Data Visualization</h1>
        <h4>Visualization 1: Total Badge Count by Type</h4>
        <img
          class="profile-image"
          src="Website/assets/Total_Badge_Count_by_Type.png"
          alt="Visualisation-1"
        />
        <ol>
          <li>
            - <b>Distribution Shape:</b> The badge distribution shows that
            bronze badges are the most common, followed by silver, while gold
            badges are comparatively rare.
          </li>
          <li>
            - <b>Central Tendency:</b> The count difference between badge types
            suggests that users are more frequently awarded lower-tier badges.
          </li>
          <li>
            - <b>Spread:</b> The spread is significant between the badge types,
            with a substantial gap between bronze and gold.
          </li>
          <li>
            - <b>Outliers:</b> There are no evident outliers since the count of
            each badge type follows the expected hierarchical structure of
            difficulty and rarity.
          </li>
          <li>
            - <b>Overall Interpretation:</b> The visualization indicates that
            while users are highly rewarded with bronze and silver badges, gold
            badges remain exclusive to top contributors or achievements.
          </li>
        </ol>

        <h4>Visualization 2: Reputation Distribution</h4>
        <img
          class="profile-image"
          src="Website/assets/Reputation_Distribution.png"
          alt="Visualisation-2"
        />
        <ol>
          <li>
            - <b>Distribution Shape:</b> The reputation distribution is heavily
            right-skewed, with most users having low reputation scores.
          </li>
          <li>
            - <b>Central Tendency:</b> The majority of users have a reputation
            close to zero, indicating a high concentration near the lower end.
          </li>
          <li>
            - <b>Spread:</b> The range of reputation values extends to over 1.4
            million, showing a wide disparity between users with low and high
            reputation.
          </li>
          <li>
            - <b>Outliers:</b> A few users have extremely high reputations,
            representing outliers with significant contributions or
            long-standing presence.
          </li>
          <li>
            - <b>Overall Interpretation:</b> The reputation distribution
            highlights that while a small subset of users is highly reputed, the
            majority are new or infrequent contributors to the platform.
          </li>
        </ol>

        <h4>Visualization 3: Account Age vs Reputation</h4>
        <img
          class="profile-image"
          src="Website/assets/Account_Age_vs_Reputation.png"
          alt="Visualisation-3"
        />
        <ol>
          <li>
            - <b>Visualization:</b> A scatterplot representing the relationship
            between account age (in years) and user reputation, with points
            plotted in a light red color.
          </li>
          <li>
            - <b>Insight:</b> The plot shows a positive correlation, where older
            accounts tend to have higher reputations. However, many users with
            older accounts still maintain low reputation scores, indicating that
            active contribution over time is necessary to build reputation,
            rather than account longevity alone.
          </li>
        </ol>
        <h4>Visualization 4: Reputation Change Over the Year</h4>
        <img
          class="profile-image"
          src="Website/assets/Reputation_Change_Over_the_Year.png"
          alt="Visualisation-4"
        />
        <ol>
          <li>
            - <b>Distribution Shape:</b> The annual reputation change
            distribution is highly skewed, with most users showing minimal to
            zero change in reputation.
          </li>
          <li>
            - <b>Central Tendency:</b> The majority of reputation changes are
            centered near zero, indicating little or no yearly variation for
            most users.
          </li>
          <li>
            - <b>Spread:</b> The reputation change values span a broad range,
            with a few users experiencing significant positive or negative
            changes.
          </li>
          <li>
            - <b>Outliers:</b> A small number of users have large positive
            reputation increases, representing exceptional activity or
            contributions within a year.
          </li>
          <li>
            - <b>Overall Interpretation:</b> This distribution suggests that
            while most users do not experience major changes in reputation
            annually, a select few have substantial fluctuations driven by their
            activity levels on the platform.
          </li>
        </ol>
        <h4>Visualization 5: Badge Counts by User</h4>
        <img
          class="profile-image"
          src="Website/assets/Badge_Counts_by_User.png"
          alt="Visualisation-5"
        />
        <ol>
          <li>
            - <b>Distribution Shape:</b> The distribution of total badges per
            user is highly skewed, with the majority of users having very few
            badges.
          </li>
          <li>
            - <b>Central Tendency:</b> Most users have a total badge count close
            to zero, indicating minimal recognition or contributions.
          </li>
          <li>
            - <b>Spread:</b> The badge count ranges from 0 to over 10,000,
            demonstrating wide variability in user achievements on the platform.
          </li>
          <li>
            - <b>Outliers:</b> A few users have extremely high badge counts,
            representing highly active and accomplished contributors.
          </li>
          <li>
            - <b>Overall Interpretation:</b> The badge distribution indicates
            that while a select group of users earn significant recognition
            through badges, the vast majority engage less frequently or with
            fewer impactful contributions.
          </li>
        </ol>
        <h4>Visualization 6: User Count by Account Age Range</h4>
        <img
          class="profile-image"
          src="Website/assets/User_Count_by_Account _Age_Range.png"
          alt="Visualisation-6"
        />
        <ol>
          <li>
            - <b>Distribution Shape:</b> The distribution shows a peak in user
            count for accounts aged between 12 to 15 years, with a steady
            decrease in older and newer accounts.
          </li>
          <li>
            - <b>Central Tendency:</b> The majority of users fall within the 10
            to 18-year age groups, suggesting a high concentration of users with
            long-standing accounts.
          </li>
          <li>
            - <b>Spread:</b> The user base is unevenly spread, with fewer users
            in the less than 1-year and greater than 20-year account age ranges.
          </li>
          <li>
            - <b>Outliers:</b> The newest accounts (less than 1 year old) and
            the oldest accounts (over 20 years old) have minimal user
            representation.
          </li>
          <li>
            - <b>Overall Interpretation:</b> The platform maintains a strong
            core of users with significant account longevity, reflecting
            long-term engagement and retention trends.
          </li>
        </ol>
        <h4>Visualization 7: Distribution of Accept Rate</h4>
        <img
          class="profile-image"
          src="Website/assets/Distribution_of_Accept_Rate.png"
          alt="Visualisation-7"
        />
        <ol>
          <li>
            - <b>Distribution Shape:</b> The accept rate distribution shows a
            right-skewed pattern, with a peak around the 80% mark.
          </li>
          <li>
            - <b>Central Tendency:</b> A significant portion of users have an
            accept rate between 70% and 90%, indicating a strong tendency toward
            high acceptance rates.
          </li>
          <li>
            - <b>Spread:</b> Accept rates vary widely, ranging from 0% to 100%,
            though lower acceptance rates are much less frequent.
          </li>
          <li>
            - <b>Outliers:</b> Very low accept rates near 0% have minimal
            representation in the dataset.
          </li>
          <li>
            - <b>Overall Interpretation:</b> The graph highlights that the
            majority of users maintain a high accept rate, suggesting consistent
            user engagement with answers received on the platform.
          </li>
        </ol>
        <h4>Visualization 8: Reputation vs Accept Rate</h4>
        <img
          class="profile-image"
          src="Website/assets/Reputation_vs_Accept_Rate.png"
          alt="Visualisation-8"
        />
        <ol>
          <li>
            - <b>Visualization:</b> A scatterplot showing the relationship
            between 'Reputation' and 'Accept Rate', with data points distributed
            across varying values of both variables.
          </li>
          <li>
            - <b>Insight:</b> The plot reveals a positive trend where higher
            accept rates tend to be associated with higher reputations. However,
            there is a significant concentration of users with low reputation
            across all accept rate levels, indicating that reputation growth
            might require consistent high acceptance over time or other factors.
          </li>
        </ol>
        <h4>Visualization 9: Box Plot of Reputation by User Type</h4>
        <img
          class="profile-image"
          src="Website/assets/Box Plot_of_Reputation_by_User_Type.png"
          alt="Visualisation-9"
        />
        <ol>
          <li>
            - <b>Visualization:</b> A box plot showing the distribution of user
            reputation categorized by user types: moderator, registered, and
            unregistered.
          </li>
          <li>
            - <b>Insight:</b> Registered users show a wide spread in reputation
            with many outliers, including extremely high reputations. Moderators
            have a relatively consistent reputation range with fewer outliers,
            while unregistered users have minimal reputation variations.
          </li>
        </ol>
        <h4>Visualization 10: Most Recent User Access Dates</h4>
        <img
          class="profile-image"
          src="Website/assets/Most_Recent_User_ Access_Dates.png"
          alt="Visualisation-10"
        />
        <ol>
          <li>
            - <b>Visualization:</b> A line plot illustrating the number of users
            accessing the platform over time in January 2025.
          </li>
          <li>
            - <b>Insight:</b> There is a steady, minimal user access trend until
            late January, followed by a sharp spike around January 29th,
            potentially due to a major event or system update, before the user
            count declines quickly after.
          </li>
        </ol>
        <p>
          Python Code for Data Visualization -
          <a
            href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Datacleaning/Data_cleaning.ipynb"
            target="_blank"
            >Data Visualization Code</a
          >
        </p>
        <p>
          Github Repository -
          <a
            href="https://github.com/AkhilaAnnireddy/Machine_learning_project"
            target="_blank"
            >Link to github code</a
          >
        </p>
      </section>

      <!-- Models Section -->
      <section id="models" class="page-section">
        <!-- Internal Tabs for PCA and Clustering -->
        <div class="internal-tabs">
          <button class="tab-btn active" data-target="pca">PCA</button>
          <button class="tab-btn" data-target="clustering">Clustering</button>
          <button class="tab-btn" data-target="arm">ARM</button>
          <button class="tab-btn" data-target="naivebayes">Naive Bayes</button>
          <button class="tab-btn" data-target="decisiontree">
            Decision Tree
          </button>
          <button class="tab-btn" data-target="Regression">Regression</button>
          <button class="tab-btn" data-target="svm">SVM</button>
          <button class="tab-btn" data-target="Ensemble">
            Ensemble Methods
          </button>
          <button class="tab-btn" data-target="train-test-split">
            Train-Test-Split
          </button>
        </div>

        <!-- PCA Content -->
        <div id="pca" class="model-content active">
          <h1>Overview</h1>

          <h2>Principal Component Analysis (PCA)</h2>
          <p>
            Principal Component Analysis (PCA) is a widely used technique for
            dimensionality reduction in data analysis and machine learning. It
            simplifies high-dimensional datasets by transforming them into a
            smaller set of meaningful features while preserving most of the
            original information. PCA works by identifying the directions,
            called <strong>principal components</strong>, along which the data
            varies the most. By projecting the dataset onto these new axes, PCA
            reduces redundancy, improves computational efficiency, and makes
            pattern analysis easier.
          </p>
          <p>
            PCA is particularly beneficial when datasets contain
            <strong>correlated features</strong> that add complexity without
            providing additional insights. By converting the original variables
            into
            <strong
              >a smaller number of uncorrelated principal components</strong
            >, PCA ensures that only the most important patterns are retained.
            This technique is widely applied in
            <strong
              >image processing, finance, genetics, and recommendation
              systems</strong
            >, where high-dimensional data can be challenging to interpret
            effectively.
          </p>

          <h2>Principal Components</h2>
          <p>
            Principal components are the <strong>new features</strong> created
            by PCA that capture the <strong>maximum variance</strong> in the
            dataset.
          </p>
          <ul>
            <li>
              <strong>First Principal Component (PC1):</strong> Represents the
              direction with the most variation, retaining the highest amount of
              information from the original data.
            </li>
            <li>
              <strong>Second Principal Component (PC2):</strong> Captures the
              second-highest variance while remaining uncorrelated with the
              first.
            </li>
            <li>
              <strong>Subsequent Components:</strong> Each additional component
              captures progressively less variance, ensuring that only the most
              essential structure of the dataset is retained.
            </li>
          </ul>
          <p>
            By selecting only the <strong>top principal components</strong>, PCA
            reduces the number of dimensions while preserving essential
            patterns, making analysis more efficient and interpretable.
          </p>

          <h2>Variance Explained by Principal Components</h2>
          <p>
            To determine how many components are needed,
            <strong>variance explained plots</strong> are used:
          </p>

          <h3>Scree Plot (Variance Explained per Component)</h3>
          <img
            class="profile-image"
            src="Website/assets/cumulative_variance_2d_pca.png"
            alt="Scree Plot"
          />
          <p>
            The scree plot shows how much variance each principal component
            captures. The
            <strong>first few components retain most of the variance</strong>,
            while additional components contribute less.
          </p>

          <h3>Cumulative Variance Plot</h3>
          <img
            class="profile-image"
            src="Website/assets/cummulative_variance_vs_components.png"
            alt="Cumulative Variance Plot"
          />
          <p>
            This graph indicates the
            <strong>cumulative variance retained</strong> as more components are
            added. In this project, the
            <strong
              >first X principal components capture at least 95% of the
              variance</strong
            >, making them sufficient for further analysis.
          </p>

          <h2>Why PCA?</h2>
          <p>
            In this project, PCA is applied to analyze
            <strong>user engagement patterns on Stack Overflow</strong> by
            <strong>reducing the dataset’s dimensionality</strong> while
            preserving crucial information. The dataset contains
            <strong
              >user reputation, badge counts, accept rates, and activity
              levels</strong
            >, which are often <strong>highly correlated</strong>. Direct
            analysis of such high-dimensional data is both computationally
            expensive and difficult to visualize.
          </p>
          <p>
            By applying PCA, the dataset is transformed into a smaller set of
            principal components that
            <strong>retain the highest variance</strong>, helping to
            <strong>remove redundancy and improve efficiency</strong>. This
            allows for
            <strong
              >better interpretation of key factors driving user
              engagement</strong
            >
            while filtering out less significant features.
          </p>

          <h2>How PCA Improves This Project</h2>
          <p>The main goal of PCA in this project is to:</p>
          <ul>
            <li>
              <strong>Enhance Data Visualization:</strong> Reducing dimensions
              makes user engagement patterns easier to visualize.
            </li>
            <li>
              <strong>Improve Model Performance:</strong> Selecting the most
              <strong>informative</strong> components helps refine predictions.
            </li>
            <li>
              <strong>Optimize Feature Selection:</strong> Identifies the most
              relevant attributes without unnecessary complexity.
            </li>
            <li>
              <strong>Increase Computational Efficiency:</strong> Reducing data
              dimensions speeds up analysis and reduces noise.
            </li>
          </ul>

          <h1>Data Preprocessing</h1>
          <p>
            Preprocessing is a crucial step before applying Principal Component
            Analysis (PCA) to ensure that the dataset is properly formatted for
            dimensionality reduction. PCA works by identifying the principal
            components that capture the most variance in the data, but if the
            dataset is not cleaned and standardized, the results can be
            misleading. Features with larger numerical values may dominate the
            analysis, while missing or inconsistent data can introduce errors.
            To ensure that PCA provides meaningful and reliable insights,
            several preprocessing steps are performed, including removing
            non-numeric data, handling missing values, and standardizing the
            dataset.
          </p>
          <p>
            Since PCA relies on variance and covariance calculations, it can
            only process numerical data. Therefore, any categorical or
            text-based columns are removed to ensure that PCA operates
            correctly. Additionally, PCA is sensitive to differences in scale,
            meaning that features with varying magnitudes—such as "reputation"
            in thousands and "badge counts" in single digits—could
            disproportionately influence the principal components. To prevent
            this, standardization is applied using StandardScaler from
            sklearn.preprocessing, which transforms all features to have a mean
            of 0 and standard deviation of 1, ensuring equal contribution to the
            PCA transformation.
          </p>
          <p>
            Another critical preprocessing step is handling missing values, as
            incomplete data can distort variance calculations and affect the
            reliability of principal components. The dataset is checked for
            missing values using isnull().sum(), and missing data is either
            removed or imputed using statistical techniques like the mean or
            median to maintain consistency. Once the dataset is fully cleaned
            and standardized, it is converted back into a structured pandas
            DataFrame, making it easier to interpret and apply PCA effectively.
            These preprocessing steps ensure that PCA successfully reduces
            dimensionality while retaining the most essential information,
            leading to more efficient and insightful data analysis.
          </p>
          <h3>Raw Data</h3>
          <img
            class="profile-image"
            src="Website/assets/cleaned_data.png"
            alt="data columns"
          />
          <h3>Cleaned Data for PCA</h3>
          <img
            class="profile-image"
            src="Website/assets/cleaned_data_for_pca.png"
            alt="data columns"
          />
          <p>
            Code for preproccesing -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/pca_model_impl.ipynb"
              target="_blank"
              >PCA Data Preproccesing Code</a
            >
          </p>
          <p>
            Link to raw dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/cleaned_user_data.csv"
              target="_blank"
              >Raw Dataset</a
            >
          </p>
          <p>
            Link to proccessed dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/user_data_cleaned_for_pca.csv"
              target="_blank"
              >Processed Dataset</a
            >
          </p>

          <h1>Implementation</h1>
          <p>
            Principal Component Analysis (PCA) was applied to the dataset to
            reduce dimensionality while retaining the most important
            information. The dataset initially contained multiple numerical
            features, making it complex to visualize and analyze directly. PCA
            was performed twice—once with two principal components (2D PCA) and
            once with three principal components (3D PCA)—to compare how much
            variance is preserved in each transformation.
          </p>
          <h3>PCA with 2 Components</h3>
          <ul>
            <li>
              - The data was projected onto two principal components to
              visualize it in <b>2D space</b>.
            </li>
            <li>
              - The <b>variance retained</b> in this transformation was
              <b>69.17%</b>, meaning that the two components capture most of the
              important structure in the dataset but still leave out about
              <b>30.83%</b> of the information.
            </li>
            <li>
              - The <b>2D scatter plot</b> shows a
              <b>dense clustering of points</b>, with some outliers indicating
              potential variations or distinct patterns in the data.
            </li>
          </ul>

          <h3>PCA with 3 Components</h3>
          <ul>
            <li>
              - The data was then projected onto
              <b>three principal components</b> for a <b>3D representation</b>.
            </li>
            <li>
              - This transformation retained <b>78.60% of the variance</b>,
              suggesting that adding one more component helps capture an
              additional <b>9.43%</b> of the dataset’s variability.
            </li>
            <li>
              - The <b>3D scatter plot</b> provides a clearer separation of data
              points, indicating that higher dimensions may help in retaining
              more meaningful patterns.
            </li>
          </ul>

          <h3>Visualization for PCA with 2 Components</h3>
          <img
            class="profile-image"
            src="Website/assets/pca_with_2c.png"
            alt="data columns"
          />
          <h4>Observations</h4>
          <ul>
            <li>
              <b>Variance Retention:</b> Captures <b>69.17%</b> of the variance,
              losing <b>30.83%</b> of the dataset’s information.
            </li>
            <li>
              <b>Data Clustering:</b> Points are <b>densely packed</b>,
              indicating that PC1 and PC2 capture major patterns.
            </li>
            <li>
              <b>Outliers:</b> A few <b>distant points</b> suggest potential
              anomalies or distinct patterns.
            </li>
            <li>
              <b>Data Spread:</b> PC1 explains the <b>most variance</b>, while
              PC2 adds minimal new information.
            </li>
            <li>
              <b>Limitation:</b> Loss of variance may <b>oversimplify</b> data
              relationships.
            </li>
          </ul>
          <h3>Visualization for PCA with 3 Components</h3>
          <img
            class="profile-image"
            src="Website/assets/pca_with_3c.png"
            alt="data columns"
          />
          <h4>Observations</h4>
          <ul>
            <li>
              <b>Variance Retention:</b> Preserves <b>78.60%</b> of variance,
              capturing <b>9.43% more information</b> than 2D PCA.
            </li>
            <li>
              <b>Better Separation:</b> Points are <b>more evenly spread</b>,
              meaning PC3 contributes valuable structural details.
            </li>
            <li>
              <b>Outliers:</b> Still present, but <b>better distributed</b>,
              requiring further analysis.
            </li>
            <li>
              <b>Data Depth:</b> PC3 <b>reduces variance loss</b>, making
              patterns more distinguishable.
            </li>
            <li>
              <b>Advantage:</b> <b>Improved representation</b> over 2D, but
              higher dimensions may still be needed for
              <b>95% variance retention</b>.
            </li>
          </ul>

          <h3>Visualization for Cumulative variance retained in 2D PCA</h3>
          <img
            class="profile-image"
            src="Website/assets/cumulative_variance_2d_pca.png"
            alt="data columns"
          />
          <h4>Observations</h4>
          <ul>
            <li>
              <strong>First Component Dominance:</strong> The first principal
              component captures around 45-50% of the variance.
            </li>
            <li>
              <strong>Cumulative Variance:</strong> Adding the second component
              increases total variance retention to 69.17%, reducing information
              loss.
            </li>
            <li>
              <strong>Diminishing Returns:</strong> The second component
              contributes less variance than the first, indicating reduced
              impact of additional components.
            </li>
            <li>
              <strong>Information Loss:</strong> 30.83% variance is lost,
              suggesting that higher dimensions may be needed for better data
              representation.
            </li>
          </ul>

          <h3>Visualization for Cumulative variance retained in 3D PCA</h3>
          <img
            class="profile-image"
            src="Website/assets/cumulative_variance_3d_pca.png"
            alt="data columns"
          />
          <h4>Observations</h4>
          <ul>
            <li>
              <strong>First Component Impact:</strong> The first principal
              component captures around 40-45% of the variance.
            </li>
            <li>
              <strong>Increased Retention:</strong> Adding the second component
              raises cumulative variance to 69.17%, similar to 2D PCA.
            </li>
            <li>
              <strong>Third Component Benefit:</strong> The third principal
              component improves variance retention to 78.60%, adding 9.43% more
              information over 2D PCA.
            </li>
            <li>
              <strong>Reduced Information Loss:</strong> Only 21.40% variance is
              lost, making 3D PCA a better representation compared to 2D PCA.
            </li>
          </ul>

          <h3>Optimal Number of Components for 95% Variance Retention</h3>
          <img
            class="profile-image"
            src="Website/assets/cummulative_variance_vs_components.png"
            alt="data columns"
          />
          <h4>Observations</h4>
          <p>
            To balance dimensionality reduction and information retention, PCA
            was used to determine how many components are needed to preserve at
            least <strong>95%</strong> of the dataset's variance. The analysis
            found that <strong>6 principal components</strong> achieve this
            goal.
          </p>
          <p>The cumulative variance plot visually represents this:</p>
          <ul>
            <li>
              <strong>The red dashed line</strong> marks the
              <strong>95% variance threshold</strong>.
            </li>
            <li>
              <strong>The blue vertical line</strong> shows that
              <strong>6 components</strong> are enough to retain most of the
              data’s structure.
            </li>
          </ul>
          <p>
            Using <strong>6 components</strong> simplifies the dataset while
            ensuring <strong>minimal information loss</strong>, making it
            efficient for further analysis.
          </p>

          <h3>Top three Eigen values</h3>
          <img
            class="profile-image"
            src="Website/assets/eigen_values_pca.png"
            alt="data columns"
          />
          <p>
            Eigenvalues represent the
            <strong>amount of variance</strong> captured by each principal
            component. The higher the eigenvalue, the more important the
            component is in explaining the dataset's variability.
          </p>
          <p>
            For this dataset, the <strong>top three eigenvalues</strong> are:
          </p>
          <ul>
            <li><strong>PC1:</strong> 5.75 (captures the highest variance)</li>
            <li><strong>PC2:</strong> 3.94 (adds significant variance)</li>
            <li>
              <strong>PC3:</strong> 1.32 (contributes less but still meaningful)
            </li>
          </ul>
          <p>
            The <strong>bar chart visualization</strong> highlights the
            difference in variance captured by each component.
            <strong>PC1 dominates</strong>, followed by <strong>PC2</strong>,
            while <strong>PC3 captures much less variance</strong>. This
            analysis helps in understanding how much information each component
            retains and guides the selection of an
            <strong>optimal number of dimensions</strong> for data
            representation.
          </p>
          <p>
            Link to Model Implementation -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/pca_model_impl.ipynb"
              target="_blank"
              >Model Implementation</a
            >
          </p>

          <h1>Results and Conclusion</h1>
          <h2>Results</h2>
          <p>
            The Principal Component Analysis (PCA) was applied to simplify the
            dataset while preserving its core information. The analysis provided
            key insights into the balance between dimensionality reduction and
            information retention.
          </p>

          <h3>Variance Retention in 2D PCA</h3>
          <ul>
            <li>
              <b>Two principal components</b> retained
              <b>69.17% of the total variance</b>.
            </li>
            <li>
              This allowed for <b>simplified visualization</b>, but resulted in
              a <b>30.83% information loss</b>, meaning some patterns were not
              fully captured.
            </li>
            <li>
              The <b>2D PCA scatter plot</b> highlights distinct clusters,
              though some overlap and outliers indicate missing variability.
            </li>
          </ul>

          <h3>Variance Retention in 3D PCA</h3>
          <ul>
            <li>
              Using <b>three principal components</b> increased variance
              retention to <b>78.60%</b>, improving information capture.
            </li>
            <li>
              This represents a <b>9.43% gain</b> compared to 2D PCA, making the
              data structure more distinct.
            </li>
            <li>
              The <b>3D PCA scatter plot</b> provides better separation of data
              points, revealing clearer patterns.
            </li>
            <li>
              However, <b>21.40% of variance remains unaccounted</b>, suggesting
              the need for more dimensions to fully represent the dataset.
            </li>
          </ul>

          <h3>Optimal Number of Components</h3>
          <ul>
            <li>
              <b>Cumulative variance analysis</b> shows that at least
              <b>six principal components</b> are required to retain
              <b>95% of the total variance</b>.
            </li>
            <li>
              This ensures the dataset’s key structure and relationships are
              <b>preserved while reducing complexity</b>.
            </li>
            <li>
              Using <b>six components</b> provides an optimal balance between
              <b>computational efficiency</b> and <b>data integrity</b>.
            </li>
          </ul>

          <h3>Eigenvalue Analysis</h3>
          <ul>
            <li>
              Eigenvalues represent the
              <b>importance of each principal component</b> in capturing
              variance:
            </li>
            <li>
              <b>PC1 (5.75)</b> captures the highest variance, making it the
              most influential.
            </li>
            <li><b>PC2 (3.94)</b> significantly improves representation.</li>
            <li>
              <b>PC3 (1.32)</b> holds meaningful information but contributes
              less.
            </li>
            <li>
              The <b>eigenvalue bar chart</b> visually confirms that
              <b>PC1 dominates</b>, followed by <b>PC2 and PC3</b>, reinforcing
              the importance of the first few components.
            </li>
          </ul>

          <h2>Conclusion</h2>
          <p>
            PCA effectively reduced the dataset's dimensionality while
            preserving its most significant features. The analysis demonstrated
            that:
          </p>
          <ul>
            <li>
              <b>2-component PCA</b> is useful for visualization but causes
              <b>substantial information loss</b>.
            </li>
            <li>
              <b>3-component PCA</b> provides better structure while still
              losing <b>21.40% of variance</b>.
            </li>
            <li>
              <b>6 principal components</b> retain at least
              <b>95% of the dataset's information</b>, making them ideal for
              analysis.
            </li>
            <li>
              <b>Eigenvalue analysis</b> confirms that the first few components
              capture most of the dataset's meaningful structure.
            </li>
          </ul>
          <p>
            By applying PCA, this project successfully
            <b>removed redundancy while maintaining interpretability</b>. The
            findings provide
            <b
              >valuable insights into user engagement patterns on Stack
              Overflow</b
            >, helping to <b>identify key features</b> that drive activity on
            the platform.
          </p>

          <p>
            Link to complete code -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/pca_model_impl.ipynb"
              target="_blank"
              >Complete Implementation</a
            >
          </p>
          <p>
            Github Repository -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project"
              target="_blank"
              >Link to github code</a
            >
          </p>
        </div>

        <!-- Clustering Content -->
        <div id="clustering" class="model-content">
          <h1>Overview</h1>
          <h2>Clustering</h2>
          <p>
            Clustering is an unsupervised machine learning technique used to
            group similar data points based on patterns and relationships.
            Unlike classification, clustering does not rely on predefined
            labels; instead, it identifies natural structures within the
            dataset. It is widely used in applications such as customer
            segmentation, anomaly detection, and pattern recognition. The three
            main types of clustering algorithms—K-Means (partition-based),
            Hierarchical Clustering (tree-based), and DBSCAN
            (density-based)—each have unique strengths and weaknesses depending
            on the dataset's characteristics. Clustering helps uncover hidden
            insights, simplifies data for further analysis, and enhances
            decision-making in various domains, from marketing to
            bioinformatics.
          </p>
          <h2>Role of Distance Metrics in Clustering</h2>
          <p>
            Distance metrics play a crucial role in clustering algorithms as
            they determine how similarity between data points is measured. The
            choice of distance metric impacts how clusters are formed and how
            well-separated they are. Commonly used distance metrics include:
          </p>
          <ul>
            <li>
              <b>Euclidean Distance:</b> Measures the straight-line distance
              between two points in a multi-dimensional space. It is widely used
              in K-Means clustering but struggles with high-dimensional data.
            </li>
            <li>
              <b>Manhattan Distance:</b> Measures the distance between two
              points by summing the absolute differences of their coordinates.
              It is useful for grid-like data structures.
            </li>
            <li>
              <b>Cosine Similarity:</b> Measures the angle between two vectors
              rather than the actual distance. It is commonly used in text
              mining and hierarchical clustering.
            </li>
            <li>
              <b>Minkowski Distance:</b> A generalization of Euclidean and
              Manhattan distances, allowing flexibility in measuring distances
              in different ways.
            </li>
          </ul>
          <p>
            Each clustering algorithm may perform differently based on the
            distance metric chosen. K-Means primarily relies on Euclidean
            distance, while hierarchical clustering can use various metrics such
            as cosine similarity or correlation distance. DBSCAN, on the other
            hand, uses epsilon-based (density) distance to group points
            effectively.
          </p>
          <img
            class="profile-image"
            src="Website/assets/clustering_2.png"
            style="
              max-width: 100%;
              height: auto;
              display: block;
              margin: 0 auto;
            "
            alt="Clustering 1"
          />
          <h2>Clustering Techniques</h2>
          <h2>K-Means Clustering</h2>
          <p>
            K-Means is a partition-based clustering algorithm that divides data
            into k groups by minimizing the variance within each cluster. It
            begins by randomly selecting k centroids, then assigns each data
            point to the closest centroid based on Euclidean distance. The
            centroids are iteratively updated until cluster assignments
            stabilize. K-Means is computationally efficient and works well for
            large datasets with well-defined, spherical clusters. However, it
            requires k to be predefined, making it less flexible when the number
            of natural clusters is unknown. It is also sensitive to outliers, as
            they can skew the centroid locations and affect clustering accuracy.
          </p>
          <h2>Hierarchical clustering</h2>
          <p>
            Hierarchical clustering builds a hierarchy of clusters by either
            merging smaller clusters (agglomerative) or splitting larger
            clusters (divisive). It does not require specifying the number of
            clusters beforehand and produces a dendrogram, which visually
            represents the merging or splitting process. This allows users to
            determine the optimal number of clusters by analyzing the hierarchy.
            Unlike K-Means, hierarchical clustering can capture nested
            relationships between clusters, making it useful for structured
            data. However, it is computationally expensive for large datasets,
            as it requires storing a distance matrix for all data points,
            leading to scalability issues.
          </p>
          <h2>DBSCAN (Density-Based Clustering)</h2>
          <p>
            DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
            identifies clusters based on high-density regions separated by
            sparse areas. Unlike K-Means and hierarchical clustering, DBSCAN
            does not require the number of clusters to be predefined. It works
            by defining core points that have a minimum number of neighbors
            within a specified distance (eps). Clusters expand by grouping
            together nearby core points, while points in sparse regions are
            treated as noise. This makes DBSCAN highly effective for identifying
            arbitrarily shaped clusters and handling outliers. However, its
            performance is sensitive to the choice of eps and min_samples,
            making parameter tuning crucial for accurate clustering results.
          </p>
          <img
            class="profile-image"
            src="Website/assets/clustering_1.png"
            style="
              max-width: 100%;
              height: auto;
              display: block;
              margin: 0 auto;
            "
            alt="Clustering 1"
          />
          <h2>Comparison of Clustering Algorithms</h2>
          <table border="1" cellspacing="0" cellpadding="8">
            <thead>
              <tr>
                <th>Criteria</th>
                <th>K-Means</th>
                <th>Hierarchical Clustering</th>
                <th>DBSCAN</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><b>Clustering Approach</b></td>
                <td>Partition-based</td>
                <td>Hierarchical (Agglomerative/Divisive)</td>
                <td>Density-based</td>
              </tr>
              <tr>
                <td><b>Number of Clusters</b></td>
                <td>Requires predefining k</td>
                <td>No need to specify k; determined by dendrogram</td>
                <td>Automatically detects clusters based on density</td>
              </tr>
              <tr>
                <td><b>Shape of Clusters</b></td>
                <td>Assumes spherical clusters</td>
                <td>Can capture complex hierarchical relationships</td>
                <td>Can detect arbitrarily shaped clusters</td>
              </tr>
              <tr>
                <td><b>Handling Noise & Outliers</b></td>
                <td>Sensitive to outliers; can distort centroids</td>
                <td>Less sensitive but still influenced by outliers</td>
                <td>Effectively handles noise by marking it as outliers</td>
              </tr>
              <tr>
                <td><b>Computational Complexity</b></td>
                <td>O(n*k*d) (Fast for large datasets)</td>
                <td>O(n²) (Slow for large datasets)</td>
                <td>O(n log n) (Efficient for large datasets)</td>
              </tr>
              <tr>
                <td><b>Memory Requirement</b></td>
                <td>Low (only centroids stored)</td>
                <td>High (stores distance matrix)</td>
                <td>Moderate (depends on density parameters)</td>
              </tr>
              <tr>
                <td><b>Best Use Cases</b></td>
                <td>Large, well-separated clusters in structured data</td>
                <td>Hierarchical structures, small datasets</td>
                <td>Detecting noise, arbitrary cluster shapes</td>
              </tr>
              <tr>
                <td><b>Weaknesses</b></td>
                <td>Needs k; struggles with non-spherical clusters</td>
                <td>Not scalable; sensitive to noise</td>
                <td>Hard to tune parameters; struggles with varying density</td>
              </tr>
            </tbody>
          </table>
          <p>
            Applying clustering techniques to this project provides valuable
            insights into user behavior and engagement patterns on Stack
            Overflow. By grouping users based on similarities in reputation,
            activity levels, and badge counts, clustering helps identify
            distinct user segments, such as highly active contributors,
            occasional participants, and inactive users. This segmentation
            allows for a deeper understanding of how different user groups
            interact with the platform, enabling targeted strategies to improve
            user retention and engagement. Additionally, clustering helps detect
            anomalies, such as users with unusual activity spikes or potential
            spam accounts, allowing for better moderation and community
            management. By leveraging clustering, the project enhances
            data-driven decision-making, optimizes content recommendations, and
            improves overall platform effectiveness.
          </p>
          <h1>Data Preprocessing</h1>
          <h2>Importance of Data Preprocessing</h2>
          <p>
            Clustering is an unsupervised learning technique that groups similar
            data points based on shared characteristics. However, the quality of
            clustering results is highly dependent on proper data preprocessing.
            Raw data often contains inconsistencies, varying feature scales, and
            non-numeric attributes that can distort the clustering process. To
            ensure meaningful and reliable clusters, essential preprocessing
            steps include
            <b
              >removing labels, selecting only numerical features, standardizing
              the data, and applying dimensionality reduction techniques like
              PCA</b
            >. These steps enhance computational efficiency and improve the
            interpretability of clustering results.
          </p>

          <h2>Removing Labels for Unbiased Clustering</h2>
          <p>
            A key preprocessing step involves
            <b>removing the age_group column</b> before clustering. Clustering
            aims to identify natural groupings in the data without predefined
            labels. By excluding this categorical variable, the model is allowed
            to find inherent structures without bias. The removed labels are
            stored separately for later comparison, enabling an evaluation of
            how well the clustering results align with actual user engagement
            patterns.
          </p>

          <h2>Selection of Numerical Features</h2>
          <p>
            To ensure compatibility with clustering algorithms,
            <b>only numerical features</b> are retained, while categorical and
            text-based columns such as display names and profile links are
            removed. Clustering relies on mathematical distance calculations,
            making non-numeric features unsuitable for analysis.
          </p>

          <h2>Standardization for Consistent Scaling</h2>
          <p>
            To ensure fair feature representation, <b>standardization</b> is
            performed using <b>StandardScaler</b>, which transforms all
            numerical features to have a mean of <b>0</b> and a standard
            deviation of <b>1</b>. This process prevents features with larger
            numerical values from disproportionately influencing the clustering
            results.
          </p>

          <h2>Dimensionality Reduction Using PCA</h2>
          <p>
            For further optimization,
            <b>Principal Component Analysis (PCA)</b> is applied to reduce the
            dataset's dimensionality while preserving most of its variance. Two
            versions of the dataset are prepared:
          </p>
          <ul>
            <li>
              <b>Original Dataset (Without PCA):</b> This dataset retains all
              numerical features after preprocessing, maintaining the full
              feature set.
            </li>
            <li>
              <b>PCA-Transformed Dataset:</b> This dataset is reduced to
              <b>six principal components</b>, preserving
              <b>95% of the variance</b> while eliminating redundancy.
            </li>
          </ul>

          <h2>Impact on Clustering</h2>
          <p>
            Using both versions, clustering will be performed to assess whether
            dimensionality reduction enhances the clustering process. The
            PCA-transformed dataset may provide better-defined clusters by
            reducing noise, while the original dataset retains more detailed
            information. Comparing results from both approaches will determine
            the effectiveness of PCA in improving clustering accuracy.
          </p>

          <h2>Final Preprocessed Dataset</h2>
          <p>
            By implementing these preprocessing steps, the dataset is structured
            to ensure efficient and meaningful clustering, leading to more
            interpretable insights into user engagement and behavior patterns.
          </p>
          <h3>Raw Data</h3>
          <img
            class="profile-image"
            src="Website/assets/cleaned_data.png"
            alt="data columns"
          />
          <h3>Cleaned Data for Clustering</h3>
          <img
            class="profile-image"
            src="Website/assets/cleaned_data_for_pca.png"
            alt="data columns"
          />
          <p>
            Code for preproccesing -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/clustering_model_impl.ipynb"
              target="_blank"
              >Clustering Data Preproccesing Code</a
            >
          </p>
          <p>
            Link to raw dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/cleaned_user_data.csv"
              target="_blank"
              >Raw Dataset</a
            >
          </p>
          <p>
            Link to proccessed dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/user_data_cleaned_for_clustering.csv"
              target="_blank"
              >Processed Dataset</a
            >
          </p>
          <h1>Implementation</h1>
          <h2>K - Means Clustering</h2>
          <p>
            K-Means clustering was applied to the PCA-transformed dataset to
            identify different patterns of user engagement. The goal was to
            group users into meaningful clusters based on their activity and
            reputation. Various values of k (the number of clusters) were
            tested, and the optimal k was determined using silhouette analysis,
            which measures how well each point fits within its cluster. The
            results showed that k=6 provided the most well-separated and
            balanced clusters, achieving a silhouette score of approximately
            0.51, indicating strong cohesion within clusters and clear
            separation between them.
          </p>
          <p>
            For k=3, users were broadly divided into three main categories: low,
            moderate, and highly active engagement levels. Increasing the number
            of clusters to k=4 and k=5 refined these groups further, capturing
            more specific engagement behaviors. At k=5, a unique cluster
            emerged, representing users with high but inconsistent activity,
            highlighting how a higher k-value helps in uncovering more detailed
            insights into user engagement.
          </p>
          <img
            class="profile-image"
            src="Website/assets/optimal_k.png"
            alt="Scree Plot"
          />
          <p>
            As the number of clusters increased, the cluster centers (centroids)
            adjusted dynamically, allowing for more precise grouping of users.
            This helped in defining the boundaries between different engagement
            levels. Additionally, outliers—users who did not fit neatly into any
            cluster—were identified. These users might have unusual activity
            patterns, such as a sudden spike in reputation or inconsistent
            participation over time.
          </p>
          <p>
            When comparing the clustering results with account age distribution,
            k=5 emerged as the most balanced choice. It effectively segmented
            users into distinct groups while maintaining clarity and
            interpretability. Beyond k=6, the clusters started to lose
            distinction, making it harder to identify meaningful patterns.
          </p>
          <p>
            Code for K-Means -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/clustering_model_impl.ipynb"
              target="_blank"
              >K - Means clustering Implementation</a
            >
          </p>
          <h2>Hierarchical Clustering</h2>
          <p>
            Hierarchical clustering was applied to segment users based on
            engagement patterns. Unlike K-Means, which requires a predefined
            number of clusters, hierarchical clustering builds a tree-like
            structure called a dendrogram, allowing a more flexible approach to
            cluster selection.
          </p>

          <h3>Dendrogram Analysis Cosine Similarity</h3>
          <p>
            The dendrogram represents hierarchical relationships among data
            points using cosine similarity, which measures how similar users are
            based on their engagement behavior.
          </p>

          <h3>Key Observations</h3>
          <ul>
            <li>
              Distinct Clusters: At a cosine distance threshold of approximately
              60, about five to six well-separated clusters emerge.
            </li>
            <li>
              Compact Groups: The leftmost clusters in orange and green show
              users with highly similar behavior, merging at lower distances.
            </li>
            <li>
              Diverse Behaviors: The rightmost clusters in purple and pink merge
              at higher distances, indicating distinct user engagement patterns
              or potential outliers.
            </li>
            <li>
              Flexible Clustering: By analyzing where major splits occur, an
              optimal number of clusters can be chosen dynamically.
            </li>
          </ul>
          <img
            class="profile-image"
            src="Website/assets/dendrogram_!.png"
            alt="Cosine Similarity Dendrogram"
          />
          <h3>Cluster Interpretation</h3>
          <p>
            The final clustering results, plotted in a PCA-reduced space, reveal
            well-defined user groups.
          </p>
          <ul>
            <li>
              Cluster One Red: Densely packed, representing users with similar
              engagement behavior, likely regular contributors.
            </li>
            <li>
              Cluster Two Blue: A widely spread cluster, possibly containing
              outliers or highly unique users such as new users or extremely
              high reputation users.
            </li>
            <li>
              Cluster Three Green: A structured group with engagement patterns
              slightly different from Cluster One.
            </li>
          </ul>
          <h3>Comparison with K-Means</h3>
          <ul>
            <li>
              Visual Clarity: The dendrogram provides an intuitive structure for
              cluster formation.
            </li>
            <li>
              Automatic Cluster Selection: No need to predefine the number of
              clusters as clusters are determined based on natural separations.
            </li>
            <li>
              Better Outlier Detection: Unlike K-Means, hierarchical clustering
              does not force all points into clusters, helping isolate anomalies
              effectively.
            </li>
          </ul>
          <p>
            Code for Hierarchical Clustering -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/clustering_model_impl.ipynb"
              target="_blank"
              >Hierarchical clustering Implementation</a
            >
          </p>
          <h2>DBSCAN</h2>
          <p>
            DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
            was applied to group users based on their engagement patterns while
            effectively identifying outliers. Unlike K-Means, DBSCAN does not
            require specifying the number of clusters beforehand. Instead, it
            relies on two key parameters: <b>eps</b> (the neighborhood radius)
            and <b>min_samples</b>
            (the minimum number of points required to form a dense region).
          </p>

          <h3>K-Distance Graph Analysis for eps Selection</h3>
          <p>
            To determine the optimal <b>eps</b> value, a
            <b>K-Distance Graph</b> was plotted using the 5th nearest neighbor
            distances:
          </p>
          <ul>
            <li>
              <b>Flat Region at the Start:</b> Most points have small
              nearest-neighbor distances, indicating dense clusters.
            </li>
            <li>
              <b>Sharp Increase at the End:</b> A steep rise at the far right
              suggests isolated points (outliers).
            </li>
            <li>
              <b>Optimal eps Selection:</b> The ideal eps value is chosen just
              before the sharp increase, ensuring DBSCAN distinguishes between
              <b>core points</b> and <b>noise</b> effectively.
            </li>
          </ul>
          <img
            class="profile-image"
            src="Website/assets/dbscan_pioc.png"
            alt="Scree Plot"
          />
          <h3>DBSCAN Clustering Results</h3>
          <p>After applying DBSCAN, the following observations were made:</p>
          <ul>
            <li>
              <b>Dominant Cluster:</b> The majority of data points (88,398)
              belong to <b>Cluster 1</b>, showing a well-defined dense region.
            </li>
            <li>
              <b>Significant Outliers:</b> DBSCAN identified
              <b>9,408 outliers</b>, categorized under <b>Cluster -1</b>,
              indicating noisy or anomalous points.
            </li>
            <li>
              <b>Second-Largest Cluster:</b> <b>Cluster 0</b> contains
              <b>6,686 points</b>, forming another meaningful segment.
            </li>
            <li>
              <b>Smaller Clusters:</b> The remaining clusters have very few
              points, with most containing fewer than 20 data points, suggesting
              highly specific user groups.
            </li>
          </ul>

          <h3>DBSCAN Visualization and Interpretation</h3>
          <ul>
            <li>
              <b>Cluster Formation:</b> Multiple clusters were detected, each
              represented by different colors in the PCA-reduced scatter plot.
            </li>
            <li>
              <b>Outliers and Noise:</b> Scattered points categorized as
              <b>Cluster -1</b> represent anomalies or isolated data points.
            </li>
            <li>
              <b>Dense Core:</b> A compact grouping of most points suggests
              strong clustering behavior.
            </li>
            <li>
              <b>Vertical Spread:</b> Data is concentrated in a narrow
              <b>Principal Component 1 (PC1)</b> range, while variation is more
              visible along <b>Principal Component 2 (PC2)</b>.
            </li>
          </ul>

          <h3>Comparison with K-Means and Hierarchical Clustering</h3>
          <ul>
            <li>
              <b>Outlier Detection:</b> DBSCAN effectively
              <b>identifies anomalies</b>, unlike K-Means, which forces all
              points into clusters.
            </li>
            <li>
              <b>Automatic Cluster Detection:</b> Unlike K-Means, where
              <b>k</b> must be predefined, DBSCAN automatically determines the
              number of clusters.
            </li>
            <li>
              <b>Flexibility in Cluster Shapes:</b> DBSCAN can detect
              <b>arbitrarily shaped clusters</b>, while K-Means assumes
              spherical clusters.
            </li>
          </ul>
          <p>
            Code for DBSCAN -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/clustering_model_impl.ipynb"
              target="_blank"
              >DBSCAN Implementation</a
            >
          </p>
          <h1>Comparison of Clustering Models</h1>
          <table border="1">
            <tr>
              <th>Feature</th>
              <th>K Means</th>
              <th>DBSCAN</th>
              <th>Hierarchical Clustering</th>
            </tr>
            <tr>
              <td>Cluster Shape</td>
              <td>Spherical, equal-sized clusters</td>
              <td>Arbitrary shapes based on density</td>
              <td>Tree-based hierarchical clusters</td>
            </tr>
            <tr>
              <td>Outlier Detection</td>
              <td>No direct detection</td>
              <td>Effectively detects outliers</td>
              <td>No direct detection</td>
            </tr>
            <tr>
              <td>Best For</td>
              <td>Well-separated data</td>
              <td>Densely packed, noisy data</td>
              <td>Understanding cluster hierarchy</td>
            </tr>
            <tr>
              <td>Scalability</td>
              <td>Fast for large datasets</td>
              <td>Slower for large datasets</td>
              <td>Computationally expensive</td>
            </tr>
            <tr>
              <td>Cluster Count</td>
              <td>Fixed, predefined number of clusters</td>
              <td>Automatic, density-based</td>
              <td>Flexible, determined by hierarchy</td>
            </tr>
            <tr>
              <td>Noise Handling</td>
              <td>Sensitive to outliers</td>
              <td>Handles noise effectively</td>
              <td>Does not handle noise well</td>
            </tr>
            <tr>
              <td>Flexibility</td>
              <td>Requires tuning the number of clusters</td>
              <td>Automatically detects clusters</td>
              <td>No need for predefined clusters</td>
            </tr>
            <tr>
              <td>Interpretability</td>
              <td>Easy to interpret</td>
              <td>Harder to visualize</td>
              <td>Provides a clear hierarchy</td>
            </tr>
            <tr>
              <td>Best Use Case</td>
              <td>Balanced data, predefined groups</td>
              <td>Irregularly shaped clusters, noisy data</td>
              <td>Visualizing relationships between clusters</td>
            </tr>
          </table>
          <h1>Results and Conclusion</h1>
          <h3>Clustering Analysis Results</h3>
          <p>
            The clustering analysis was conducted using three models:
            <strong>K-Means, DBSCAN, and Hierarchical Clustering</strong>. Each
            model provided unique insights into user engagement on Stack
            Overflow.
          </p>

          <h4>K-Means Clustering</h4>
          <p>
            K-Means identified <b>six distinct clusters</b>, with the highest
            silhouette score of <b>0.51</b>. It effectively categorized users
            based on reputation, engagement, and activity levels. The clusters
            captured varying user behaviors, from highly active contributors to
            low-engagement users. However, K-Means struggled with noise and
            outliers, leading to some misclassified data points.
          </p>

          <h4>DBSCAN Clustering</h4>
          <p>
            DBSCAN efficiently handled noise and detected <b>9,408 outliers</b>,
            distinguishing them from meaningful user clusters. It successfully
            captured dense user groups but also formed several small, scattered
            clusters. This method proved beneficial for identifying anomalous
            users or less frequent engagement patterns but was sensitive to
            variations in cluster density.
          </p>

          <h4>Hierarchical Clustering</h4>
          <p>
            Hierarchical Clustering provided a structured view of the data,
            illustrating relationships between users through a dendrogram. The
            analysis suggested
            <b>three primary clusters</b>, reflecting different engagement
            levels. The visualization helped in understanding the natural
            hierarchy of users, but computational complexity made it less
            scalable for larger datasets.
          </p>

          <h3>Conclusion</h3>
          <p>
            This project successfully demonstrated how different clustering
            models can be applied to analyze user engagement patterns on Stack
            Overflow. <b>K-Means</b> was effective in segmenting structured
            data, <b>DBSCAN</b> handled outliers efficiently, and
            <b>Hierarchical Clustering</b> provided a deeper hierarchical
            insight into user relationships.
          </p>

          <p>
            The findings highlight that user engagement follows structured
            patterns, with experienced users forming well-defined groups, while
            newer or less active users are more dispersed. These insights can
            help in designing targeted engagement strategies, personalized
            recommendations, and improving community retention.
          </p>

          <p>
            Future enhancements could involve integrating deep learning
            techniques for improved cluster analysis and user behavior
            prediction, enabling more refined segmentation and better platform
            optimization.
          </p>
          <p>
            Link to complete code -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/clustering_model_impl.ipynb"
              target="_blank"
              >Complete Implementation</a
            >
          </p>
          <p>
            Github Repository -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project"
              target="_blank"
              >Link to github code</a
            >
          </p>
        </div>
        <div id="arm" class="model-content">
          <h1>Overview</h1>
          <h2>Association Rule Mining</h2>
          <p>
            Association Rule Mining is a data mining technique used to identify
            hidden relationships between variables in large datasets. It is
            commonly used in market basket analysis, where businesses analyze
            customer purchases to uncover patterns such as customers who buy
            bread often buy butter. ARM helps in making data-driven decisions
            for product recommendations, inventory management, and marketing
            strategies by discovering frequent itemsets and generating rules
            that highlight meaningful associations.
          </p>
          <h2>Key Measures in ARM: Support, Confidence, and Lift</h2>
          <p>
            Support measures how frequently an itemset appears in the dataset. A
            higher support value indicates that the pattern is more commonly
            occurring.
          </p>
          <p>
            Confidence represents how often the rule "If X, then Y" holds true.
            It calculates the probability that Y is purchased given X is already
            present in the transaction.
          </p>
          <p>
            Lift measures the strength of an association by comparing its
            occurrence with what would be expected if X and Y were independent.
            A lift value greater than one indicates a strong positive
            correlation between X and Y.
          </p>
          <p>
            These measures help filter out weak rules and ensure that only the
            strongest relationships are identified.
          </p>
          <img
            class="profile-image"
            src="Website/assets/arm_pic_1.png"
            alt="data columns"
          />

          <h2>Association Rules</h2>
          <p>
            Association rules are statements that define relationships between
            two or more items in a dataset. They take the form:
          </p>

          <p class="rule">X ⇒ Y</p>

          <p>
            Where <strong>X (Antecedent)</strong> is the condition and
            <strong>Y (Consequent)</strong> is the outcome. For example:
          </p>

          <div class="example">
            <p>
              <strong>{Laptop} → {Mouse}</strong> (If a customer buys a laptop,
              they are likely to buy a mouse.)
            </p>
            <p>
              <strong>{Milk, Bread} → {Butter}</strong> (If a customer buys milk
              and bread, they are likely to buy butter.)
            </p>
          </div>

          <p>
            Strong association rules help in making
            <strong>business recommendations</strong>, improving
            <strong>user experience</strong>, and optimizing
            <strong>inventory management</strong>.
          </p>
          <h2>The Apriori Algorithm and How It Works</h2>

          <p>
            The Apriori Algorithm is a widely used method for discovering
            association rules by identifying frequently occurring itemsets and
            generating meaningful rules.
          </p>

          <p>
            The first step involves scanning the dataset to find individual
            items that appear frequently based on a minimum support threshold.
            Pairs, triplets, and higher-order itemsets are generated, keeping
            only those that meet the support criteria.
          </p>

          <p>
            Next, the algorithm applies the Apriori Property, which states that
            if an itemset is infrequent, its supersets will also be infrequent.
            This helps eliminate unnecessary calculations, making the algorithm
            efficient and scalable.
          </p>

          <p>
            Frequent itemsets are then used to create rules, and their
            confidence and lift values are calculated. Only rules meeting
            predefined confidence and lift thresholds are retained.
          </p>

          <p>
            Finally, the rules are ranked based on Support, Confidence, and
            Lift, ensuring only the strongest and most meaningful associations
            are considered.
          </p>
          <img
            class="profile-image"
            src="Website/assets/arm_pic_2.png"
            alt="data columns"
          />
          <h2>How ARM is Used in This Project</h2>
          <p>
            In this project, ARM was applied to analyze user engagement based on
            factors like reputation, badges, and account longevity. The dataset
            was preprocessed to ensure compatibility with the Apriori algorithm.
          </p>
          <p>
            Numerical values were converted into binary categories, such as
            assigning one to high reputation if the reputation score is greater
            than one thousand.
          </p>
          <p>
            Categorical attributes were one-hot encoded to ensure efficient rule
            extraction. Frequent itemsets were extracted using Apriori with a
            minimum support of five percent. Association rules were generated
            based on Lift greater than one, ensuring only meaningful connections
            were retained.
          </p>
          <p>
            The results showed strong correlations between high reputation and
            badge accumulation, as well as a connection between high acceptance
            rates and silver badges, reinforcing how user engagement impacts
            recognition.
          </p>
          <h1>Data Preparation</h1>
          <p>
            Before applying <strong>Association Rule Mining (ARM)</strong>, the
            dataset needs to be transformed into a format suitable for
            algorithms like <strong>Apriori</strong>. The following
            preprocessing steps were applied to prepare the data:
          </p>

          <h3>1. Converting Numerical Attributes into Binary Categories</h3>
          <p>
            ARM works with binary data, where each attribute is either present
            (1) or absent (0). To achieve this, numerical attributes were
            categorized into meaningful binary indicators. Examples include:
          </p>
          <ul>
            <li>
              <code>high_reputation = 1</code> if
              <code>reputation > 1000</code>, else <code>0</code>.
            </li>
            <li>
              <code>many_bronze_badges = 1</code> if
              <code>badge_bronze > 10</code>, else <code>0</code>.
            </li>
            <li>
              <code>old_account = 1</code> if
              <code>account_age_days > 1000</code>, else <code>0</code>.
            </li>
          </ul>

          <h3>2. Encoding Categorical Variables</h3>
          <p>
            To ensure categorical attributes can be used in ARM, one-hot
            encoding was applied. This converts categories into separate binary
            columns. Example:
          </p>
          <ul>
            <li>
              <code>user_type</code> and <code>age_group</code> were transformed
              using one-hot encoding.
            </li>
            <li>
              The first category in each variable was dropped
              (<code>drop_first=True</code>) to avoid redundancy.
            </li>
          </ul>

          <h3>3. Selecting Relevant Features for ARM</h3>
          <p>
            Only binary-transformed attributes were retained for ARM. The final
            dataset included:
          </p>
          <ul>
            <li>
              <code>high_reputation</code>, <code>many_bronze_badges</code>,
              <code>many_silver_badges</code>
            </li>
            <li>
              <code>many_gold_badges</code>, <code>old_account</code>,
              <code>high_accept_rate</code>
            </li>
          </ul>

          <h3>4. Final Data Structure</h3>
          <p>
            The transformed dataset now contains binary values (1 or 0),
            ensuring compatibility with ARM algorithms. Example:
          </p>
          <pre>
         high_reputation  many_bronze_badges  many_silver_badges  many_gold_badges  old_account  high_accept_rate
      0              1                   1                   1                 1            1                 1
      1              1                   1                   1                 1            1                 1
      2              1                   1                   1                 1            1                 1
              </pre
          >

          <p>
            With this preprocessing, the dataset is now ready for ARM, allowing
            for efficient discovery of meaningful association rules.
          </p>
          <h3>Raw Data</h3>
          <img
            class="profile-image"
            src="Website/assets/cleaned_data.png"
            alt="data columns"
          />
          <h3>Cleaned Data for ARM</h3>
          <img
            class="profile-image"
            src="Website/assets/arm_cleaned_dataset.png"
            alt="data columns"
          />
          <p>
            Code for preproccesing -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/arm_model_impl.ipynb"
              target="_blank"
              >ARM Data Preproccesing Code</a
            >
          </p>
          <p>
            Link to raw dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/cleaned_user_data.csv"
              target="_blank"
              >Raw Dataset</a
            >
          </p>
          <p>
            Link to proccessed dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/user_data_cleaned_for_arm.csv"
              target="_blank"
              >Processed Dataset</a
            >
          </p>
          <h1>Implementation</h1>
          <p>
            Association Rule Mining (ARM) was applied using the Apriori
            algorithm to uncover meaningful relationships within the dataset.
            The process began with identifying frequent itemsets that appeared
            in at least 5% of transactions, ensuring that only significant
            patterns were considered. Once the frequent itemsets were
            established, association rules were generated based on the lift
            metric, with a minimum threshold of 1.0, ensuring that the
            discovered relationships held strong and valuable connections.
          </p>
          <p>
            The extracted rules provided valuable insights into user behavior.
            Users with many bronze badges almost always had high reputation,
            emphasizing that active participation contributes to credibility.
            Similarly, older accounts were strongly linked to high reputation,
            indicating that platform longevity plays a key role in building user
            trust. Another notable pattern revealed that users with high
            acceptance rates had a moderate likelihood of having high
            reputation, highlighting that consistent contribution acceptance
            influences recognition.
          </p>
          <p>
            To further refine these insights, rules were ranked based on
            support, confidence, and lift, with the top 15 rules selected for
            each metric. This ranking helped filter out weaker patterns and
            highlighted the most impactful associations. These insights can be
            leveraged for recommendation systems, user engagement strategies,
            and customer segmentation, allowing for data-driven decision-making.
          </p>
          <h3>Association Rules Network Visualization</h3>
          <img
            class="profile-image"
            src="Website/assets/arm_network_final.png"
            alt="data columns"
          />
          <p>
            The Association Rules Network Graph visually represents the
            discovered relationships between different user attributes. Each
            node in the graph represents an attribute, while edges between nodes
            indicate a strong association between them.
          </p>
          <p>
            The visualization clearly highlights that high reputation is
            strongly connected to many badges and old accounts, suggesting that
            long-term engagement and earning multiple badges contribute to user
            credibility. Additionally, a high acceptance rate is linked to many
            silver badges, indicating that users whose contributions are
            frequently accepted tend to receive greater recognition.
          </p>
          <p>
            The network structure forms distinct clusters, showing how
            engagement, experience, and recognition are interconnected. This
            visualization provides an intuitive understanding of user behavior,
            helping platforms identify key influencers, optimize engagement
            strategies, and enhance recommendation systems.
          </p>

          <h1>Results</h1>
          <p>
            Association Rule Mining was applied using the Apriori algorithm to
            uncover relationships between different user attributes. Frequent
            itemsets were extracted with a minimum support threshold of 5%,
            ensuring that only the most significant patterns were considered.
            The association rules were then generated based on the lift metric,
            with a minimum threshold of 1.0, ensuring that the discovered
            patterns had meaningful relationships.
          </p>
          <p>
            The extracted rules provided key insights into user behavior. A
            strong relationship was observed between users with many bronze
            badges and high reputation, indicating that reputation grows with
            increased participation. Similarly, users with old accounts were
            highly likely to have high reputation, showing that longer platform
            engagement contributes to credibility. Additionally, users with a
            high acceptance rate had a moderate likelihood of possessing a high
            reputation, suggesting that accepted contributions play a role in
            recognition.
          </p>
          <p>
            To refine these insights, the rules were ranked based on support,
            confidence, and lift, selecting the top 15 rules for each metric.
            These rankings helped in identifying the strongest relationships,
            ensuring that only the most impactful patterns were retained.
          </p>
          <p>
            A network visualization was created to represent these relationships
            graphically. The visualization illustrated the key factors
            influencing reputation, engagement, and recognition, making it
            easier to understand the underlying connections in user behavior.
          </p>
          <h2>Conclusion</h2>
          <p>
            Understanding how users engage with a platform is crucial for
            improving user experience, recognition systems, and retention
            strategies. The findings from Association Rule Mining highlight the
            importance of engagement, consistency, and contribution quality in
            building reputation and credibility. Just like in real-world
            communities where trust is earned over time, users on digital
            platforms also gain recognition through continued participation and
            meaningful contributions.
          </p>
          <p>
            These insights can be applied in various ways. Platforms can use
            these patterns to design better reward systems, recommend relevant
            content, and identify active users for engagement programs.
            Businesses can leverage similar techniques for customer
            segmentation, personalized recommendations, and targeted marketing
            strategies.
          </p>
          <p>
            By understanding these patterns, platforms can create a more
            engaging and rewarding experience for users, fostering stronger
            communities and encouraging meaningful contributions. The
            implementation of Association Rule Mining not only enhances
            decision-making but also provides a data-driven approach to
            improving engagement and user satisfaction.
          </p>
          <p>
            Link to complete code -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/arm_model_impl.ipynb"
              target="_blank"
              >Complete Implementation</a
            >
          </p>
          <p>
            Github Repository -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project"
              target="_blank"
              >Link to github code</a
            >
          </p>
        </div>
        <div id="naivebayes" class="model-content">
          <h1>Overview</h1>
          <h2>Naive Bayes</h2>
          <p>
            Naïve Bayes (NB) is a supervised machine learning algorithm commonly
            used for classification tasks. It is based on Bayes' Theorem, a
            statistical principle that calculates the probability of a certain
            outcome occurring, given prior knowledge or evidence. In the context
            of machine learning, Naïve Bayes predicts the most likely class or
            category a data point belongs to, based on the values of its input
            features. This makes it especially useful in situations where we
            want to categorize or label data automatically.
          </p>
          <p>
            The algorithm is called naïve because it makes a strong and often
            unrealistic assumption: that all input features are independent of
            one another. In other words, it assumes that the presence or value
            of one feature does not influence or relate to another — which is
            almost never the case in real-world data. Despite this
            oversimplification, Naïve Bayes tends to work surprisingly well in
            practice, and often provides accurate and reliable results,
            especially for large-scale classification problems.
          </p>
          <p>
            Naïve Bayes is widely used across a variety of domains due to its
            simplicity and effectiveness. One common application is spam
            detection, where the algorithm classifies incoming emails as spam or
            not spam based on keywords and patterns. It's also frequently used
            in sentiment analysis, such as determining whether a product review
            is positive or negative. In document classification, Naïve Bayes
            helps categorize articles, news stories, or posts into topics like
            sports, politics, or technology. In the healthcare domain, it can be
            used for medical diagnosis, where patient symptoms are analyzed to
            predict the likelihood of a certain disease. Additionally, it can be
            applied to predict user behavior, such as classifying types of users
            on platforms like Stack Overflow based on their activity,
            reputation, and other profile features.
          </p>
          <h3>Working of Naive Bayes</h3>
          <p>
            Naïve Bayes is a method that helps us predict which category or
            class something belongs to by looking at its features or
            characteristics. It works by calculating the chance (probability)
            that the item fits into each possible class, and then picks the one
            with the highest chance.
          </p>
          <p>
            To do this, Naïve Bayes uses something called Bayes' Theorem. This
            formula helps us figure out how likely a class is, based on the
            features we know. For example, if we're trying to guess if a user is
            a “moderator” or “regular user” on Stack Overflow, the algorithm
            looks at things like their reputation, badges, and whether they have
            a profile image — and calculates the probability for each class.
          </p>
          <p>Formula for Naive Bayes</p>
          <img
            class="profile-image"
            src="Website/assets/naive bayes formula.png"
            alt="data columns"
          />
          <h4>Why Smoothing Is Important</h4>
          <p>
            Smoothing is used in Naïve Bayes models to handle the problem of
            zero probabilities. If a class and feature combination never
            occurred in the training data, it would be assigned a probability of
            zero — which can completely nullify the final prediction. Laplace
            smoothing (also called additive smoothing) adds a small constant
            (typically 1) to every count, ensuring that no probability is ever
            exactly zero. This improves model robustness and helps it generalize
            better to unseen data.
          </p>
          <h2>Types of Naïve Bayes</h2>
          <h3>Multinomial Naïve Bayes (MultinomialNB)</h3>
          <p>
            The Multinomial Naïve Bayes model is appropriate for classification
            tasks involving discrete count data. It is particularly effective
            when features represent the frequency or occurrence of events.
            Typical applications include text classification, where features are
            often word counts or term frequencies, and other domains where
            feature values are non-negative integers. For instance, in community
            platforms, counts of badges or posts can serve as relevant features
            under this model. It is important to note that the model does not
            support negative or continuous values.
          </p>
          <h3>Gaussian Naïve Bayes (GaussianNB)</h3>
          <p>
            The Gaussian Naïve Bayes model is designed for datasets containing
            continuous numerical features. This model assumes that the
            distribution of the feature values within each class follows a
            Gaussian (normal) distribution. It is well-suited for cases
            involving measurements such as age, score, reputation, or
            percentages. Since many real-world metrics follow a bell-shaped
            curve, this assumption often holds in practical scenarios.
            GaussianNB is appropriate when the dataset includes real-valued
            variables.
          </p>
          <h3>Bernoulli Naïve Bayes (BernoulliNB)</h3>
          <p>
            The Bernoulli Naïve Bayes model is best applied to datasets where
            the features are binary (Boolean) in nature, meaning they can take
            on values such as 0 or 1, representing absence or presence. The
            model evaluates whether a specific attribute is present rather than
            how often it appears. This is common in applications where data is
            structured around binary outcomes, such as email spam detection or
            feature toggles in user profiles. Each feature is treated
            independently, and their binary values are used to estimate class
            probabilities.
          </p>
          <h3>Categorical Naïve Bayes (CategoricalNB)</h3>
          <p>
            The Categorical Naïve Bayes model handles categorical variables,
            where features consist of a finite set of non-numeric, non-ordered
            categories. This model is suitable for data such as country names,
            user roles, product types, or any labeled groups that are not
            inherently numerical or hierarchical. CategoricalNB does not assume
            any particular ordering or distance between category values, making
            it effective for handling nominal features directly without
            numerical encoding.
          </p>
          <img
            class="profile-image2"
            src="Website/assets/nb_intro.png"
            alt="data columns"
          />
          <table border="1" cellpadding="8" cellspacing="0">
            <thead>
              <tr>
                <th>Naïve Bayes Type</th>
                <th>Suitable Feature Type</th>
                <th>Expected Data Format</th>
                <th>Common Applications</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>MultinomialNB</strong></td>
                <td>Discrete count data</td>
                <td>Non-negative integers (0, 1, 2, ...)</td>
                <td>Text classification, badge/post counts</td>
              </tr>
              <tr>
                <td><strong>GaussianNB</strong></td>
                <td>Continuous numeric data</td>
                <td>Real numbers (floating-point)</td>
                <td>Scores, percentages, reputation, continuous metrics</td>
              </tr>
              <tr>
                <td><strong>BernoulliNB</strong></td>
                <td>Binary (yes/no) features</td>
                <td>Boolean or binary (0 or 1)</td>
                <td>
                  Presence/absence detection, email spam, binary indicators
                </td>
              </tr>
              <tr>
                <td><strong>CategoricalNB</strong></td>
                <td>Categorical (nominal) features</td>
                <td>Labeled categories (non-numeric)</td>
                <td>
                  User type, country, group memberships, categorical attributes
                </td>
              </tr>
            </tbody>
          </table>
          <h1>Data Preparation</h1>
          <p>
            To prepare the dataset for Naive Bayes classification, the
            continuous
            <code>reputation</code> scores were transformed into four
            quantile-based categories labeled as <strong>Beginner</strong>,
            <strong>Intermediate</strong>, <strong>Advanced</strong>, and
            <strong>Expert</strong>. This transformation enabled the task to be
            framed as a multi-class classification problem rather than a
            regression problem.
          </p>

          <p>
            Irrelevant fields such as user IDs, profile URLs, and the original
            continuous reputation values were removed to simplify the feature
            space. All missing values across features were filled with 0 to
            ensure compatibility with Naive Bayes algorithms, which do not
            handle null values well. Additionally, rows containing any negative
            values were filtered out when required, especially for count-based
            models.
          </p>

          <p>
            Different feature sets and preprocessing techniques were applied
            based on the specific assumptions of each Naive Bayes model:
          </p>

          <ul>
            <li>
              <strong>MultinomialNB & ComplementNB:</strong> These models
              require discrete, non-negative integer features. Features selected
              include badge counts (<code>badge_bronze</code>,
              <code>badge_silver</code>, <code>badge_gold</code>) and reputation
              change metrics (<code>reputation_change_day</code>,
              <code>_week</code>, <code>_month</code>, etc.). Negative values
              were removed, and missing values were filled with zeros.
            </li>

            <li>
              <strong>GaussianNB:</strong> This model assumes features are
              continuous and normally distributed. The dataset included raw
              numeric features such as <code>accept_rate</code>,
              <code>account_age_years</code>, and other continuous metrics like
              reputation change history. No discretization or encoding was
              performed here; only missing value handling was applied.
            </li>

            <li>
              <strong>CategoricalNB:</strong> This model expects categorical
              input features. Categorical variables such as
              <code>user_type</code>, <code>age_group</code>, and
              <code>is_employee</code> were label-encoded into numerical values.
              Continuous features like <code>accept_rate</code>, badge counts,
              and account age were discretized into quantile-based bins using
              <code>KBinsDiscretizer</code> to convert them into categorical
              ranges compatible with this model.
            </li>
          </ul>

          <p>
            Once preprocessing was complete, the data was split into training
            and testing sets using an 80-20 stratified split. Stratified
            sampling ensured that each subset retained the original class
            distribution, allowing fair comparison of model performance across
            all expertise levels.
          </p>

          <h3>Raw Data</h3>
          <img
            class="profile-image"
            src="Website/assets/cleaned_data.png"
            alt="data columns"
          />
          <h3>Cleaned Data for MultinomialNB ad ComplementNB</h3>
          <img
            class="profile-image"
            src="Website/assets/Mutinomial_dataframe.png"
            alt="data columns"
          />
          <h3>Cleaned Data for GaussianNB</h3>
          <img
            class="profile-image"
            src="Website/assets/gauisan_dataframe.png"
            alt="data columns"
          />
          <h3>Cleaned Data for CategoricalNB</h3>
          <img
            class="profile-image"
            src="Website/assets/categorical_dataframe.png"
            alt="data columns"
          />
          <p>
            Code for preproccesing -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/naive_bayes_model_impl.ipynb"
              target="_blank"
              >Naive Bayes Data Preproccesing Code</a
            >
          </p>
          <p>
            Link to raw dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/cleaned_user_data.csv"
              target="_blank"
              >Raw Dataset</a
            >
          </p>
          <p>
            Link to all proccessed dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/tree/main/Dataset/naive_bayes_data"
              target="_blank"
              >Processed Dataset for Naive Bayes</a
            >
          </p>
          <h1>Implementation</h1>
          <p>
            Each Naive Bayes model was implemented using the
            <code>scikit-learn</code> library and trained on a processed Stack
            Overflow user dataset to predict user expertise levels. The dataset
            was carefully tailored for each model type, respecting the
            assumptions and data requirements of each Naive Bayes variant. Below
            is a breakdown of the specific preprocessing steps, model training,
            and evaluation strategy used for each classifier.
          </p>

          <h3>1. Multinomial Naive Bayes</h3>
          <ul>
            <li>
              Selected only count-based features, including badge counts
              (<code>badge_bronze</code>, <code>badge_silver</code>,
              <code>badge_gold</code>) and reputation change metrics across
              various timeframes.
            </li>
            <li>
              Ensured all features were non-negative integers by removing rows
              containing negative values and filling missing entries with zeros.
            </li>
            <li>
              Encoded the target variable <code>expertise_level</code> into
              numeric labels using <code>LabelEncoder</code>.
            </li>
            <li>
              Split the dataset into training and testing subsets using an 80-20
              stratified split to preserve the original class distribution.
            </li>
            <li>
              Trained the <code>MultinomialNB</code> classifier and evaluated it
              using classification metrics including accuracy, precision,
              recall, and F1-score.
            </li>
          </ul>
          <img
            class="profile-image"
            src="Website/assets/nb4.png"
            alt="data columns"
          />

          <h3>2. Gaussian Naive Bayes</h3>
          <ul>
            <li>
              Selected continuous features such as <code>accept_rate</code>,
              <code>account_age_years</code>, and reputation-related metrics.
            </li>
            <li>
              Handled missing values by replacing them with zeros and filtered
              out any rows with invalid or negative entries.
            </li>
            <li>
              Used <code>LabelEncoder</code> to encode the target labels and
              applied a stratified train-test split for balanced class
              representation.
            </li>
            <li>
              Trained the <code>GaussianNB</code> model, which assumes the
              features follow a normal distribution, and evaluated the model
              using standard classification metrics.
            </li>
          </ul>
          <img
            class="profile-image"
            src="Website/assets/nb1.png"
            alt="data columns"
          />

          <h3>3. Complement Naive Bayes</h3>
          <ul>
            <li>
              Used the same non-negative count-based features as the Multinomial
              model.
            </li>
            <li>
              Handled preprocessing similarly by filtering invalid rows and
              encoding the target variable.
            </li>
            <li>
              Performed an 80-20 stratified split of the data to train and test
              the model.
            </li>
            <li>
              Trained the <code>ComplementNB</code> classifier, which is
              designed to work better with imbalanced class distributions by
              using information from all classes except the target class.
            </li>
            <li>
              Evaluated the model using a confusion matrix, accuracy score, and
              detailed classification report.
            </li>
          </ul>
          <img
            class="profile-image"
            src="Website/assets/nb2.png"
            alt="data columns"
          />

          <h3>4. Categorical Naive Bayes</h3>
          <ul>
            <li>
              Converted categorical features such as <code>user_type</code>,
              <code>age_group</code>, and <code>is_employee</code> into numeric
              form using <code>LabelEncoder</code>.
            </li>
            <li>
              Discretized continuous numeric features (e.g.,
              <code>accept_rate</code>, badge counts, and account age) into
              ordinal bins using <code>KBinsDiscretizer</code> to make them
              suitable for categorical modeling.
            </li>
            <li>
              Combined the encoded categorical variables with the binned
              numerical features to form the input feature set.
            </li>
            <li>
              Encoded the <code>expertise_level</code> target variable and
              performed a stratified train-test split.
            </li>
            <li>
              Trained the <code>CategoricalNB</code> model and assessed its
              performance using standard classification metrics.
            </li>
          </ul>
          <img
            class="profile-image"
            src="Website/assets/nb3.png"
            alt="data columns"
          />
          <p>
            Code for Implementation -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/naive_bayes_model_impl.ipynb"
              target="_blank"
              >Naive Bayes Implementation Code</a
            >
          </p>

          <h1>Results</h1>
          <p>
            Each Naive Bayes model was evaluated on the test dataset using
            accuracy, precision, recall, and F1-score metrics. Confusion
            matrices were also plotted to visualize the prediction performance
            across the four expertise classes: Beginner, Intermediate, Advanced,
            and Expert. The following summarizes the key results and insights
            for each model:
          </p>

          <h3>1. Multinomial Naive Bayes</h3>
          <p>
            The model achieved an overall accuracy of <strong>41.3%</strong>. It
            performed particularly well in identifying
            <strong>Beginner</strong> users, with a high recall of
            <strong>0.69</strong>. It also showed strong precision
            (<strong>0.67</strong>) for the <strong>Expert</strong> class,
            indicating confidence when classifying users as experts. However,
            performance on the <strong>Intermediate</strong> class was weak,
            with a recall of only <strong>0.14</strong>. These results suggest
            that while the model effectively distinguishes users at the
            expertise extremes, it struggles to classify mid-level users
            accurately.
          </p>

          <h3>2. Gaussian Naive Bayes</h3>
          <p>
            GaussianNB achieved a slightly lower accuracy of
            <strong>40.4%</strong>. It showed excellent recall for
            <strong>Beginner</strong> users (<strong>0.83</strong>) and the
            highest precision of all models for the
            <strong>Expert</strong> class (<strong>0.85</strong>). However, both
            <strong>Intermediate</strong> and <strong>Advanced</strong> users
            were often misclassified, with precision and recall values falling
            below <strong>0.35</strong>. The model displayed strong performance
            at the boundaries but limited capability in capturing the nuances of
            mid-range expertise levels.
          </p>

          <h3>3. Complement Naive Bayes</h3>
          <p>
            The <code>ComplementNB</code> model reached an accuracy of
            <strong>39.4%</strong>. It had an extremely high recall for
            <strong>Beginner</strong> users (<strong>0.87</strong>) and
            performed moderately well on <strong>Expert</strong> users. However,
            it failed to correctly identify any <strong>Advanced</strong> users
            and showed very poor performance on the
            <strong>Intermediate</strong> group. This indicates that the model
            is overly biased toward the dominant classes, highlighting potential
            class imbalance issues and limitations in generalizing to
            underrepresented categories.
          </p>

          <h3>4. Categorical Naive Bayes</h3>
          <p>
            The <code>CategoricalNB</code> model outperformed all other
            variants, achieving the highest overall accuracy of
            <strong>45.5%</strong>. It demonstrated strong classification
            performance for <strong>Expert</strong> users, with a precision of
            <strong>0.62</strong> and recall of <strong>0.71</strong>. It also
            performed reasonably well on the <strong>Beginner</strong> class.
            Although the <strong>Intermediate</strong> and
            <strong>Advanced</strong> groups still had lower recall, this model
            delivered the most balanced results, showing improved generalization
            across all expertise levels.
          </p>

          <h1>Conclusion</h1>
          <p>
            The Naive Bayes classification approach proved to be effective for
            modeling user expertise levels, especially when using categorical or
            count-based data. Among the four models tested:
          </p>
          <ul>
            <li>
              <strong>CategoricalNB</strong> delivered the best overall
              performance, achieving the highest accuracy and a more balanced
              classification across all classes.
            </li>
            <li>
              <strong>MultinomialNB</strong> and
              <strong>ComplementNB</strong> were strong at identifying the
              extremes (Beginner and Expert) but weak on mid-level classes.
            </li>
            <li>
              <strong>GaussianNB</strong> worked well with continuous features
              and showed high confidence in predictions but struggled with
              overlapping class boundaries.
            </li>
          </ul>
          <p>
            These results highlight the importance of selecting a model that
            aligns with the nature of the input data. Proper preprocessing—such
            as feature selection, discretization, and encoding—is critical to
            enabling each model to reach its full potential. For future work,
            ensemble methods or hybrid approaches could be explored to further
            improve performance, particularly on challenging mid-level
            classifications.
          </p>
          <p>
            Link to complete code -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/naive_bayes_model_impl.ipynb"
              target="_blank"
              >Complete Implementation</a
            >
          </p>
          <p>
            Github Repository -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project"
              target="_blank"
              >Link to github code</a
            >
          </p>
        </div>
        <div id="decisiontree" class="model-content">
          <h1>Overview</h1>
          <h2>Decision Trees</h2>

          <p>
            A <strong>Decision Tree</strong> is a type of supervised machine
            learning algorithm used for both <em>classification</em> and
            <em>regression</em> tasks. The core idea is to split data based on
            feature values into subsets that are increasingly homogeneous with
            respect to the target variable. It mimics human decision-making by
            asking a sequence of if-else questions, forming a tree-like
            structure.
          </p>

          <img
            class="profile-image"
            src="Website/assets/dt_intro_pic.png"
            alt="data columns"
          />

          <p>
            In a decision tree, each <strong>internal node</strong> corresponds
            to a decision based on a feature, each
            <strong>branch</strong> represents the outcome of that decision, and
            each <strong>leaf node</strong> gives the final prediction.
          </p>

          <h2>How Decision Trees Work</h2>
          <p>
            The training process involves selecting the best feature at each
            step to split the data into smaller subsets. This continues
            recursively until a stopping condition is met. For example, in a
            tree predicting Stack Overflow user expertise, a decision node might
            check <code>badge_silver > 50</code> and branch accordingly.
          </p>

          <h2>Common Applications of Decision Trees</h2>
          <ul>
            <li>Medical diagnosis (e.g., predicting disease risk)</li>
            <li>Credit scoring and risk analysis</li>
            <li>Spam detection in emails</li>
            <li>Customer segmentation in marketing</li>
            <li>
              Expertise classification of online users (like Stack Overflow)
            </li>
          </ul>

          <h2>Splitting Criteria: Gini, Entropy, and Information Gain</h2>
          <p>
            To decide which feature to split on at each node, decision trees use
            a criterion that measures how "pure" each resulting group is. Two
            common impurity measures are <strong>Gini Impurity</strong> and
            <strong>Entropy</strong>.
          </p>

          <h3>Gini Impurity</h3>
          <p>
            Gini measures the probability of a random sample being incorrectly
            labeled if it was randomly classified according to the distribution
            of labels in the subset. A lower Gini value means higher purity.
          </p>

          <h3>Entropy</h3>
          <p>
            Entropy measures the amount of uncertainty or disorder. A subset
            with only one class has zero entropy. The formula for entropy is:
          </p>
          <pre>
Entropy(S) = -∑ (pᵢ * log₂(pᵢ))
</pre
          >

          <h3>Information Gain</h3>
          <p>
            Information Gain is the reduction in entropy achieved by
            partitioning the data using a particular feature. It is calculated
            as:
          </p>
          <pre>
Information Gain = Entropy(Parent) - Weighted Average Entropy(Children)
</pre
          >

          <h3>Example: Using Entropy and Information Gain</h3>
          <p>Suppose we have 10 users: 6 are "Expert" and 4 are "Beginner".</p>
          <ul>
            <li>
              <strong>Initial Entropy:</strong> - (6/10)log₂(6/10) -
              (4/10)log₂(4/10) ≈ <strong>0.971</strong>
            </li>
          </ul>

          <p>Now we split based on the <code>is_employee</code> feature:</p>
          <ul>
            <li>Group A: 4 Experts, 1 Beginner → Entropy ≈ 0.721</li>
            <li>Group B: 2 Experts, 3 Beginners → Entropy ≈ 0.971</li>
          </ul>

          <p><strong>Weighted Entropy:</strong></p>
          <pre>
= (5/10 * 0.721) + (5/10 * 0.971) = 0.846
</pre
          >

          <p>
            <strong>Information Gain:</strong> 0.971 - 0.846 =
            <strong>0.125</strong>
          </p>

          <p>
            A higher Information Gain indicates a better split. In this case,
            splitting by <code>is_employee</code> improves the separation of
            classes, but not by a lot. This helps the model decide if this is
            the best feature to split on at this node.
          </p>

          <h2>Why Decision Trees Can Become Infinitely Deep</h2>
          <p>
            Without constraints, a decision tree could keep splitting until each
            leaf node contains only one data point. This results in a tree that
            perfectly fits the training data, known as
            <strong>overfitting</strong>.
          </p>

          <p>
            For instance, if your dataset contains 10,000 unique user records
            with subtle differences, the tree could grow very deep to capture
            all those variations. This makes it complex, hard to interpret, and
            likely to perform poorly on new (test) data.
          </p>

          <h3>How to Prevent Overfitting</h3>
          <p>
            To avoid building excessively large trees, we use parameters such
            as:
          </p>
          <ul>
            <li>
              <strong>max_depth</strong> - Maximum depth of the tree (e.g., 3 or
              5)
            </li>
            <li>
              <strong>min_samples_split</strong> - Minimum samples required to
              split a node
            </li>
            <li>
              <strong>min_samples_leaf</strong> - Minimum samples required in a
              leaf node
            </li>
            <li>
              <strong>pruning</strong> - Reducing the size of the tree after it
              has been built
            </li>
          </ul>
          <h1>Data Preparation</h1>
          <p>
            The dataset used for decision tree modeling consists of Stack
            Overflow user profile attributes, including reputation metrics,
            badge counts, account age, and user demographic data. To facilitate
            classification modeling, the continuous reputation variable was
            transformed into a categorical target variable named
            <code>expertise_level</code>. This was achieved by applying a
            quantile-based discretization strategy that divided users into four
            equally sized categories: Beginner, Intermediate, Advanced, and
            Expert. This transformation enabled the application of supervised
            learning methods designed for multi-class classification tasks.
          </p>

          <p>
            Prior to modeling, several columns deemed irrelevant to the
            predictive task were removed. These included user identifiers (e.g.,
            <code>user_id</code>, <code>account_id</code>), textual or URL-based
            fields (e.g., <code>display_name</code>, <code>link</code>,
            <code>profile_image</code>), and raw datetime variables (e.g.,
            <code>creation_date</code>, <code>last_access_date</code>).
            Additionally, redundant or overly granular features such as daily
            and weekly reputation changes were excluded. The remaining
            attributes—such as badge counts, broader reputation change summaries
            (monthly, quarterly, yearly), <code>accept_rate</code>,
            <code>account_age_days</code>, and <code>age_group</code>—were
            retained as predictive features.
          </p>

          <p>
            Data cleaning procedures were applied to handle missing values and
            ensure compatibility with the decision tree algorithm. Numerical
            attributes were filled with zeroes where missing, and categorical
            variables were label-encoded to convert them into integer-based
            formats. These steps ensured that all feature values were numeric,
            as required for training decision tree classifiers in scikit-learn.
          </p>

          <p>
            To support generalizable evaluation, the dataset was partitioned
            into training and testing sets using an 80-20 split. Stratified
            sampling was applied to preserve the proportion of classes within
            both subsets, thereby preventing class imbalance issues during model
            evaluation. The training set was used to fit the decision tree
            models, while the test set remained disjoint to provide an unbiased
            assessment of model performance.
          </p>

          <p>
            A visual inspection of the prepared datasets confirms that the data
            is clean, fully numeric, and structured appropriately for
            classification modeling. This preprocessing approach ensures that
            the decision tree algorithm can interpret and split data
            effectively, enabling the extraction of meaningful patterns for
            expertise level prediction.
          </p>

          <h3>Raw Data</h3>
          <img
            class="profile-image"
            src="Website/assets/cleaned_data.png"
            alt="data columns"
          />
          <h3>Cleaned Data for Decision Tree</h3>
          <img
            class="profile-image"
            src="Website/assets/cleaned_data_for_decision_tree.png"
            alt="data columns"
          />
          <p>
            Code for preproccesing -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/decision_tree_model_impl.ipynb"
              target="_blank"
              >Decision tree Data Preproccesing Code</a
            >
          </p>
          <p>
            Link to raw dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/cleaned_user_data.csv"
              target="_blank"
              >Raw Dataset</a
            >
          </p>
          <p>
            Link to proccessed dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/cleaned_data_deicion_tree.csv"
              target="_blank"
              >Processed Dataset</a
            >
          </p>
          <h1>Implementation</h1>
          <h3>Model 1 - GINI (All Features, Max Depth = 3)</h3>
          <p>
            The first decision tree was constructed using the GINI impurity
            criterion and incorporated all available features. The model was
            limited to a maximum depth of 3 and achieved an accuracy of
            <strong>45.72%</strong>. It performed well at identifying
            <strong>Beginner</strong> users, demonstrating high recall, and also
            showed strong precision and recall for the
            <strong>Expert</strong> category. However, it struggled with
            <strong>Intermediate</strong> users, failing to classify any
            correctly, and showed limited success with the
            <strong>Advanced</strong> group. The root node selected was
            <code>badge_silver</code>, indicating its significant influence in
            distinguishing expertise levels. While interpretable, the limited
            depth may have prevented the model from capturing more complex
            patterns across all classes.
          </p>
          <h3>Decision Tree for model - 1</h3>
          <img
            class="profile-image2"
            src="Website/assets/dt1.png"
            alt="data columns"
          />

          <h3>Model 2 - GINI (Excluding Root of Model 1)</h3>
          <p>
            The second decision tree excluded <code>badge_silver</code>—the most
            influential feature in Model 1—to encourage variation in tree
            structure. The model was also trained with the GINI criterion and a
            maximum depth of 3. The resulting root node became
            <code>badge_bronze</code>, shifting the focus of the model. It
            achieved a slightly lower accuracy of <strong>43.02%</strong>. The
            model continued to perform well in classifying
            <strong>Expert</strong> users and moderately for
            <strong>Beginner</strong> users. However, like the previous model,
            it completely failed to identify <strong>Intermediate</strong> users
            and had limited accuracy on <strong>Advanced</strong> users. This
            variation highlights how the exclusion of a key feature affects the
            decision-making structure and class balance.
          </p>
          <h3>Decision Tree - model 2</h3>
          <img
            class="profile-image2"
            src="Website/assets/dt2.png"
            alt="data columns"
          />

          <h3>Model 3 - Entropy (Excluding Roots of Model 1 & 2)</h3>
          <p>
            In the third model, both <code>badge_silver</code> and
            <code>badge_bronze</code> were excluded to force the decision tree
            to use alternate features for its root and subsequent splits. This
            tree used the <strong>Entropy</strong> criterion instead of GINI and
            maintained the same maximum depth of 3. The model produced an
            overall accuracy of <strong>41.34%</strong>. Notably, it showed
            improved performance for the <strong>Intermediate</strong> class
            with a recall of 41%, which had been entirely misclassified in the
            previous two models. The root feature was <code>badge_gold</code>,
            with further splits involving reputation change metrics. While
            slightly lower in overall accuracy, this model offered a more
            balanced classification across all expertise levels, indicating
            better generalization in identifying diverse user types.
          </p>
          <h3>Decision Tree - model 3</h3>
          <img
            class="profile-image2"
            src="Website/assets/dt3.png"
            alt="data columns"
          />

          <p>
            Code for Model Implementation -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/decision_tree_model_impl.ipynb"
              target="_blank"
              >Decison Tree Data Implementation Code</a
            >
          </p>
          <h1>Results</h1>

          <h3>Model 1: Decision Tree Using GINI (All Features)</h3>
          <p>
            The first model was trained using the GINI impurity criterion with
            all available features and a maximum tree depth of 3. It achieved an
            accuracy of <strong>45.72%</strong>. Performance was strongest for
            the <strong>Beginner</strong> and <strong>Expert</strong> classes,
            with high recall and precision, particularly in correctly
            identifying users at the two extremes of the expertise spectrum.
          </p>
          <p>
            The model struggled with the <strong>Intermediate</strong> category,
            where no instances were correctly classified, and showed mixed
            performance on <strong>Advanced</strong> users. The root node
            selected was <code>badge_silver</code>, suggesting that this badge
            type played the most significant role in the initial classification
            split. The confusion matrix showed that many
            <strong>Intermediate</strong> and <strong>Advanced</strong> users
            were misclassified into the <strong>Beginner</strong> or
            <strong>Expert</strong> categories, potentially due to overlapping
            feature distributions and the limited tree depth.
          </p>
          <img
            class="profile-image"
            src="Website/assets/dt1_cm.png"
            alt="data columns"
          />

          <h3>
            Model 2: Decision Tree Using GINI (Excluding Root Feature of Model
            1)
          </h3>
          <p>
            To analyze the effect of individual features on model performance,
            the second decision tree was trained after removing
            <code>badge_silver</code>, the root node of the first model. This
            encouraged the model to explore different decision paths. The new
            tree used <code>badge_bronze</code> as the root and maintained the
            GINI criterion with the same depth constraint.
          </p>
          <p>
            The accuracy dropped slightly to <strong>43.02%</strong>. Similar to
            the first model, <strong>Expert</strong> users were classified with
            the highest accuracy, followed by <strong>Beginner</strong> users.
            However, there was no improvement in recognizing
            <strong>Intermediate</strong> users, and classification of
            <strong>Advanced</strong> users remained limited. The results
            highlight the sensitivity of decision trees to root feature
            selection and the influence of feature hierarchy on downstream
            splits.
          </p>
          <img
            class="profile-image"
            src="Website/assets/dt2_cm.png"
            alt="data columns"
          />

          <h3>
            Model 3: Decision Tree Using Entropy (Excluding Roots of Models 1 &
            2)
          </h3>
          <p>
            The third model was designed to diversify the root structure by
            excluding both <code>badge_silver</code> and
            <code>badge_bronze</code>, the initial split features in Models 1
            and 2. This tree used the <strong>Entropy</strong> criterion instead
            of GINI and retained the maximum depth of 3.
          </p>
          <p>
            The overall accuracy was <strong>41.34%</strong>, slightly lower
            than the previous models, but the performance was more balanced
            across the four classes. The model correctly identified a
            significant number of <strong>Intermediate</strong> users, which had
            not been the case in earlier models. Additionally,
            <strong>Expert</strong> and <strong>Beginner</strong> predictions
            remained relatively accurate. The root node chosen was
            <code>badge_gold</code>, indicating that this feature provided the
            highest information gain among the remaining set.
          </p>
          <p>
            The improved detection of <strong>Intermediate</strong> users and
            use of temporal activity metrics such as
            <code>reputation_change_month</code> and
            <code>reputation_change_quarter</code> suggest that a combination of
            behavioral features and refined splitting criteria can enhance
            performance in classifying mid-range expertise levels.
          </p>
          <img
            class="profile-image"
            src="Website/assets/ct3_cm,.png"
            alt="data columns"
          />

          <h1>Conclusion</h1>

          <p>
            This decision tree modeling exercise offered valuable insights into
            classifying Stack Overflow users based on their activity and
            reputation attributes. Through a systematic implementation of three
            decision tree models, it was possible to observe the effects of
            feature selection, splitting criteria, and tree depth on
            classification outcomes.
          </p>

          <p>
            Model 1, which included all features and used the GINI criterion,
            provided a baseline performance with the highest accuracy among the
            three models. It showed strong classification capability for users
            at the extremes of the expertise spectrum but struggled to
            distinguish mid-level classes such as Intermediate and Advanced.
            This highlighted the impact of class overlap and limited model
            depth.
          </p>

          <p>
            Model 2 demonstrated how excluding a dominant root feature
            (badge_silver) prompted the tree to reconfigure its decision-making
            process. Although the overall accuracy slightly declined, this model
            reinforced the influence of feature hierarchy and suggested that
            certain features play a pivotal role in classification structure.
          </p>

          <p>
            Model 3 introduced an Entropy-based criterion and excluded the
            primary root features used in previous models. While the accuracy
            was slightly lower, this model achieved better balance across all
            four classes, particularly improving recognition of the Intermediate
            class. The use of temporal and behavioral features in this model
            emphasized the importance of diversified feature sets when dealing
            with multi-class problems.
          </p>

          <p>
            Overall, the analysis underlined several key learning outcomes: the
            role of impurity measures in splitting decisions, the
            interpretability of decision tree structures, and the trade-offs
            between accuracy, depth, and feature inclusion. It also demonstrated
            the challenges in distinguishing between closely related classes and
            the potential of ensemble or deeper models for future improvements.
          </p>

          <p>
            Link to complete code -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/decision_tree_model_impl.ipynb"
              target="_blank"
              >Complete Implementation</a
            >
          </p>
          <p>
            Github Repository -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project"
              target="_blank"
              >Link to github code</a
            >
          </p>
        </div>
        <div id="Regression" class="model-content">
          <h1>Overview</h1>

          <h2>Regression</h2>

          <h3>Understanding Linear Regression</h3>
          <p>
            Linear regression is one of the most fundamental algorithms in
            supervised machine learning. It is used to model the relationship
            between a dependent variable (also called the target or output) and
            one or more independent variables (features or inputs). The
            algorithm assumes that this relationship can be represented as a
            straight line in the form of <em>y = mx + c</em>, where
            <em>m</em> is the slope and <em>c</em> is the intercept. The goal of
            linear regression is to find the best-fitting line by minimizing the
            difference between the predicted values and the actual values in the
            training data. This is done using the least squares method, which
            calculates the line that minimizes the sum of squared errors. Linear
            regression is commonly applied in forecasting trends, predicting
            prices, and analyzing relationships between variables.
          </p>

          <h3>Understanding Logistic Regression</h3>
          <p>
            Logistic regression, despite its name, is a classification algorithm
            rather than a regression one. It is used when the target variable is
            categorical, particularly for binary classification tasks where the
            outcome is either 0 or 1. Instead of predicting a continuous value
            like linear regression, logistic regression predicts the probability
            that a given input belongs to a particular class. It does this by
            applying the sigmoid (or logistic) function to the linear
            combination of input features, which maps the result to a value
            between 0 and 1. If the probability is greater than a certain
            threshold (commonly 0.5), the instance is classified as class 1,
            otherwise as class 0. Logistic regression is widely used in
            applications such as medical diagnosis, spam detection, and credit
            risk modeling.
          </p>
          <img
            class="profile-image2"
            src="Website/assets/logis_intro.jpeg"
            alt="data columns"
          />

          <h3>
            Similarities and Differences Between Linear and Logistic Regression
          </h3>
          <p>
            Both linear and logistic regression are supervised learning
            algorithms that involve learning a linear combination of input
            features to make predictions. They both assume a linear relationship
            between the input variables and the output (in the case of logistic
            regression, this relationship is with the log-odds of the outcome).
            However, the key difference lies in their purpose and output: linear
            regression is used for predicting continuous values, while logistic
            regression is used for predicting class probabilities. Another
            important distinction is in the loss function used during
            training—linear regression minimizes mean squared error, whereas
            logistic regression maximizes likelihood using the cross-entropy
            loss. Additionally, logistic regression includes the sigmoid
            function, which makes it suitable for classification tasks by
            constraining the output to a [0,1] range.
          </p>

          <h3>The Role of the Sigmoid Function in Logistic Regression</h3>
          <p>
            The sigmoid function is a crucial part of logistic regression, as it
            allows the model to convert a linear combination of input features
            into a probability. The sigmoid function is defined as
            <em>σ(z) = 1 / (1 + e<sup>−z</sup>)</em>, where <em>z</em> is the
            weighted sum of input features. This function squashes any
            real-valued number into a range between 0 and 1, making it
            interpretable as a probability score. This is particularly useful in
            binary classification, where predictions must reflect the likelihood
            of belonging to class 1. Once this probability is calculated, a
            threshold (commonly 0.5) is used to assign a final class label.
            Without the sigmoid function, logistic regression would not be able
            to provide meaningful class probabilities.
          </p>

          <h3>
            Connection Between Logistic Regression and Maximum Likelihood
            Estimation
          </h3>
          <p>
            Logistic regression is trained using a statistical approach called
            maximum likelihood estimation (MLE). The objective of MLE is to find
            the set of parameters (weights) that make the observed data most
            probable under the model. In logistic regression, this means
            adjusting the model’s parameters so that the predicted probabilities
            of the actual class labels are maximized. The likelihood function is
            defined using the predicted probabilities for all training examples,
            and the logarithm of this likelihood (log-likelihood) is optimized
            during training. This optimization process ensures that the logistic
            regression model finds parameters that best separate the classes
            based on the input features. Unlike linear regression which uses
            squared errors, logistic regression uses a likelihood-based
            objective that is more suited to classification tasks.
          </p>
          <h1>Data Preparation</h1>

          <p>
            The reputation scores were divided into four quantile-based
            categories labeled as Beginner, Intermediate, Advanced, and Expert
            using the <code>qcut()</code> function. To frame this as a binary
            classification task, these categories were then grouped into two
            broader classes: <strong>Low Expertise</strong> (combining Beginner
            and Intermediate) and <strong>High Expertise</strong> (combining
            Advanced and Expert). A new column <code>expertise_binary</code> was
            created to reflect this transformation, and the original
            <code>expertise_level</code> and <code>reputation</code> columns
            were dropped.
          </p>

          <p>
            To eliminate irrelevant or non-contributory features, columns such
            as <code>user_id</code>, <code>display_name</code>,
            <code>profile_image</code>, <code>link</code>,
            <code>account_id</code>, and raw date-related fields were removed.
            These columns are either identifiers, textual, or timestamps that do
            not directly influence classification outcomes and may introduce
            noise.
          </p>

          <p>
            Missing values were addressed by filling them with zero, ensuring
            consistency across the dataset. Categorical variables were encoded
            using label encoding to convert them into numerical values, which is
            essential for compatibility with machine learning models. The newly
            created binary target column <code>expertise_binary</code> was also
            label encoded into <code>expertise_binary_encoded</code> with values
            0 for "Low" and 1 for "High".
          </p>

          <p>
            The final step involved splitting the dataset into training and
            testing sets using an 80/20 ratio. Stratified sampling was applied
            to maintain the original class distribution across both sets,
            thereby preventing class imbalance during model evaluation. This
            clean, encoded, and balanced dataset was then used to train and test
            both the Logistic Regression and Multinomial Naive Bayes models.
          </p>
          <h3>Raw Data</h3>
          <img
            class="profile-image"
            src="Website/assets/cleaned_data.png"
            alt="data columns"
          />
          <h3>Cleaned Data for Regression</h3>
          <img
            class="profile-image"
            src="Website/assets/cleaned_for_logis.png"
            alt="data columns"
          />
          <p>
            Code for preproccesing -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/regression_model_impl.ipynb"
              target="_blank"
              >Regression Data Preproccesing Code</a
            >
          </p>
          <p>
            Link to raw dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/cleaned_user_data.csv"
              target="_blank"
              >Raw Dataset</a
            >
          </p>
          <p>
            Link to proccessed dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/cleaned_data_for_regression.csv"
              target="_blank"
              >Processed Dataset</a
            >
          </p>
          <h1>Implementation</h1>

          <h3>Logistic Regression</h3>
          <p>
            Logistic Regression was chosen as the first model for binary
            classification of user expertise. The cleaned dataset was split into
            training and testing sets using an 80/20 ratio with stratification
            to preserve the class distribution. The
            <code>LogisticRegression</code> model from scikit-learn was used
            with a maximum iteration cap of 200 to ensure convergence. After
            training the model on the training data, predictions were made on
            the test set.
          </p>
          <p>
            Evaluation was carried out using accuracy, precision, recall,
            F1-score, and a confusion matrix. The results indicated that
            Logistic Regression achieved an accuracy of <strong>73.64%</strong>.
            It demonstrated a balanced performance across both expertise levels,
            with strong precision and recall for both Low and High Expertise
            categories. The model offered clear interpretability and
            generalizability, making it a reliable baseline for classification
            in this context.
          </p>

          <h3>Multinomial Naive Bayes</h3>
          <p>
            The second model implemented was Multinomial Naive Bayes, which is
            particularly effective for categorical data and commonly used in
            text classification problems. Since this algorithm requires all
            input features to be non-negative, any negative values in the
            dataset were replaced with zeros. The same training and testing
            split was used as in the Logistic Regression model to allow for a
            fair comparison.
          </p>
          <p>
            After training, the model achieved an accuracy of
            <strong>67.59%</strong>. While it showed high recall for the High
            Expertise class (85%), it struggled with the Low Expertise group,
            achieving only 50% recall, indicating a higher rate of false
            negatives. The imbalance in class-wise performance suggests that the
            model favored predicting High Expertise and had lower precision.
            Despite this, Multinomial Naive Bayes remained computationally
            efficient and simple to implement.
          </p>

          <p>
            Link to Implementation code -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/regression_model_impl.ipynb"
              target="_blank"
              >Implementation</a
            >
          </p>

          <h1>Results</h1>

          <h3>Logistic Regression</h3>
          <p>
            The Logistic Regression model achieved an overall accuracy of
            <strong>73.64%</strong>, indicating effective performance in
            classifying users as either "Low Expertise" or "High Expertise." The
            model demonstrated a good balance between precision and recall,
            especially for the High Expertise category.
          </p>
          <p>
            For Low Expertise, the model achieved a precision of 0.77 and a
            recall of 0.67. This indicates that while most predictions labeled
            as Low were correct, the model still missed 33% of actual Low
            Expertise users. In contrast, High Expertise classification yielded
            a precision of 0.71 and a recall of 0.81, showing that the model was
            better at identifying users with high expertise.
          </p>
          <p>
            The confusion matrix shows that out of 10,479 Low Expertise users,
            6,990 were correctly classified, while 3,489 were misclassified. For
            the 10,481 High Expertise users, 8,445 were correctly identified,
            and 2,036 were incorrectly labeled. Overall, the model’s macro and
            weighted averages for precision, recall, and F1-score were all
            approximately 0.74, showing consistent and reliable performance
            across both classes.
          </p>
          <img
            class="profile-image"
            src="Website/assets/logis.png"
            alt="data columns"
          />

          <h3>Multinomial Naive Bayes</h3>
          <p>
            The Multinomial Naive Bayes model achieved a lower overall accuracy
            of <strong>67.59%</strong>, indicating moderate effectiveness in the
            binary classification task. Although it was computationally
            efficient, its class-wise performance was not as balanced as
            Logistic Regression.
          </p>
          <p>
            For Low Expertise, the model scored a precision of 0.77 but had a
            significantly lower recall of 0.50, meaning that while correct
            predictions were high when it predicted Low Expertise, half of the
            actual Low users were misclassified. Conversely, the High Expertise
            class had a precision of 0.63 and a higher recall of 0.85, showing a
            bias toward identifying High Expertise users even at the cost of
            increased false positives.
          </p>
          <p>
            This imbalance is reflected in the confusion matrix, where the model
            correctly predicted only 5,239 Low Expertise users and misclassified
            5,240. However, for High Expertise, it correctly identified 8,905
            out of 10,481 users. The macro average of precision, recall, and
            F1-score ranged from 0.67 to 0.70, reinforcing the model’s tendency
            to favor one class over another.
          </p>
          <img
            class="profile-image"
            src="Website/assets/reg_nb.png"
            alt="data columns"
          />

          <h3>Comparison of Models</h3>
          <p>
            When comparing the two models, Logistic Regression outperformed
            Multinomial Naive Bayes in both overall accuracy and class-wise
            balance. Logistic Regression offered higher recall for Low Expertise
            users (0.67 vs. 0.50) and better overall F1-scores across both
            classes. This resulted in a more consistent performance, especially
            for applications where balanced classification is important.
          </p>
          <p>
            Multinomial Naive Bayes, although fast and easy to implement, showed
            a bias toward the High Expertise class, with a higher recall (0.85)
            but lower precision (0.63). In contrast, Logistic Regression
            provided more reliable predictions for both groups, maintaining
            balanced precision-recall tradeoffs.
          </p>
          <p>
            In summary, Logistic Regression is the more suitable choice for this
            classification task due to its balanced and accurate predictions. It
            offers better interpretability and generalizability, making it
            preferable when model fairness and performance across classes are
            essential.
          </p>

          <h1>Summary</h1>

          <p>
            This project focused on applying supervised machine learning
            algorithms—Logistic Regression and Multinomial Naive Bayes—to
            classify Stack Overflow users into two broad expertise levels:
            <strong>Low Expertise</strong> and <strong>High Expertise</strong>.
            The classification was derived by transforming a multi-class
            reputation-based label into a binary target, enabling focused
            evaluation using widely-used linear and probabilistic models.
          </p>

          <p>
            The data was preprocessed by removing irrelevant or redundant
            features, encoding categorical variables, and ensuring non-negative
            inputs required by Naive Bayes. Following this, both models were
            trained and evaluated on the same dataset using an 80-20 train-test
            split with stratification to preserve class distribution.
          </p>

          <p>
            In terms of performance, <strong>Logistic Regression</strong> proved
            to be more balanced and accurate with an overall accuracy of
            <strong>73.64%</strong>. It maintained good precision and recall
            across both classes and demonstrated better generalizability.
            <strong>Multinomial Naive Bayes</strong>, while computationally
            efficient, achieved a lower accuracy of <strong>67.59%</strong> and
            showed a noticeable bias toward classifying users as High Expertise,
            resulting in a higher false negative rate for Low Expertise users.
          </p>

          <p>
            This comparison highlighted the importance of model selection based
            on class balance and prediction reliability. Logistic Regression
            emerged as a more robust model for binary classification in this
            context, especially when fair classification of both expertise
            groups is important.
          </p>

          <p>
            Future enhancements may include experimenting with other
            classification algorithms, performing hyperparameter tuning, and
            incorporating more engineered features such as post frequency,
            question-answer ratios, or time-based activity metrics to further
            improve model performance and interpretability.
          </p>

          <p>
            Link to complete code -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/regression_model_impl.ipynb"
              target="_blank"
              >Complete Implementation</a
            >
          </p>
          <p>
            Github Repository -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project"
              target="_blank"
              >Link to github code</a
            >
          </p>
        </div>
        <div id="svm" class="model-content">
          <h1>Overview</h1>

          <h2>SVM</h2>

          <!-- <h2>What are SVMs?</h2>
          <p>
            Support Vector Machines (SVMs) are powerful supervised machine
            learning models used for both classification and regression tasks.
            The main objective of an SVM is to find an optimal boundary—called a
            <strong>hyperplane</strong>—that best separates different classes in
            the dataset. For binary classification, this hyperplane is a line
            (in 2D) or a flat surface (in higher dimensions) that maximizes the
            margin between the two classes.
          </p>
          <p>
            The closest data points to the hyperplane from each class are known
            as <strong>support vectors</strong>. These support vectors are
            crucial in defining the decision boundary and influence the
            orientation and position of the hyperplane.
          </p>

          <h2>Why SVMs are Linear Separators</h2>
          <p>
            In their basic form, SVMs function as linear classifiers. They look
            for the straight line (or hyperplane) that separates the two classes
            with the maximum margin. This makes SVMs linear separators when
            working with linearly separable data. However, most real-world
            datasets are not linearly separable, and SVMs address this by using
            <strong>kernel functions</strong> to project data into
            higher-dimensional spaces where a linear separator can be applied.
          </p>

          <h2>Kernel Functions and the Importance of the Dot Product</h2>
          <p>
            The kernel function enables SVMs to work in higher-dimensional
            feature spaces without explicitly transforming the data. This is
            achieved through the <strong>kernel trick</strong>, which computes
            the <strong>dot product</strong> of data points in the transformed
            space efficiently. The dot product is central because it allows the
            SVM to evaluate similarity in the high-dimensional space without
            computing the coordinates explicitly.
          </p>
          <p>
            By evaluating the dot product in the transformed space, kernels help
            capture non-linear relationships while still allowing the SVM to
            operate using a linear classifier in this new space.
          </p>

          <h2>Popular Kernel Types</h2>

          <h3>Polynomial Kernel</h3>
          <p>
            The polynomial kernel enables the SVM to fit curved boundaries by
            including interactions between features. Its mathematical form is:
          </p>
          <p>
            <code>K(x, y) = (x · y + r)<sup>d</sup></code>
          </p>
          <p>
            Where <code>x · y</code> is the dot product of the input vectors,
            <code>r</code> is a constant (often 1), and <code>d</code> is the
            degree of the polynomial. Higher degrees lead to more complex
            decision boundaries.
          </p>

          <h3>Radial Basis Function (RBF) Kernel</h3>
          <p>
            The RBF kernel, also known as the Gaussian kernel, is useful for
            capturing intricate decision boundaries. Its formula is:
          </p>
          <p><code>K(x, y) = exp(-γ ||x - y||²)</code></p>
          <p>
            Here, <code>γ</code> controls the width of the Gaussian function,
            and <code>||x - y||²</code> is the squared Euclidean distance
            between vectors <code>x</code> and <code>y</code>. RBF kernels are
            effective when the relationship between class labels and features is
            highly non-linear.
          </p>

          <h2>Example: Mapping a 2D Point Using Polynomial Kernel</h2>
          <p>
            Suppose we have a 2D input vector <code>x = (x₁, x₂)</code>. Using a
            polynomial kernel with <code>r = 1</code> and <code>d = 2</code>, we
            can map this vector into a higher-dimensional space:
          </p>
          <p>
            The transformed feature vector becomes:
            <code>(x₁², √2·x₁·x₂, x₂², √2·x₁, √2·x₂, 1)</code>
          </p>
          <p>
            This allows the SVM to draw a linear decision boundary in this 6D
            space, which translates to a non-linear curve in the original 2D
            space. This is how SVMs handle non-linearly separable data using
            kernels.
          </p>

          <h2>Visual Aids</h2>
          <ul>
            <li>
              <strong>Image 1:</strong> Linear SVM showing a straight-line
              boundary with support vectors highlighted.
            </li>
            <li>
              <strong>Image 2:</strong> RBF kernel SVM demonstrating a curved
              boundary separating two complex classes.
            </li>
          </ul>
          <p>
            These visualizations highlight how SVMs adjust their decision
            boundaries depending on the kernel used and the underlying structure
            of the data.
          </p>
          <h1>Data Preparation</h1>

          <p>
            For SVM modeling, the dataset underwent essential preprocessing
            steps to ensure compatibility with supervised learning requirements.
            Irrelevant and non-contributory columns—such as user identifiers,
            textual display names, URLs, timestamps, and other metadata—were
            dropped to remove noise and enhance feature clarity.
          </p>

          <p>
            Categorical features were label-encoded to transform text values
            into numerical format. Missing values were filled with zeroes to
            maintain data consistency. The target column,
            <code>expertise_binary</code>, indicating user expertise level as
            either "Low" or "High", was label-encoded into
            <code>expertise_binary_encoded</code> with 0 and 1 values,
            respectively.
          </p>

          <p>
            An 80/20 train-test split was applied using stratified sampling to
            preserve the original distribution of the target classes across both
            subsets. This stratification is crucial to ensure unbiased model
            training and evaluation. As SVMs are sensitive to feature scaling,
            standardization was performed using <code>StandardScaler</code> to
            normalize all feature values to a standard range.
          </p>

          <p>
            The same train-test split and scaled dataset were used consistently
            across all three SVM kernel experiments (Linear, Polynomial, and
            RBF) to ensure fair comparison of model performance. All features
            were numeric and properly labeled, satisfying SVM's strict input
            requirements.
          </p>
          <h3>Raw Data</h3>
          <img
            class="profile-image"
            src="Website/assets/cleaned_data.png"
            alt="data columns"
          />
          <h3>Cleaned Data for Regression</h3>
          <img
            class="profile-image"
            src="Website/assets/cleaned_data_for_decision_tree.png"
            alt="data columns"
          />
          <p>
            Code for preproccesing -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/arm_model_impl.ipynb"
              target="_blank"
              >Regression Data Preproccesing Code</a
            >
          </p>
          <p>
            Link to raw dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/cleaned_user_data.csv"
              target="_blank"
              >Raw Dataset</a
            >
          </p>
          <p>
            Link to proccessed dataset -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Dataset/user_data_cleaned_for_arm.csv"
              target="_blank"
              >Processed Dataset</a
            >
          </p>

          <h1>Implementation</h1>
          <p>
            To perform classification using Support Vector Machines (SVM), we
            implemented three kernel types—Linear, Polynomial, and Radial Basis
            Function (RBF)—on the prepared dataset. After preprocessing, the
            data was split into an 80/20 train-test ratio using stratified
            sampling to maintain balanced class distributions. Standardization
            was then applied to ensure all features had the same scale, which is
            essential for SVM to perform optimally.
          </p>

          <p>
            The first model used a <strong>Linear kernel</strong> with a
            regularization parameter (C) of 1. This model achieved an accuracy
            of <strong>74.98%</strong>. It was particularly strong in detecting
            high expertise users with a recall of 0.85, though it missed some
            low expertise cases. The linear kernel provided a strong and
            interpretable baseline.
          </p>

          <p>
            Next, a <strong>Polynomial kernel</strong> with degree 3 and C = 1
            was applied. This model slightly underperformed compared to the
            linear version, with an accuracy of <strong>73.23%</strong>. It
            exhibited balanced performance across classes but did not
            significantly improve overall classification. This suggests that
            introducing non-linearity helped capture complex patterns but also
            added slight overfitting.
          </p>

          <p>
            Finally, the <strong>RBF kernel</strong> (C = 1, gamma = 'scale')
            yielded the highest accuracy of <strong>75.65%</strong>. It showed a
            strong ability to identify high expertise users (recall: 0.87) while
            maintaining good precision. Although it slightly underperformed in
            identifying low expertise users (recall: 0.64), its overall
            generalization was superior. The RBF kernel's performance suggests
            it is well-suited for this classification task.
          </p>

          <p>
            A comparative bar chart was created to visualize the performance of
            each kernel. The results highlighted that kernel choice
            significantly impacts model performance, with the RBF kernel
            standing out as the most effective in this context.
          </p>

          <p>
            This implementation demonstrates the impact of different kernels on
            classification tasks and provides a framework for evaluating SVM
            models through accuracy, precision, recall, and confusion matrices.
          </p>
          <h1>Results</h1>
          <p>
            The Support Vector Machine (SVM) models were evaluated based on
            their ability to classify users into Low and High Expertise
            categories. Each model used a different kernel—Linear, Polynomial,
            and RBF—to explore how different transformations of the input space
            affect classification performance.
          </p>

          <p>
            The <strong>Linear kernel</strong> achieved an accuracy of
            <strong>74.98%</strong>, demonstrating solid overall performance. It
            was particularly effective in identifying High Expertise users with
            a recall of 0.85, though it was slightly less accurate with Low
            Expertise users. The confusion matrix indicated a higher number of
            misclassifications for the Low class, suggesting that the linear
            decision boundary may not fully capture the underlying complexity of
            the data.
          </p>

          <p>
            The <strong>Polynomial kernel</strong> reached an accuracy of
            <strong>73.23%</strong>. It provided fairly balanced precision and
            recall across both classes, with minor improvements in Low Expertise
            predictions compared to the linear model. However, the overall
            improvement was not significant, which suggests that the added
            complexity of polynomial transformation did not lead to better
            generalization for this specific dataset.
          </p>

          <p>
            The <strong>RBF kernel</strong> performed the best, achieving an
            accuracy of <strong>75.65%</strong>. It delivered high recall for
            High Expertise users (0.87) while maintaining good precision for
            both classes. Although it slightly underperformed in detecting Low
            Expertise users, it provided the most balanced and generalizable
            results overall. The confusion matrix further supported this,
            showing that the RBF model was the most effective in reducing false
            positives and capturing true positives across the board.
          </p>

          <p>
            A comparative bar chart illustrated the accuracy of all three
            kernels, clearly highlighting the superiority of the RBF kernel for
            this task. These results demonstrate how different kernel choices
            influence model effectiveness and support the selection of the RBF
            kernel for optimal classification performance.
          </p>
          <h1>Summary</h1>
          <p>
            This project applied Support Vector Machines (SVMs) to a binary
            classification task of predicting user expertise levels. Three
            different kernels—Linear, Polynomial, and RBF—were tested to assess
            their effectiveness in separating Low and High Expertise categories
            based on user data. The data was thoroughly cleaned, encoded, and
            standardized to meet the requirements of the SVM algorithm.
          </p>

          <p>
            The evaluation revealed that the
            <strong>RBF kernel</strong> delivered the best overall performance,
            achieving an accuracy of <strong>75.65%</strong> and strong recall
            for High Expertise users. The <strong>Linear kernel</strong> also
            performed well with <strong>74.98%</strong> accuracy, making it a
            strong baseline model. The <strong>Polynomial kernel</strong>, while
            capable of modeling non-linear relationships, provided the lowest
            performance in this context with an accuracy of
            <strong>73.23%</strong>.
          </p>

          <p>
            This comparison highlights the importance of kernel selection when
            using SVMs, as different kernels can significantly impact
            classification performance. The RBF kernel's ability to capture
            complex patterns in the data makes it particularly suitable for this
            use case. Future work could explore hyperparameter tuning, feature
            selection, or ensemble methods to further enhance performance.
          </p>

          <p>
            Link to complete code -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/pca_model_impl.ipynb"
              target="_blank"
              >Complete Implementation</a
            >
          </p>
          <p>
            Github Repository -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project"
              target="_blank"
              >Link to github code</a
            >
          </p> -->
        </div>
        <div id="Ensemble" class="model-content">
          <h1>Overview</h1>

          <h2>Ensemble Methods</h2>

          <!-- <h2>Concept and Purpose</h2>
          <p>
            Ensemble learning is a machine learning approach that combines
            multiple models to produce a single, more accurate and stable
            predictive outcome. The goal is to enhance performance by reducing
            errors that individual models might make due to bias, variance, or
            limited complexity. This strategy is particularly effective when
            dealing with complex datasets, as it leverages the strengths of
            various base models.
          </p>

          <h2>Improving Accuracy with Ensembles</h2>
          <p>
            By aggregating the predictions of several models, ensemble
            techniques create a more generalized solution than any single model
            could provide. This collective approach improves accuracy and
            reduces overfitting, making ensemble learning a powerful tool in
            both classification and regression tasks.
          </p>

          <h2>Common Ensemble Techniques</h2>

          <h3>Bagging (Bootstrap Aggregating)</h3>
          <p>
            Bagging involves training multiple instances of a base estimator on
            different random subsets of the data, drawn with replacement.
            Predictions are combined through averaging (for regression) or
            majority voting (for classification). Random Forest is a well-known
            bagging technique that uses decision trees as base learners.
          </p>

          <h3>Boosting</h3>
          <p>
            Boosting builds models sequentially, where each new model focuses on
            correcting the errors of the previous one. This technique gradually
            improves performance by emphasizing difficult cases. AdaBoost,
            Gradient Boosting, and XGBoost are common boosting algorithms.
          </p>

          <h3>Stacking</h3>
          <p>
            Stacking combines multiple models by training a meta-model that
            learns to blend the predictions of the base models. Each base model
            contributes its output to the final decision, allowing diverse
            learning algorithms to collaborate effectively.
          </p>

          <h3>Voting</h3>
          <p>
            Voting ensembles combine predictions from different models by
            selecting the most frequent class label (hard voting) or by
            averaging probability scores (soft voting). This technique is
            straightforward and useful when multiple strong but diverse models
            are available.
          </p>

          <h2>Use in Classification Tasks</h2>
          <p>
            Ensemble learning is widely applied in scenarios that require
            reliable and accurate predictions, including fraud detection, spam
            filtering, recommendation systems, and medical diagnosis. In
            classification tasks, these methods are particularly useful for
            handling imbalanced or noisy data by improving prediction
            robustness.
          </p>

          <h2>Application in the Current Project</h2>
          <p>
            The ensemble learning approach was applied to classify Stack
            Overflow users into binary expertise levels: Low and High. Two
            ensemble methods—Random Forest and AdaBoost—were implemented and
            compared using a preprocessed and scaled dataset. Each model was
            trained on the same training set and evaluated using accuracy,
            classification reports, and confusion matrices.
          </p>

          <h2>Visual Representation</h2>
          <p>
            The diagram below illustrates how ensemble models integrate outputs
            from multiple weak learners to form a strong final prediction.
          </p>
          <img
            src="assets/ensemble_structure.png"
            alt="Ensemble Learning Structure"
            width="600"
          />

          <p>
            A separate visual comparison of accuracy scores achieved by each
            ensemble method highlights the performance of different techniques.
          </p>
          <img
            src="assets/ensemble_accuracy_chart.png"
            alt="Ensemble Model Accuracy Chart"
            width="600"
          />
          <h1>Data Preparation</h1>

          <h2>Data Cleaning and Transformation</h2>
          <p>
            The original dataset included various user-specific details such as
            user IDs, display names, account links, and profile metadata. Since
            these attributes do not contribute meaningfully to the predictive
            task, they were removed during the cleaning phase. Additionally, any
            missing values present in the dataset were filled with zeros to
            maintain consistency and avoid training disruptions.
          </p>

          <h2>Target Label Creation</h2>
          <p>
            The reputation scores in the dataset were segmented into four
            quantile-based categories: Beginner, Intermediate, Advanced, and
            Expert. These were then grouped into two broader classes—Low
            Expertise (Beginner + Intermediate) and High Expertise (Advanced +
            Expert)—to simplify the problem into a binary classification task. A
            new column was added to store these binary labels.
          </p>

          <h2>Encoding and Feature Engineering</h2>
          <p>
            To ensure compatibility with machine learning algorithms, all
            categorical features were encoded into numerical values using label
            encoding. This step is essential as most ensemble models like Random
            Forest and AdaBoost require purely numeric input features. The
            binary target label was also encoded into a new numeric column for
            model training.
          </p>

          <h2>Train-Test Split</h2>
          <p>
            The cleaned and encoded dataset was split into training and testing
            sets using an 80/20 ratio. Stratified sampling was applied to
            preserve the class distribution across both sets. This helps ensure
            that both the training and testing sets reflect the same proportions
            of Low and High Expertise users, which is crucial for evaluating
            model performance reliably.
          </p>

          <h2>Feature Scaling</h2>
          <p>
            Ensemble methods such as Random Forest do not require feature
            scaling, but for consistency across all ensemble models,
            particularly for methods like AdaBoost and gradient-based
            approaches, standard scaling was applied to normalize the features.
            This step transforms all features to have zero mean and unit
            variance, improving convergence and stability for some models.
          </p>

          <h2>Visual Snapshots</h2>
          <p>
            Below are sample snapshots of the data before and after
            preprocessing, showing the cleaned structure and the final set of
            features used for modeling.
          </p>
          <h1>Implementation</h1>
          <p>
            The implementation of ensemble learning models began with
            preprocessing the dataset to prepare it for classification. After
            encoding categorical variables and scaling numerical features, the
            dataset was split into training and testing sets using an 80/20
            stratified approach. This ensured balanced representation of both
            classes—Low and High Expertise—across the sets. Standard scaling was
            applied to normalize feature ranges, which is particularly helpful
            for ensemble models like AdaBoost.
          </p>
          <p>
            Two popular ensemble learning techniques were implemented: Random
            Forest and AdaBoost. The Random Forest classifier was configured
            with 100 decision trees to perform bagging and reduce variance by
            combining multiple predictions. The AdaBoost classifier, on the
            other hand, was built using a series of weak learners—specifically
            shallow decision trees with a maximum depth of 1—and iteratively
            adjusted weights to focus on previously misclassified instances.
          </p>
          <p>
            Both models were trained on the scaled training data and evaluated
            on the test set using metrics such as accuracy, precision, recall,
            and F1-score. Additionally, confusion matrices were plotted to
            visually analyze the performance of each model. A final comparison
            chart was created to show overall accuracy across the ensemble
            methods and evaluate their relative effectiveness. This structured
            approach enabled a fair and interpretable comparison of ensemble
            learning techniques for binary expertise classification.
          </p>
          <h1>Results</h1>

          <h2>Random Forest Results</h2>
          <p>
            The <strong>Random Forest</strong> classifier achieved an overall
            accuracy of <strong>75.01%</strong>, reflecting strong and balanced
            classification performance. For the <em>Low Expertise</em> class, it
            achieved a precision of <strong>0.77</strong> and recall of
            <strong>0.71</strong>, indicating reliable identification with some
            missed cases. For the <em>High Expertise</em> class, the recall was
            <strong>0.79</strong> and precision was <strong>0.73</strong>,
            showing effectiveness in retrieving high expertise users.
          </p>
          <p>
            The confusion matrix shows <strong>7,467</strong> correctly
            identified Low Expertise users and <strong>8,255</strong> High
            Expertise users. The macro F1-score of
            <strong>0.75</strong> supports the model's consistency and balance
            across both classes.
          </p>

          <h2>AdaBoost Results</h2>
          <p>
            The <strong>AdaBoost</strong> classifier produced similar
            performance with an accuracy of <strong>74.81%</strong>. It showed a
            precision of <strong>0.78</strong> and recall of
            <strong>0.69</strong> for the <em>Low Expertise</em> class. For
            <em>High Expertise</em>, the model achieved a recall of
            <strong>0.81</strong> and precision of <strong>0.72</strong>,
            suggesting it was more effective at classifying high expertise
            users.
          </p>
          <p>
            According to the confusion matrix, <strong>7,213</strong> Low
            Expertise and <strong>8,467</strong> High Expertise users were
            correctly classified. The macro average F1-score of
            <strong>0.75</strong> confirms consistent and balanced performance,
            with a slight bias toward High Expertise predictions.
          </p>

          <h2>Model Comparison</h2>
          <p>
            The bar chart below compares the accuracy of both ensemble models:
          </p>
          <ul>
            <li><strong>Random Forest:</strong> 75.01%</li>
            <li><strong>AdaBoost:</strong> 74.81%</li>
          </ul>
          <p>
            Both models demonstrated closely matched performance. Random Forest
            offered a slightly higher accuracy and stronger class balance, while
            AdaBoost excelled in identifying High Expertise users. These
            findings highlight how ensemble methods can improve predictive
            performance through different strengths and decision strategies.
          </p>

          <h1>Summary</h1>

          <p>
            This section explored ensemble learning methods applied to a binary
            classification problem involving user expertise levels. Two
            widely-used ensemble techniques—<strong>Random Forest</strong> and
            <strong>AdaBoost</strong>—were implemented and evaluated using a
            standardized dataset. Both models demonstrated comparable
            performance, each achieving an overall accuracy close to
            <strong>75%</strong>.
          </p>

          <p>
            Random Forest showed a more balanced classification across the two
            classes, while AdaBoost slightly favored identifying users in the
            High Expertise category. The precision-recall trade-offs and
            confusion matrices offered insight into each model's strengths,
            allowing for a clearer understanding of model behavior in real-world
            scenarios.
          </p>

          <p>
            The comparative accuracy chart visualized the marginal difference
            between the two ensemble approaches. These results underscore the
            effectiveness of ensemble learning techniques in improving
            classification accuracy and robustness, especially when working with
            diverse and high-dimensional datasets.
          </p>

          <p>
            Overall, ensemble methods serve as powerful tools for refining
            prediction models. Future enhancements may include experimenting
            with additional ensemble strategies such as Gradient Boosting,
            Stacking, or Voting Classifiers to further boost performance and
            interpretability.
          </p>

          <p>
            Link to complete code -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project/blob/main/Models/pca_model_impl.ipynb"
              target="_blank"
              >Complete Implementation</a
            >
          </p>
          <p>
            Github Repository -
            <a
              href="https://github.com/AkhilaAnnireddy/Machine_learning_project"
              target="_blank"
              >Link to github code</a
            >
          </p> -->
        </div>
        <div id="train-test-split" class="model-content active">
          <h1>Train-Test Split</h1>

          <p>
            In supervised machine learning, splitting the dataset into a
            <strong>training set</strong> and a <strong>testing set</strong> is
            a fundamental step. This disjoint split ensures that the model
            learns patterns from one subset of the data (training set) and is
            evaluated on completely unseen data (testing set). This helps assess
            the model’s generalizability and prevents overfitting, where the
            model performs well on training data but poorly on new data.
          </p>

          <p>
            For consistency across all models in this project, an 80/20
            stratified train-test split was applied unless otherwise specified.
            Stratification ensures that the distribution of classes in the
            target variable is preserved across both the training and testing
            datasets, which is particularly important for binary classification
            tasks where imbalanced classes may skew performance metrics.
          </p>

          <h2>Split Configuration by Algorithm</h2>

          <h3>Decision Tree (DT)</h3>
          <p>
            The dataset was split using an 80/20 ratio with stratified sampling.
            This maintained class balance across train and test sets, ensuring
            fair performance evaluation. The same split was reused for all GINI
            and Entropy models for consistent comparison across multiple
            decision tree configurations.
          </p>

          <h3>Logistic Regression</h3>
          <p>
            The same 80/20 stratified split was applied to the binary expertise
            dataset used for regression-based classification. This ensures that
            both "Low" and "High" expertise labels are proportionally
            represented in both sets. This consistency was important for
            comparing Logistic Regression with Multinomial Naive Bayes under
            identical data conditions.
          </p>

          <h3>Multinomial Naive Bayes</h3>
          <p>
            Multinomial Naive Bayes used the exact same training and testing
            split as Logistic Regression. To meet the algorithm’s requirement,
            any negative values in the feature set were set to zero post-split.
            Using the same split allowed for a fair side-by-side evaluation of
            model performance.
          </p>

          <h3>Ensemble Models (To be updated)</h3>
          <p>
            Once ensemble models such as Random Forest or Gradient Boosted Trees
            are implemented, this section will be updated accordingly. The
            intent is to maintain the same 80/20 stratified split to ensure
            performance comparability unless algorithm-specific preprocessing
            dictates otherwise.
          </p>

          <h3>Support Vector Machines (SVMs) (To be updated)</h3>
          <p>
            For SVMs, a consistent stratified split will be applied. Any
            deviation due to normalization or kernel-specific preprocessing
            (like scaling for RBF kernels) will be documented here after
            implementation.
          </p>

          <h2>Why Disjoint Splits Matter</h2>
          <p>
            Ensuring that the training and testing sets are disjoint (i.e.,
            contain no overlapping samples) is critical for unbiased evaluation.
            If the same samples are seen during both training and testing,
            performance metrics will be inflated and not reflective of
            real-world deployment. A clean, disjoint test set simulates unseen
            data, allowing accurate assessment of how the model will generalize
            to new inputs.
          </p>
        </div>
      </section>

      <!-- Conclusion Section -->
      <section id="conclusion" class="page-section">
        <h1>Conclusion</h1>
        <p>The conclusion will be added in future milestones.</p>
      </section>
      <section id="aboutme" class="page-section">
        <h1>About</h1>

        <img
          src="Website/assets/akhila_annireddy_headshot.png"
          alt="Akhila Annireddy"
          class="profile-image"
        />

        <div class="section">
          <h2>Professional Background</h2>
          <p>
            Akhila Annireddy is a dedicated data science professional currently
            pursuing a Master of Science in Data Science at the University of
            Colorado Boulder. With a solid background in system design and
            software development, she is passionate about integrating data
            science and machine learning to develop real-world AI solutions.
          </p>
          <p>
            Prior to her graduate studies, Akhila worked as a Software
            Development Engineer at JP Morgan Chase, where she contributed to
            cloud migration projects, automated user onboarding systems, and
            CI/CD pipelines for seamless deployment. Her experience in AWS,
            full-stack development, and software engineering principles enables
            her to bridge the gap between machine learning models and scalable
            system architecture.
          </p>
        </div>

        <div class="highlight">
          <h2>Passion for Data Science</h2>
          <p>
            Akhila is deeply interested in applying machine learning, data
            analytics, and AI to solve complex real-world challenges. She is
            particularly focused on building end-to-end AI systems that are
            efficient, scalable, and impactful.
          </p>
          <p>
            Throughout her academic and professional journey, she has led and
            contributed to data-driven projects, exploring areas such as
            predictive modeling, NLP, and AI-powered automation. Her approach
            blends strong technical expertise with practical applications,
            ensuring that machine learning models are not only well-optimized
            but also industry-ready.
          </p>
        </div>

        <div class="section">
          <h2>Creative Interests & Hobbies</h2>
          <p>
            Beyond coding and data science, Akhila enjoys a wide range of
            creative and relaxing hobbies that provide her with balance and
            inspiration. She is passionate about traditional mandala art and
            quilling, where she explores intricate designs and patterns.
          </p>
          <p>
            In addition to art, she finds joy in dancing, which serves as both
            an expressive and energetic outlet. She also loves long walks, which
            help her unwind, reflect, and stay connected with nature.
          </p>
        </div>
      </section>
    </div>

    <!-- JavaScript to handle section switching -->
    <script>
      document.addEventListener("DOMContentLoaded", function () {
        // Main navigation switching logic
        const menuLinks = document.querySelectorAll(".menu a");
        const sections = document.querySelectorAll(".page-section");

        menuLinks.forEach((link) => {
          link.addEventListener("click", (e) => {
            e.preventDefault();
            const target = link.getAttribute("data-target");
            sections.forEach((section) => section.classList.remove("active"));
            document.getElementById(target).classList.add("active");
          });
        });

        // Internal tab switching for PCA & Clustering inside "models" section
        const tabButtons = document.querySelectorAll(".tab-btn");
        const tabContents = document.querySelectorAll(".model-content");

        tabButtons.forEach((button) => {
          button.addEventListener("click", function () {
            tabButtons.forEach((btn) => btn.classList.remove("active"));
            tabContents.forEach((content) =>
              content.classList.remove("active")
            );

            this.classList.add("active");
            const target = this.getAttribute("data-target");
            document.getElementById(target).classList.add("active");
          });
        });
      });
    </script>
  </body>
</html>
